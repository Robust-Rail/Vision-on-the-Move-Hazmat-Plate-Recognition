{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating YOLO data annotations for ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "The data annotations for ultralytics require the following structure:\n",
    "```\n",
    "data/\n",
    "    - images/\n",
    "        - train/\n",
    "            - image1.jpg\n",
    "            - image2.jpg\n",
    "            - ...\n",
    "        - val/\n",
    "            - image1.jpg\n",
    "            - image2.jpg\n",
    "            - ...\n",
    "        - test/\n",
    "            - image1.jpg\n",
    "            - image2.jpg\n",
    "            - ...\n",
    "    - labels/\n",
    "        - train/\n",
    "            - image1.txt\n",
    "            - image2.txt\n",
    "            - ...\n",
    "        - val/\n",
    "            - image1.txt\n",
    "            - image2.txt\n",
    "            - ...\n",
    "        - test/\n",
    "            - image1.txt\n",
    "            - image2.txt\n",
    "            - ...\n",
    "dataset.yaml\n",
    "```\n",
    "Additionally, the dataset.yaml file should contain the following structure:\n",
    "```yaml\n",
    "path: ../images\n",
    "train: ../images/train\n",
    "val: ../images/val\n",
    "test: ../images/test\n",
    "\n",
    "nc: 1\n",
    "names: ['haz_sign']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "1. Load the lable dataset\n",
    "2. Filter sources by locally available videos\n",
    "3. Create the dataset structure\n",
    "4. Loop through the videos and create the images and labels per frame that contains the bounding boxes\n",
    "5. Save the images and labels in the dataset structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\Documents\\Code\\Python\\HIN\\UN-number-detection\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = [('train', 0.8), ('test', 0.1), ('val', 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "Path to dataset files: C:\\Users\\stani\\.cache\\kagglehub\\datasets\\stanislavlevendeev\\hazmat-detection\\versions\\13\n",
      "Path to video files: C:/Users/stani/Documents/WagonVideos\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"stanislavlevendeev/hazmat-detection\")\n",
    "video_directory = os.environ[\"PATH_TO_DATA\"]\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(\"Path to video files:\", video_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we are looking at the videos that are labled by grouping the labels into sources. We will then create the annotations for the videos in the format required by ultralytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1690279852.mp4', '1690281365.mp4', '1692830440.mp4',\n",
       "       '1690801380.mp4', '1691487366.mp4', '1692875102.mp4',\n",
       "       '1692945482.mp4', '1693811855.mp4', '1693954819.mp4',\n",
       "       '1693805101.mp4', '1693820172.mp4', '1692787289.mp4',\n",
       "       '1691496786.mp4', '1692872075.mp4', '1693308534.mp4',\n",
       "       '1693308657.mp4', '1693309263.mp4', '1693820241.mp4',\n",
       "       '1693820504.mp4', '1693820904.mp4', '1696009577.mp4',\n",
       "       '1696374314.mp4', '1696416413.mp4', '1696441496.mp4'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path + \"/labels_dataframe.csv\")\n",
    "videos = df[\"Source\"].unique()\n",
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task ID           9551\n",
       "Task Name         9551\n",
       "Job Id            9551\n",
       "Source            9551\n",
       "Frames            9551\n",
       "Absolute Frame    9551\n",
       "Relative Frame    9551\n",
       "XTL               9551\n",
       "YTL               9551\n",
       "XBR               9551\n",
       "YBR               9551\n",
       "Code              9551\n",
       "Issue             3421\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique task source pairs from the dataframe\n",
    "unique_tasks = df.drop_duplicates(subset=[\"Job Id\", \"Source\", \"Relative Frame\"])\n",
    "unique_tasks.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, we will filter the unique videos for which labels are created by these that are available locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1690281365.mp4', '1692830440.mp4', '1696009577.mp4']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_videos = os.listdir(video_directory)\n",
    "available_videos = [video for video in available_videos if video.endswith(\".mp4\")]\n",
    "available_videos = [video for video in available_videos if video in videos]\n",
    "available_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(950)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_frames = df[df[\"Source\"].isin(available_videos)].count()[\"Task ID\"]\n",
    "total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createYOLODataAnnotation(\n",
    "    path=None,\n",
    "    label_name=None,\n",
    "    classId=None,\n",
    "    img_width=0,\n",
    "    img_height=0,\n",
    "    rows=None\n",
    "):\n",
    "    #if already exists, skip\n",
    "    if os.path.exists(os.path.join(path, label_name)):\n",
    "        return\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    if not classId or classId == 0:\n",
    "        with open(os.path.join(path, label_name), \"w\") as f:\n",
    "            f.write(f\"\")\n",
    "        return\n",
    "    lable = ''\n",
    "    for index, row in rows.iterrows():\n",
    "        x_center = (row['XTL'] + row[\"XBR\"]) / 2 / img_width\n",
    "        y_center = (row['YTL'] + row['YBR']) / 2 / img_height\n",
    "        width = (row[\"XBR\"] - row['XTL']) / img_width\n",
    "        height = (row['YBR'] - row['YTL']) / img_height\n",
    "        lable += f\"0 {x_center} {y_center} {width} {height}\\n\"\n",
    "    with open(os.path.join(path, label_name), \"w\") as f:\n",
    "        f.write(lable)\n",
    "\n",
    "frames_dir = path + \"/images/\"\n",
    "os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "def saveFrame(video_name=\"\", frame_num=0, frame=None, frames_dir=frames_dir):\n",
    "    # if already exists, skip\n",
    "    if os.path.exists(f\"{frames_dir}/{video_name}_{frame_num}.jpg\"):\n",
    "        return\n",
    "    if frame is None:\n",
    "        return\n",
    "    frame_index = str(frame_num).zfill(5)\n",
    "    frame_name = f\"{video_name}_{frame_index}.jpg\"\n",
    "    cv2.imwrite(f\"{frames_dir}/{frame_name}\", frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to train dataset with 760.0 frames\n",
      "File C:/Users/stani/Documents/WagonVideos/1690281365.mp4 has 733 frames\n",
      "Created frames and labels for video C:/Users/stani/Documents/WagonVideos/1690281365.mp4: Progress 198/950\n",
      "File C:/Users/stani/Documents/WagonVideos/1692830440.mp4 has 4965 frames\n",
      "Created frames and labels for video C:/Users/stani/Documents/WagonVideos/1692830440.mp4: Progress 359/950\n",
      "File C:/Users/stani/Documents/WagonVideos/1696009577.mp4 has 4640 frames\n",
      "Switched to test dataset with 855.0 frames\n",
      "Switched to val dataset with 950.0 frames\n",
      "Created frames and labels for video C:/Users/stani/Documents/WagonVideos/1696009577.mp4: Progress 950/950\n"
     ]
    }
   ],
   "source": [
    "frame_count = 0\n",
    "dataset = 0\n",
    "switch_frame = distribution[dataset][1] * total_frames.astype(int)\n",
    "print(f\"Switched to {distribution[dataset][0]} dataset with {switch_frame} frames\")\n",
    "labels_dir = os.path.join(path, \"yolo\", \"labels\", distribution[dataset][0])\n",
    "frames_dir = os.path.join(path, \"yolo\", \"images\", distribution[dataset][0])\n",
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "os.makedirs(frames_dir, exist_ok=True)\n",
    "for video in available_videos:\n",
    "    video_path = video_directory + \"/\" + video\n",
    "    if os.path.exists(video_path) == False:\n",
    "        print(f\"File {video_path} not found\")\n",
    "        continue\n",
    "    processed_source = video.split(\".\")[0]\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    number_of_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"File {video_path} has {number_of_frames} frames\")\n",
    "    video_labels = df[df[\"Source\"] == video]\n",
    "    frame_idx = 0\n",
    "    # save each frame and .txt file with labels\n",
    "    while frame_idx < number_of_frames:\n",
    "        frame_idx += 1\n",
    "        if frame_count > switch_frame:\n",
    "            dataset += 1\n",
    "            switch_frame += distribution[dataset][1] * total_frames.astype(int)\n",
    "            labels_dir = os.path.join(path, \"yolo\", \"labels\", distribution[dataset][0])\n",
    "            frames_dir = os.path.join(path, \"yolo\", \"images\", distribution[dataset][0])\n",
    "            os.makedirs(frames_dir, exist_ok=True)\n",
    "            os.makedirs(labels_dir, exist_ok=True)\n",
    "            print(f\"Switched to {distribution[dataset][0]} dataset with {switch_frame} frames\")\n",
    "        ret, frame = cap.read()  # Read each frame\n",
    "       \n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "        img_height, img_width, _ = frame.shape\n",
    "        img_height = int(img_height)\n",
    "        img_width = int(img_width)\n",
    "        label_frames = video_labels[video_labels[\"Relative Frame\"] == frame_idx-1]\n",
    "        isObject = frame_idx-1 in video_labels[\"Relative Frame\"].values\n",
    "        if(not isObject):\n",
    "                continue\n",
    "            # Save frame\n",
    "        saveFrame(processed_source, frame_idx-1, frame, frames_dir)\n",
    "        frame_index = str(frame_idx-1).zfill(5)\n",
    "        label_name = f\"{processed_source}_{frame_index}.txt\"\n",
    "        createYOLODataAnnotation(\n",
    "            path=labels_dir,\n",
    "            label_name=label_name,\n",
    "            classId=1 if isObject else None,\n",
    "            img_height=img_height,\n",
    "            img_width=img_width,\n",
    "            rows=label_frames\n",
    "        )\n",
    "        frame_count += label_frames.count()[\"XTL\"]\n",
    "    print(f\"Created frames and labels for video {video_path}: Progress {frame_count}/{total_frames}\")\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of the data annotations\n",
    "Once data annotations is created these need to be checked for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to draw rectangles on an image\n",
    "def draw_rectangles(image_path, annotation_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    img_height, img_width, _ = image.shape\n",
    "\n",
    "    # Read the annotation file\n",
    "    with open(annotation_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        print(lines)\n",
    "        for line in lines:\n",
    "            class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "            # Convert from YOLO format to bounding box coordinates\n",
    "            x_min = int((x_center - width / 2) * img_width)\n",
    "            y_min = int((y_center - height / 2) * img_height)\n",
    "            x_max = int((x_center + width / 2) * img_width)\n",
    "            y_max = int((y_center + height / 2) * img_height)\n",
    "            # Draw the rectangle\n",
    "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "    \n",
    "    # Convert BGR image to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 950\n",
      "Random image: 992\n",
      "No image found:C:\\Users\\stani\\.cache\\kagglehub\\datasets\\stanislavlevendeev\\hazmat-detection\\versions\\13\\yolo\\images\\val\\1690281365_00992.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory containing images and annotations\n",
    "image_dir = os.path.join(path, \"yolo\", \"images\")\n",
    "annotation_dir = os.path.join(path, \"yolo\", \"labels\")\n",
    "\n",
    "\n",
    "# Get a list of all images\n",
    "images = df[df[\"Source\"].isin(available_videos)]\n",
    "print(\"Number of images:\", len(images))\n",
    "dist = 'train'\n",
    "# Pick a random image\n",
    "random_image = random.choice(images[\"Absolute Frame\"].values)\n",
    "print(\"Random image:\", random_image)\n",
    "frame_info = images[images[\"Absolute Frame\"] == random_image]\n",
    "image_name = frame_info[\"Source\"].values[0].replace('.mp4','') + \"_\" + str(frame_info[\"Relative Frame\"].values[0]).zfill(5) + \".jpg\"\n",
    "image_path = os.path.join(image_dir, dist, image_name)\n",
    "if(not os.path.isfile(image_path)):\n",
    "    dist = 'test'\n",
    "    image_path = os.path.join(image_dir, dist, image_name)\n",
    "if(not os.path.isfile(image_path)):\n",
    "    dist = 'val'\n",
    "    image_path = os.path.join(image_dir, dist, image_name)\n",
    "if( os.path.isfile(image_path)):\n",
    "    print(\"Random image:\", image_path)\n",
    "    # Corresponding annotation file\n",
    "    annotation_filename = frame_info[\"Source\"].values[0].replace('.mp4','') + \"_\" + str(frame_info[\"Relative Frame\"].values[0]).zfill(5) + \".txt\"\n",
    "    annotation_path = os.path.join(annotation_dir,dist, annotation_filename)\n",
    "\n",
    "    # Draw rectangles on the random image\n",
    "    draw_rectangles(image_path, annotation_path)\n",
    "else:\n",
    "    print(\"No image found:\" + image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950\n"
     ]
    }
   ],
   "source": [
    "print(frame_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
