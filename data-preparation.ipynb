{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating YOLO data annotations for ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "The data annotations for ultralytics require the following structure:\n",
    "```\n",
    "data/\n",
    "    - images/\n",
    "        - train/\n",
    "            - image1.jpg\n",
    "            - image2.jpg\n",
    "            - ...\n",
    "        - val/\n",
    "            - image1.jpg\n",
    "            - image2.jpg\n",
    "            - ...\n",
    "        - test/\n",
    "            - image1.jpg\n",
    "            - image2.jpg\n",
    "            - ...\n",
    "    - labels/\n",
    "        - train/\n",
    "            - image1.txt\n",
    "            - image2.txt\n",
    "            - ...\n",
    "        - val/\n",
    "            - image1.txt\n",
    "            - image2.txt\n",
    "            - ...\n",
    "        - test/\n",
    "            - image1.txt\n",
    "            - image2.txt\n",
    "            - ...\n",
    "dataset.yaml\n",
    "```\n",
    "Additionally, the dataset.yaml file should contain the following structure:\n",
    "```yaml\n",
    "path: ../images\n",
    "train: ../images/train\n",
    "val: ../images/val\n",
    "test: ../images/test\n",
    "\n",
    "nc: 1\n",
    "names: ['haz_sign']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "1. Load the lable dataset\n",
    "2. Filter sources by locally available videos\n",
    "3. Create the dataset structure\n",
    "4. Loop through the videos and create the images and labels per frame that contains the bounding boxes\n",
    "5. Save the images and labels in the dataset structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = [('train', 0.8), ('test', 0.1), ('val', 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"stanislavlevendeev/hazmat-detection\")\n",
    "video_directory = os.environ[\"PATH_TO_DATA\"]\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(\"Path to video files:\", video_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we are looking at the videos that are labled by grouping the labels into sources. We will then create the annotations for the videos in the format required by ultralytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + \"/labels_dataframe.csv\")\n",
    "videos = df[\"Source\"].unique()\n",
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique task source pairs from the dataframe\n",
    "unique_tasks = df.drop_duplicates(subset=[\"Job Id\", \"Source\", \"Relative Frame\"])\n",
    "unique_tasks.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, we will filter the unique videos for which labels are created by these that are available locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_videos = os.listdir(video_directory)\n",
    "available_videos = [video for video in available_videos if video.endswith(\".mp4\")]\n",
    "available_videos = [video for video in available_videos if video in videos]\n",
    "available_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = df[df[\"Source\"].isin(available_videos)].count()[\"Absolute Frame\"]\n",
    "total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createYOLODataAnnotation(\n",
    "    path=None,\n",
    "    label_name=None,\n",
    "    classId=None,\n",
    "    img_width=0,\n",
    "    img_height=0,\n",
    "    rows=None\n",
    "):\n",
    "    #if already exists, skip\n",
    "    if os.path.exists(os.path.join(path, label_name)):\n",
    "        return\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    if not classId or classId == 0:\n",
    "        with open(os.path.join(path, label_name), \"w\") as f:\n",
    "            f.write(f\"\")\n",
    "        return\n",
    "    lable = ''\n",
    "    for index, row in rows.iterrows():\n",
    "        x_center = (row['XTL'] + row[\"XBR\"]) / 2 / img_width\n",
    "        y_center = (row['YTL'] + row['YBR']) / 2 / img_height\n",
    "        width = (row[\"XBR\"] - row['XTL']) / img_width\n",
    "        height = (row['YBR'] - row['YTL']) / img_height\n",
    "        lable += f\"0 {x_center} {y_center} {width} {height}\\n\"\n",
    "    with open(os.path.join(path, label_name), \"w\") as f:\n",
    "        f.write(lable)\n",
    "\n",
    "frames_dir = path + \"/images/\"\n",
    "os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "def saveFrame(video_name=\"\", frame_num=0, frame=None, frames_dir=frames_dir):\n",
    "    # if already exists, skip\n",
    "    if os.path.exists(f\"{frames_dir}/{video_name}_{frame_num}.jpg\"):\n",
    "        return\n",
    "    if frame is None:\n",
    "        return\n",
    "    frame_index = str(frame_num).zfill(5)\n",
    "    frame_name = f\"{video_name}_{frame_index}.jpg\"\n",
    "    cv2.imwrite(f\"{frames_dir}/{frame_name}\", frame)\n",
    "annotations_created = {\n",
    "    'train': 0,\n",
    "    'test': 0,\n",
    "    'val': 0\n",
    "}\n",
    "def get_rnd_distribution():\n",
    "    new_dist = distribution.copy()\n",
    "    while len(new_dist) > 0:\n",
    "        rnd_dist = random.choice(new_dist)\n",
    "        required_amount = int(total_frames * rnd_dist[1])\n",
    "        if required_amount >= annotations_created[rnd_dist[0]]:\n",
    "            return rnd_dist\n",
    "        else:\n",
    "            new_dist.remove(rnd_dist)\n",
    "    print(\"WHAT THE HELL IS GOING ON?\")\n",
    "    return distribution[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in distribution:\n",
    "    labels_dir = os.path.join(path, \"yolo\", \"labels\", name)\n",
    "    frames_dir = os.path.join(path, \"yolo\", \"images\", name)\n",
    "    os.makedirs(labels_dir, exist_ok=True)\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "labels_dir = os.path.join(path, \"yolo\", \"labels\")\n",
    "frames_dir = os.path.join(path, \"yolo\", \"images\")\n",
    "with tqdm(total=total_frames, desc=\"Processing\") as pbar:\n",
    "    for video in available_videos:\n",
    "        video_path = video_directory + \"/\" + video\n",
    "        if os.path.exists(video_path) == False:\n",
    "            print(f\"File {video_path} not found\")\n",
    "            continue\n",
    "        processed_source = video.split(\".\")[0]\n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        number_of_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        video_labels = df[df[\"Source\"] == video]\n",
    "        frame_idx = 0\n",
    "        # save each frame and .txt file with labels\n",
    "        while frame_idx < number_of_frames:\n",
    "            ret, frame = cap.read()  # Read each frame\n",
    "            if not ret:\n",
    "                break  # End of video\n",
    "            img_height, img_width, _ = frame.shape\n",
    "            img_height = int(img_height)\n",
    "            img_width = int(img_width)\n",
    "            label_frames = video_labels[video_labels[\"Relative Frame\"] == frame_idx]\n",
    "            isObject = frame_idx in video_labels[\"Relative Frame\"].values\n",
    "            if(isObject):\n",
    "                rnd_dist = get_rnd_distribution()\n",
    "                saveFrame(processed_source, frame_idx, frame, os.path.join(frames_dir, rnd_dist[0]))\n",
    "                frame_index = str(frame_idx).zfill(5)\n",
    "                label_name = f\"{processed_source}_{frame_index}.txt\"\n",
    "                createYOLODataAnnotation(\n",
    "                    path=os.path.join(labels_dir, rnd_dist[0]),\n",
    "                    label_name=label_name,\n",
    "                    classId=1 if isObject else None,\n",
    "                    img_height=img_height,\n",
    "                    img_width=img_width,\n",
    "                    rows=label_frames\n",
    "                )\n",
    "                annotations_created[rnd_dist[0]] += label_frames.shape[0]\n",
    "                pbar.update( label_frames.shape[0])\n",
    "            frame_idx += 1\n",
    "            pbar.set_description(f\"Processing {video}, Frame {frame_idx}/{number_of_frames}\")\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of the data annotations\n",
    "Once data annotations is created these need to be checked for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to draw rectangles on an image\n",
    "def draw_rectangles(image_path, annotation_path, issue = None):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    img_height, img_width, _ = image.shape\n",
    "\n",
    "    # Read the annotation file\n",
    "    with open(annotation_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "            # Convert from YOLO format to bounding box coordinates\n",
    "            x_min = int((x_center - width / 2) * img_width)\n",
    "            y_min = int((y_center - height / 2) * img_height)\n",
    "            x_max = int((x_center + width / 2) * img_width)\n",
    "            y_max = int((y_center + height / 2) * img_height)\n",
    "            # Draw the rectangle\n",
    "            #change color for each new rectangle so that they are distinguishable the first element is the color in BGR format\n",
    "            #Display issue\n",
    "            if issue != None and issue != \"\":\n",
    "                cv2.putText(image, str(issue), (x_min, y_min), cv2.FONT_HERSHEY_SIMPLEX, 1,  (0, 255, 50), 2)\n",
    "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            \n",
    "    \n",
    "    # Convert BGR image to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory containing images and annotations\n",
    "image_dir = os.path.join(path, \"yolo\", \"images\")\n",
    "annotation_dir = os.path.join(path, \"yolo\", \"labels\")\n",
    "\n",
    "\n",
    "# Get a list of all images\n",
    "images = df[df[\"Source\"].isin(available_videos)]\n",
    "print(\"Number of images:\", len(images))\n",
    "dist = 'train'\n",
    "# Pick a random image\n",
    "random_image = random.choice(images[\"Absolute Frame\"].values)\n",
    "print(\"Random image:\", random_image)\n",
    "frame_info = images[images[\"Absolute Frame\"] == random_image]\n",
    "image_name = frame_info[\"Source\"].values[0].replace('.mp4','') + \"_\" + str(frame_info[\"Relative Frame\"].values[0]).zfill(5) + \".jpg\"\n",
    "image_path = os.path.join(image_dir, dist, image_name)\n",
    "if(not os.path.isfile(image_path)):\n",
    "    dist = 'test'\n",
    "    image_path = os.path.join(image_dir, dist, image_name)\n",
    "if(not os.path.isfile(image_path)):\n",
    "    dist = 'val'\n",
    "    image_path = os.path.join(image_dir, dist, image_name)\n",
    "if( os.path.isfile(image_path)):\n",
    "    print(\"Random image:\", image_path)\n",
    "    # Corresponding annotation file\n",
    "    annotation_filename = frame_info[\"Source\"].values[0].replace('.mp4','') + \"_\" + str(frame_info[\"Relative Frame\"].values[0]).zfill(5) + \".txt\"\n",
    "    annotation_path = os.path.join(annotation_dir,dist, annotation_filename)\n",
    "\n",
    "    # Draw rectangles on the random image\n",
    "    draw_rectangles(image_path, annotation_path, frame_info[\"Issue\"].values[0])\n",
    "else:\n",
    "    print(\"No image found:\" + image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
