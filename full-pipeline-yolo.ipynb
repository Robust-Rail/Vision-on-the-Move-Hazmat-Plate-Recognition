{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from transformers import Idefics2Processor, Idefics2ForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"YOLO_VERBOSE\"] = \"False\"  # Prevent auto-downloads\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "iou_threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_private = kagglehub.dataset_download(\"stanislavlevendeev/hazmat-detection\")\n",
    "path_private\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_public = kagglehub.dataset_download(\"stanislavlevendeev/haz-mat-signs\")\n",
    "path_public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_private = pd.read_csv( os.path.join(path_private, \"labels_dataframe.csv\"))\n",
    "df_private[\"Checked\"] = False\n",
    "df_private.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public = pd.read_csv(os.path.join(path_public, \"images_with_boxes.csv\"))\n",
    "df_public[\"Checked\"] = False\n",
    "df_public.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_private.groupby(\"Absolute Frame\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_private.groupby(\"Absolute Frame\").filter(\n",
    "    lambda group: group[\"Code\"].nunique() > 1\n",
    ")\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df.groupby(\"Absolute Frame\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_un_numbers = pd.read_csv(os.path.join(\"./data\", \"un-number-labels.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_detection =  YOLO('./data/yolo/yolo11x_earlystopping.pt')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_detection.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Idefics2Processor.from_pretrained( \"HuggingFaceM4/idefics2-8b\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "print('Processor loaded')\n",
    "print('Device:', device)\n",
    "model_ocr = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "     \"HuggingFaceM4/idefics2-8b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device,\n",
    "    quantization_config=quantization_config,   \n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model_ocr = model_ocr.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the image and extract two key values:\n",
    "\n",
    "    The UN number visible on the upper part of the placard.\n",
    "    The code visible on the lower part of the placard, located below the horizontal line separating the two sections.\n",
    "\n",
    "Both codes are printed in black. If either the upper or lower part cannot be detected, replace the missing value with \"0.\" Output the extracted values as plain text, separated by a comma if multiple codes are present. No additional context or formatting is needed.\n",
    "\n",
    "Input Examples:\n",
    "\n",
    "    {98 {line} 4567}\n",
    "    (not found, {line}, 8901)\n",
    "    {101 {line} 3345}\n",
    "    (not found, {line}, {not found})\n",
    "    {45 {line} 2789}\n",
    "    {22 {line} 5678}\n",
    "\n",
    "Desired Output:\n",
    "\n",
    "    98, 4567\n",
    "    0, 8901\n",
    "    101, 3345\n",
    "    0, 0\n",
    "    45, 2789\n",
    "    22, 5678\n",
    "\n",
    "Expected Transformation:\n",
    "\n",
    "    For each input example, extract the UN number and the code below the horizontal line.\n",
    "    If either part is missing (i.e., \"not found\"), replace it with 0.\n",
    "    Output the extracted values as plain text, separated by a comma, without any additional context or formatting.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "text = processor.apply_chat_template(messages, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_un_number(image, bbox):\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    # crop image\n",
    "    cropped_img = pil_img.crop(bbox)\n",
    "    # OCR\n",
    "    return perform_ocr(cropped_img)\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) between two bounding boxes.\n",
    "    box1 and box2 are in the format (x_min, y_min, x_max, y_max).\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    # Compute the area of intersection\n",
    "    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "    # Compute the area of both bounding boxes\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    # Compute the union area\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if union_area == 0:\n",
    "        return 0\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = intersection_area / union_area\n",
    "    return iou   \n",
    "def perform_ocr(image): \n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "    generated_text = model_ocr.generate(**inputs, max_new_tokens=500)\n",
    "    generated_text = processor.batch_decode(generated_text, skip_special_tokens=True)[0]\n",
    "    assistant_output = generated_text.split(\"Assistant:\")[1].strip()\n",
    "    \n",
    "    # Split the output by comma to get the individual numbers\n",
    "    numbers = assistant_output.split(\",\")\n",
    "\n",
    "    numbers = [number.strip().replace('.','') for number in numbers]\n",
    "    numbers.append('0000') \n",
    "    numbers.append('0000')\n",
    "    un_number, hin_number = numbers[:2]\n",
    "    return un_number, hin_number  \n",
    "\n",
    "def get_bboxes(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    results = model_detection(img)\n",
    "    return results, img     \n",
    "\n",
    "def get_predictions(image_path):\n",
    "    image_name = image_path.split(\"\\\\\")[-1]\n",
    "    predictions = None\n",
    "    pub = False\n",
    "    if '_' in image_name:\n",
    "        video_name = image_name.split('_')[0]\n",
    "        frame_id = int(image_name.split('_')[1].split('.')[0])\n",
    "        predictions = df_private[(df_private['Source'] == video_name + '.mp4') & (df_private['Relative Frame'] == int(frame_id))]\n",
    "        \n",
    "    else:\n",
    "        image_id = image_name.split('.')[0]\n",
    "        predictions = df_public[df_public['image_id'] == int(image_id)]\n",
    "        pub = True\n",
    "    return predictions, pub\n",
    "def get_ground_truth(image_path, bbox: tuple[float, float, float, float]):\n",
    "    predictions, pub = get_predictions(image_path)\n",
    "    # Check if the predicted bbox matches any ground truth bbox\n",
    "    if predictions is not None and not predictions.empty:\n",
    "        # Check for the existence of 'XTL' or 'box_xtl' columns\n",
    "        xtl_column = 'XTL' if 'XTL' in predictions.columns else 'box_xtl'\n",
    "        ytl_column = 'YTL' if 'YTL' in predictions.columns else 'box_ytl'\n",
    "        xbr_column = 'XBR' if 'XBR' in predictions.columns else 'box_xbr'\n",
    "        ybr_column = 'YBR' if 'YBR' in predictions.columns else 'box_ybr'\n",
    "\n",
    "        # Iterate through predictions to check IoU\n",
    "        for idx, row in predictions.iterrows():\n",
    "            ground_truth_bbox = (row[xtl_column], row[ytl_column], row[xbr_column], row[ybr_column])\n",
    "            iou = calculate_iou(bbox, ground_truth_bbox)\n",
    "            \n",
    "            if iou > iou_threshold:  # IoU threshold\n",
    "                # Update the 'Checked' column in the original DataFrame\n",
    "                if pub:\n",
    "                    df_public.loc[idx, 'Checked'] = True\n",
    "                else:\n",
    "                    df_private.loc[idx, 'Checked'] = True\n",
    "                code = row['Code'] if 'Code' in row else row[\"code\"]\n",
    "                return code.split('/')\n",
    "\n",
    "    # If no valid prediction is found, return \"code\"\n",
    "    return None\n",
    "def get_description(un_number):\n",
    "    # if unnumber can not be converted to int\n",
    "    try:\n",
    "        un_number = int(un_number)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    description = df_un_numbers[df_un_numbers['number'] == un_number]\n",
    "    if not description.empty:\n",
    "        return description['description'].values[0]\n",
    "    return None\n",
    "def store_result(res):\n",
    "    print('Saving evaluation')\n",
    "    with open('full-pipeline-yolo-evaluationnn.json', 'w') as f:\n",
    "        json.dump(res, f)\n",
    "    print('Evaluation saved')        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {\"metrics\": [], \"success\": 0, \"failure\": 0}\n",
    "\n",
    "def test_path(path):\n",
    "    # list all png jpeg files in the path\n",
    "    files = [f for f in os.listdir(path) if f.endswith('.png') or f.endswith('.jpg')]\n",
    "    with tqdm(total=len(files), desc=\"Processing images\", unit=\"image\") as pbar:\n",
    "        for file in files:\n",
    "            image_path = os.path.join(path, file)\n",
    "            [results, img] = get_bboxes(image_path)\n",
    "            if results is not None:\n",
    "                for result in results:  # Assuming result is a list of bounding boxes\n",
    "                    for bbox in result.boxes:\n",
    "                        bbox = bbox.xyxy[0].tolist()\n",
    "                        ground_truth = get_ground_truth(image_path, bbox)\n",
    "                        un_number, hin_number = get_un_number(img, bbox)\n",
    "                        if ground_truth is not None:\n",
    "                            if ground_truth[1] == hin_number and ground_truth[0] == un_number:\n",
    "                                test_metrics[\"success\"] += 1\n",
    "                            else:\n",
    "                                test_metrics[\"failure\"] += 1\n",
    "                        else:\n",
    "                            test_metrics[\"failure\"] += 1\n",
    "                        test_metrics[\"metrics\"].append({\n",
    "                            \"image\": image_path,\n",
    "                            \"bbox\": bbox,\n",
    "                            \"ground_truth\": ground_truth,\n",
    "                            \"prediction\": [un_number, hin_number]\n",
    "                        })\n",
    "            preds, pub = get_predictions(image_path)\n",
    "            if preds is not None:\n",
    "                for idx, row in preds.iterrows():\n",
    "                    if not row['Checked']:\n",
    "                        test_metrics[\"failure\"] += 1\n",
    "                        test_metrics[\"metrics\"].append({\n",
    "                            \"image\": image_path,\n",
    "                            \"bbox\" : None,\n",
    "                            \"actual_bbox\": [row['XTL'], row['YTL'], row['XBR'], row['YBR']],\n",
    "                            \"ground_truth\": [row['Code']],\n",
    "                            \"prediction\": [0, 0]\n",
    "                        })\n",
    "            # Update the progress bar and description\n",
    "            pbar.set_description(f\"Processing images | Success: {test_metrics['success']} | Failure: {test_metrics['failure']}\")\n",
    "            pbar.update(1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_description('1005'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "def analyze_image(img):\n",
    "        # Convert the uploaded image to OpenCV format (BGR)\n",
    "    img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    result_out = []\n",
    "    un_numbers = []\n",
    "    # Debug: Print the shape of the image\n",
    "    results = model_detection(img)\n",
    "    if results is not None:\n",
    "        for result in results:  # Assuming result is a list of bounding boxes\n",
    "            for bbox in result.boxes:\n",
    "                bbox = bbox.xyxy[0].tolist()\n",
    "                # draw bounding box\n",
    "                un_number, hin_number = get_un_number(img, bbox)\n",
    "                desc = get_description(hin_number)\n",
    "                if desc is not None:\n",
    "                    # greeen color\n",
    "                    cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "                    # text numnber\n",
    "                    cv2.putText(img, f\"{len(un_numbers)}\", (int(bbox[0]), int(bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                    un_numbers.append({\n",
    "                        \"hin_number\": un_number,\n",
    "                        \"un_number\": hin_number,\n",
    "                        \"description\": desc\n",
    "                    })\n",
    "                else:\n",
    "                    #red color\n",
    "                    cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 0, 255), 2)\n",
    "    # convert image to pil image   \n",
    "    img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))    \n",
    "    return img, un_numbers\n",
    "demo = gr.Interface(fn=analyze_image, inputs=\"image\", outputs=[\"image\",\"json\"])\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(path_public, \"images\")\n",
    "print(path)\n",
    "test_path(path)\n",
    "# test_path(os.path.join(\"/home\", \"s3544648\", \"data\", \"yolo\", \"images\", \"val\"))\n",
    "# test_path(os.path.join(\"/home\", \"s3544648\", \"data\", \"yolo\", \"images\", \"train\"))\n",
    "# test_path(os.path.join(path_public, \"images\"))\n",
    "store_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_metrics[\"metrics\"])\n",
    "store_result(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df_public where checked flag is true\n",
    "df_public_checked = df_public[df_public[\"Checked\"] == True]\n",
    "df_public_checked.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json\n",
    "data =  None\n",
    "with open('./data/evaluation_full_pipeline_yolo.json') as f:\n",
    "  data = json.load(f)\n",
    "# make df of mteriss property\n",
    "df = pd.DataFrame(data['metrics'])\n",
    "# add description column\n",
    "df['description'] = df['prediction'].apply(lambda x: get_description(x[1]))\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Public Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort df for images that consist public\n",
    "df_public = df[df['image'].str.contains('public')]\n",
    "df_public.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look how many of them satisfy ground truth = prediction\n",
    "df_correct = df_public[df_public['ground_truth'] == df_public['prediction']]\n",
    "df_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ratio of correct predictions\n",
    "print(f'Ratio of correct predictions: {df_correct.shape[0] / df_public.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Private Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort df for images that consist public\n",
    "df_private = df[df['image'].str.contains('_')]\n",
    "df_private.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look how many of them satisfy ground truth = prediction\n",
    "df_correct = df_private[df_private['ground_truth'] == df_private['prediction']]\n",
    "df_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ratio of correct predictions\n",
    "print(f'Ratio of correct predictions: {df_correct.shape[0] / df_private.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted = df[df['prediction'] ==  None or df['prediction'] == [\"0\", \"0\"]]\n",
    "df_predicted.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
