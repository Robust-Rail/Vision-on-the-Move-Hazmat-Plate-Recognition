{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OCR techniques used in the research: Tesseract, EasyOCR and Idefics2 need to be evaluated. The metrics used for that are CER(Character Error Rate) and WER (Word Error Rate). According the fact that in the usecases there are no words but codes, the ground truth will be compared with the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import easyocr\n",
    "from tqdm import tqdm\n",
    "from transformers import Idefics2Processor, Idefics2ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "import os\n",
    "import Levenshtein\n",
    "import json\n",
    "import easyocr\n",
    "import pytesseract\n",
    "import regex as re\n",
    "import numpy as np\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "C:/Users/stani/.cache/kagglehub/datasets/stanislavlevendeev/hazmat-detection/versions/14/labels_dataframe.csv\n"
     ]
    }
   ],
   "source": [
    "annotations_path = kagglehub.dataset_download(\"stanislavlevendeev/hazmat-detection\")\n",
    "annotations_path = annotations_path.replace('\\\\', '/')\n",
    "path_video = os.environ['PATH_TO_DATA']\n",
    "data_path = annotations_path + '/labels_dataframe.csv'\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task ID</th>\n",
       "      <th>Task Name</th>\n",
       "      <th>Job Id</th>\n",
       "      <th>Source</th>\n",
       "      <th>Frames</th>\n",
       "      <th>Absolute Frame</th>\n",
       "      <th>Relative Frame</th>\n",
       "      <th>XTL</th>\n",
       "      <th>YTL</th>\n",
       "      <th>XBR</th>\n",
       "      <th>YBR</th>\n",
       "      <th>Code</th>\n",
       "      <th>Issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>Task1</td>\n",
       "      <td>133</td>\n",
       "      <td>1690279852.mp4</td>\n",
       "      <td>730</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>29.87</td>\n",
       "      <td>506.88</td>\n",
       "      <td>190.69</td>\n",
       "      <td>554.96</td>\n",
       "      <td>83/2789</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138</td>\n",
       "      <td>Task1</td>\n",
       "      <td>133</td>\n",
       "      <td>1690279852.mp4</td>\n",
       "      <td>730</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>65.26</td>\n",
       "      <td>504.87</td>\n",
       "      <td>225.50</td>\n",
       "      <td>552.95</td>\n",
       "      <td>83/2789</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138</td>\n",
       "      <td>Task1</td>\n",
       "      <td>133</td>\n",
       "      <td>1690279852.mp4</td>\n",
       "      <td>730</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>131.98</td>\n",
       "      <td>503.67</td>\n",
       "      <td>291.63</td>\n",
       "      <td>551.76</td>\n",
       "      <td>83/2789</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138</td>\n",
       "      <td>Task1</td>\n",
       "      <td>133</td>\n",
       "      <td>1690279852.mp4</td>\n",
       "      <td>730</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>198.69</td>\n",
       "      <td>502.48</td>\n",
       "      <td>357.76</td>\n",
       "      <td>550.57</td>\n",
       "      <td>83/2789</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138</td>\n",
       "      <td>Task1</td>\n",
       "      <td>133</td>\n",
       "      <td>1690279852.mp4</td>\n",
       "      <td>730</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>241.62</td>\n",
       "      <td>498.68</td>\n",
       "      <td>400.10</td>\n",
       "      <td>546.77</td>\n",
       "      <td>83/2789</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Task ID Task Name  Job Id          Source  Frames  Absolute Frame  \\\n",
       "0      138     Task1     133  1690279852.mp4     730              54   \n",
       "1      138     Task1     133  1690279852.mp4     730              55   \n",
       "2      138     Task1     133  1690279852.mp4     730              56   \n",
       "3      138     Task1     133  1690279852.mp4     730              57   \n",
       "4      138     Task1     133  1690279852.mp4     730              58   \n",
       "\n",
       "   Relative Frame     XTL     YTL     XBR     YBR     Code Issue  \n",
       "0              54   29.87  506.88  190.69  554.96  83/2789   NaN  \n",
       "1              55   65.26  504.87  225.50  552.95  83/2789   NaN  \n",
       "2              56  131.98  503.67  291.63  551.76  83/2789   NaN  \n",
       "3              57  198.69  502.48  357.76  550.57  83/2789   NaN  \n",
       "4              58  241.62  498.68  400.10  546.77  83/2789   NaN  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_labels = pd.read_csv(data_path)\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1690281365.mp4', '1692830440.mp4', '1696009577.mp4']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_sources = [f for f in os.listdir(path_video) if f.endswith('.mp4')]\n",
    "available_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Relative Frame</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1690281365.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1690281365.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>1690281365.mp4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1690281365.mp4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1690281365.mp4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Source  Relative Frame  0\n",
       "200  1690281365.mp4               0  1\n",
       "201  1690281365.mp4               1  1\n",
       "202  1690281365.mp4               2  1\n",
       "203  1690281365.mp4               3  1\n",
       "204  1690281365.mp4               4  1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by in daaataframe\n",
    "df_labels_grouped = df_labels.groupby(['Source', 'Relative Frame']).size().reset_index().iloc[:, :4]\n",
    "df_labels_grouped = df_labels_grouped[df_labels_grouped['Source'].isin(available_sources)]\n",
    "df_labels_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor loaded\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:37<00:00,  5.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# Init  idefics2\n",
    "processor = Idefics2Processor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "print('Processor loaded')\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "    \"HuggingFaceM4/idefics2-8b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device,\n",
    "    quantization_config=quantization_config,   \n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the image and extract two key values:\n",
    "\n",
    "    The UN number visible on the upper part of the placard.\n",
    "    The code visible on the lower part of the placard, located below the horizontal line separating the two sections.\n",
    "\n",
    "Both codes are printed in black. If either the upper or lower part cannot be detected, replace the missing value with \"0.\" Output the extracted values as plain text, separated by a comma if multiple codes are present. No additional context or formatting is needed.\n",
    "\n",
    "Input Examples:\n",
    "\n",
    "    {98 {line} 4567}\n",
    "    (not found, {line}, 8901)\n",
    "    {101 {line} 3345}\n",
    "    (not found, {line}, {not found})\n",
    "    {45 {line} 2789}\n",
    "    {22 {line} 5678}\n",
    "\n",
    "Desired Output:\n",
    "\n",
    "    98, 4567\n",
    "    0, 8901\n",
    "    101, 3345\n",
    "    0, 0\n",
    "    45, 2789\n",
    "    22, 5678\n",
    "\n",
    "Expected Transformation:\n",
    "\n",
    "    For each input example, extract the UN number and the code below the horizontal line.\n",
    "    If either part is missing (i.e., \"not found\"), replace it with 0.\n",
    "    Output the extracted values as plain text, separated by a comma, without any additional context or formatting.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "text = processor.apply_chat_template(messages, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/stani/.cache/kagglehub/datasets/stanislavlevendeev/hazmat-detection/versions/14/yolo/images'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop throuw grouped df_lables_grouped\n",
    "annotations_path = annotations_path + '/yolo/images'\n",
    "annotations_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(source, frame):\n",
    "    source = source.split('.')[0]\n",
    "    frame = str(frame).zfill(5)\n",
    "    path = f'{annotations_path}/train/{source}_{frame}.jpg'\n",
    "    image = None\n",
    "    if os.path.exists(path):\n",
    "        image = Image.open(path)\n",
    "    path = f'{annotations_path}/test/{source}_{frame}.jpg'\n",
    "    if os.path.exists(path):\n",
    "        image = Image.open(path)\n",
    "    path = f'{annotations_path}/val/{source}_{frame}.jpg'\n",
    "    if os.path.exists(path):\n",
    "        image = Image.open(path)\n",
    "    if image is None:\n",
    "        print(f'Image not found: {path}')\n",
    "    return image\n",
    "# Define the method\n",
    "def extract_bounding_box(self, bbox):\n",
    "    \"\"\"\n",
    "    Extract a bounding box from the image.\n",
    "    \n",
    "    :param bbox: A tuple of (left, top, right, bottom) coordinates.\n",
    "    :return: A new Image object containing the cropped region.\n",
    "    \"\"\"\n",
    "    return self.crop(bbox)\n",
    "\n",
    "def perform_ocr(image): \n",
    "    \n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "    generated_text = model.generate(**inputs, max_new_tokens=500)\n",
    "    generated_text = processor.batch_decode(generated_text, skip_special_tokens=True)[0]\n",
    "    assistant_output = generated_text.split(\"Assistant:\")[1].strip()\n",
    "    # Split the output by comma to get the individual numbers\n",
    "    numbers = assistant_output.split(\",\")\n",
    "    # Strip any leading or trailing whitespace from the numbers\n",
    "    numbers = [number.strip().replace('.','') for number in numbers]\n",
    "    numbers.append('0000') \n",
    "    numbers.append('0000')\n",
    "    un_number, hin_number = numbers[:2]\n",
    "    return un_number, hin_number\n",
    "def get_text_from_image_easyocr(image):\n",
    "    # Initialize the reader for digits\n",
    "    reader = easyocr.Reader([\"en\"])\n",
    "    result = reader.readtext(image, allowlist=\"0123456789\",detail=0)\n",
    "    h,w = None, None\n",
    "    try:\n",
    "        h, w = image.shape\n",
    "    except:\n",
    "        h,w,_ = image.shape\n",
    "    image_un = image[0:int(h/2), 0:w]\n",
    "    image_hin = image[int(h/2):h, 0:w]\n",
    "    result_un = reader.readtext(image_un, allowlist=\"0123456789\",detail=0)\n",
    "    result_hin = reader.readtext(image_hin, allowlist=\"0123456789\",detail=0)\n",
    "    result_un = result_un[0] if len(result_un) > 0 else '00'\n",
    "    result_hin = result_hin[0] if len(result_hin) > 0 else '0000'\n",
    "    return result_un,result_hin,result\n",
    "def extract_un_number(text):\n",
    "    un = re.findall(r'\\d{2,}', text)\n",
    "    un = un[0] if len(un) > 0 else '00'\n",
    "    return un\n",
    "\n",
    "def extract_hin_number(text):\n",
    "    hin = re.findall(r'\\d{4,}', text)\n",
    "    hin = hin[0] if len(hin) > 0 else '0000'\n",
    "    return hin\n",
    "def get_text_from_image_ocr(image):\n",
    "    #split image horizontally in two pieces\n",
    "    h, w = image.shape[:2]\n",
    "    image_upper = image[0:int(h/2), 0:w]\n",
    "    image_lower = image[int(h/2):h, 0:w]\n",
    "    psm = 6\n",
    "    option = f\"--psm {psm}\"\n",
    "    text_un = pytesseract.image_to_string(image_upper, config=option)\n",
    "    text_hin = pytesseract.image_to_string(image_lower, config=option)\n",
    "    \n",
    "    return extract_un_number(text_un), extract_hin_number(text_hin)\n",
    "def calculate_cer(gt, ocr):\n",
    "    return Levenshtein.distance(gt, ocr) / max(1, len(gt))\n",
    "def calculate_wer(gt, ocr):\n",
    "    gt_words = gt.split()\n",
    "    ocr_words = ocr.split()\n",
    "    return Levenshtein.distance(\" \".join(gt_words), \" \".join(ocr_words)) / max(1, len(gt_words))\n",
    "def to_cv2(self):\n",
    "    \"\"\"\n",
    "    Convert a PIL Image to an OpenCV image (numpy array) in grayscale.\n",
    "    \n",
    "    :return: Grayscale OpenCV image (numpy array)\n",
    "    \"\"\"\n",
    "    # Convert PIL Image to numpy array\n",
    "    numpy_image = np.array(self)\n",
    "    \n",
    "    # Convert RGB to BGR (OpenCV format)\n",
    "    opencv_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Convert BGR to Grayscale\n",
    "    grayscale_image = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    return grayscale_image\n",
    "def store_evaluation(lable_info, ocr_technique, prediction, gt):\n",
    "    cer_un = calculate_cer(gt[0], prediction[0])\n",
    "    wer_un = calculate_wer(gt[0], prediction[0])\n",
    "    cer_hin = calculate_cer(gt[1], prediction[1])\n",
    "    wer_hin = calculate_wer(gt[1], prediction[1])\n",
    "    evaluation[ocr_technique]['WER'] += (wer_un + wer_hin)/2\n",
    "    evaluation[ocr_technique]['CER'] += (cer_un + cer_hin)/2\n",
    "    evaluation[ocr_technique]['images'].append({\n",
    "        **lable_info,\n",
    "        'prediction': prediction,\n",
    "        'gt': gt,\n",
    "        'CER': (cer_un + cer_hin)/2,\n",
    "        'WER' : (wer_un + wer_hin)/2,\n",
    "    })\n",
    "\n",
    "# Add the method to the Image class\n",
    "Image.Image.extract_bounding_box = extract_bounding_box\n",
    "Image.Image.to_cv2 = to_cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/867 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     64\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m---> 65\u001b[0m         prediction_idefics \u001b[38;5;241m=\u001b[39m \u001b[43mperform_ocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcropped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m         store_evaluation(lable_info, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midefics\u001b[39m\u001b[38;5;124m'\u001b[39m, prediction_idefics, actual_code)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of lables:\u001b[39m\u001b[38;5;124m'\u001b[39m, number_lables)     \n",
      "Cell \u001b[1;32mIn[47], line 30\u001b[0m, in \u001b[0;36mperform_ocr\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mperform_ocr\u001b[39m(image): \n\u001b[0;32m     29\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, text\u001b[38;5;241m=\u001b[39mtext, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 30\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(generated_text, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     32\u001b[0m     assistant_output \u001b[38;5;241m=\u001b[39m generated_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2252\u001b[0m     )\n\u001b[0;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2275\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3251\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[1;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\transformers\\models\\idefics2\\modeling_idefics2.py:1623\u001b[0m, in \u001b[0;36mIdefics2ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_attention_mask, image_hidden_states, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep)\u001b[0m\n\u001b[0;32m   1620\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1623\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1624\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1638\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1639\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\transformers\\models\\idefics2\\modeling_idefics2.py:1444\u001b[0m, in \u001b[0;36mIdefics2Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_attention_mask, image_hidden_states, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_seen_tokens \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m image_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;66;03m# When we generate, we don't want to replace the potential image_token_id that we generated by images\u001b[39;00m\n\u001b[0;32m   1437\u001b[0m     \u001b[38;5;66;03m# that simply don't exist\u001b[39;00m\n\u001b[0;32m   1438\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs_merger(\n\u001b[0;32m   1439\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1440\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1441\u001b[0m         image_hidden_states\u001b[38;5;241m=\u001b[39mimage_hidden_states,\n\u001b[0;32m   1442\u001b[0m     )\n\u001b[1;32m-> 1444\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_legacy_cache \u001b[38;5;129;01mand\u001b[39;00m use_cache:\n\u001b[0;32m   1456\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mpast_key_values \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpast_key_values\u001b[38;5;241m.\u001b[39mto_legacy_cache()\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:561\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    550\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    551\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    558\u001b[0m         position_embeddings,\n\u001b[0;32m    559\u001b[0m     )\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 561\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    573\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:261\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[0;32m    260\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 261\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m    263\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\Code\\Python\\HIN\\second try\\UN-number-detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_lables = 0\n",
    "evaluation = {\n",
    "    \"idefics\": {\n",
    "        \"images\": [],\n",
    "        \"WER\": 0,\n",
    "        \"CER\": 0,\n",
    "    },\n",
    "    \"easyocr\": {\n",
    "        \"images\": [],\n",
    "        \"WER\": 0,\n",
    "        \"CER\": 0,\n",
    "    },\n",
    "    \"tesseract\": {\n",
    "        \"images\": [],\n",
    "        \"WER\": 0,\n",
    "        \"CER\": 0,\n",
    "    },\n",
    "}\n",
    "\n",
    "for index, frames in tqdm(df_labels_grouped.head(100).iterrows(), total=df_labels_grouped.shape[0]):\n",
    "\n",
    "    # Read image\n",
    "\n",
    "    image = read_image(frames['Source'], frames['Relative Frame'])\n",
    "\n",
    "    if image is None:\n",
    "\n",
    "        continue\n",
    "\n",
    "    data_annotation = df_labels[(df_labels['Source'] == frames['Source']) & (df_labels['Relative Frame'] == frames['Relative Frame'])]\n",
    "\n",
    "    #for loop\n",
    "\n",
    "    for index, lable in data_annotation.iterrows():\n",
    "\n",
    "        number_lables +=1\n",
    "\n",
    "        # Crop image\n",
    "\n",
    "        cropped = image.extract_bounding_box((int(lable['XTL']), int(lable['YTL']), int(lable['XBR']), int(lable['YBR'])))\n",
    "\n",
    "        # OCR\n",
    "        lable_info = {\n",
    "            'video': frames['Source'],\n",
    "            'frame': frames['Relative Frame'],\n",
    "            'XTL': int(lable['XTL']),\n",
    "            'YTL': int(lable['YTL']),\n",
    "            'XBR': int(lable['XBR']),\n",
    "            'YBR': int(lable['YBR']),\n",
    "        }\n",
    "        actual_code = str(lable['Code']).split('/')\n",
    "        cropped_cv2 = cropped.to_cv2()\n",
    "        # EasyOCR\n",
    "        un, hin, ocr = get_text_from_image_easyocr(cropped_cv2)\n",
    "        prediction_easyocr = (un, hin)\n",
    "        store_evaluation(lable_info, 'easyocr', prediction_easyocr, actual_code)\n",
    "        # Tesseract\n",
    "        un, hin = get_text_from_image_ocr(cropped_cv2)\n",
    "        prediction_ocr = (un, hin)\n",
    "        store_evaluation(lable_info, 'tesseract', prediction_ocr, actual_code)\n",
    "        # Idefics\n",
    "        prediction_idefics = perform_ocr(cropped)\n",
    "        store_evaluation(lable_info, 'idefics', prediction_idefics, actual_code)\n",
    "\n",
    "print('Number of lables:', number_lables)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation  Idefics:\n",
      "Number of lables: 100\n",
      "Raw CER: 0.75\n",
      "Raw WER: 2.5\n",
      "CER: 0.0075\n",
      "WER: 0.025\n",
      "Evaluation  EasyOCR:\n",
      "Number of lables: 100\n",
      "Raw CER: 3.125\n",
      "Raw WER: 11.5\n",
      "CER: 0.03125\n",
      "WER: 0.115\n",
      "Evaluation  OCR:\n",
      "Number of lables: 100\n",
      "Raw CER: 65.75\n",
      "Raw WER: 186.0\n",
      "CER: 0.6575\n",
      "WER: 1.86\n",
      "Saving evaluation\n",
      "Evaluation saved\n"
     ]
    }
   ],
   "source": [
    "print('Evaluation  Idefics:')\n",
    "print('Number of lables:', number_lables)\n",
    "print ('Raw CER:', evaluation['idefics']['CER'])\n",
    "print ('Raw WER:',  evaluation['idefics']['WER'])\n",
    "print('CER:',  evaluation['idefics']['CER']/number_lables)\n",
    "print('WER:',  evaluation['idefics']['WER']/number_lables)\n",
    "print('Evaluation  EasyOCR:')\n",
    "print('Number of lables:', number_lables)\n",
    "print ('Raw CER:', evaluation['easyocr']['CER'])\n",
    "print ('Raw WER:',  evaluation['easyocr']['WER'])\n",
    "print('CER:',  evaluation['easyocr']['CER']/number_lables)\n",
    "print('WER:',  evaluation['easyocr']['WER']/number_lables)\n",
    "print('Evaluation  Tesseract:')\n",
    "print('Number of lables:', number_lables)\n",
    "print ('Raw CER:',  evaluation['tesseract']['CER'])\n",
    "print ('Raw WER:', evaluation['tesseract']['WER'])\n",
    "print('CER:', evaluation['tesseract']['CER']/number_lables)\n",
    "print('WER:', evaluation['tesseract']['WER']/number_lables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving evaluation')\n",
    "with open('evaluation.json', 'w') as f:\n",
    "    json.dump(evaluation, f)\n",
    "print('Evaluation saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>frame</th>\n",
       "      <th>XTL</th>\n",
       "      <th>YTL</th>\n",
       "      <th>XBR</th>\n",
       "      <th>YBR</th>\n",
       "      <th>path</th>\n",
       "      <th>prediction</th>\n",
       "      <th>gt</th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1690279852.mp4</td>\n",
       "      <td>54</td>\n",
       "      <td>29</td>\n",
       "      <td>506</td>\n",
       "      <td>190</td>\n",
       "      <td>554</td>\n",
       "      <td>./data/yolo/images/val/1690279852_00054.jpg</td>\n",
       "      <td>[83, 2789]</td>\n",
       "      <td>[83, 2789]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1690279852.mp4</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>504</td>\n",
       "      <td>225</td>\n",
       "      <td>552</td>\n",
       "      <td>./data/yolo/images/val/1690279852_00055.jpg</td>\n",
       "      <td>[83, 2789]</td>\n",
       "      <td>[83, 2789]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1690279852.mp4</td>\n",
       "      <td>56</td>\n",
       "      <td>131</td>\n",
       "      <td>503</td>\n",
       "      <td>291</td>\n",
       "      <td>551</td>\n",
       "      <td>./data/yolo/images/val/1690279852_00056.jpg</td>\n",
       "      <td>[83, 2789]</td>\n",
       "      <td>[83, 2789]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1690279852.mp4</td>\n",
       "      <td>57</td>\n",
       "      <td>198</td>\n",
       "      <td>502</td>\n",
       "      <td>357</td>\n",
       "      <td>550</td>\n",
       "      <td>./data/yolo/images/val/1690279852_00057.jpg</td>\n",
       "      <td>[83, 2789]</td>\n",
       "      <td>[83, 2789]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1690279852.mp4</td>\n",
       "      <td>58</td>\n",
       "      <td>241</td>\n",
       "      <td>498</td>\n",
       "      <td>400</td>\n",
       "      <td>546</td>\n",
       "      <td>./data/yolo/images/val/1690279852_00058.jpg</td>\n",
       "      <td>[83, 2789]</td>\n",
       "      <td>[83, 2789]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            video  frame  XTL  YTL  XBR  YBR  \\\n",
       "0  1690279852.mp4     54   29  506  190  554   \n",
       "1  1690279852.mp4     55   65  504  225  552   \n",
       "2  1690279852.mp4     56  131  503  291  551   \n",
       "3  1690279852.mp4     57  198  502  357  550   \n",
       "4  1690279852.mp4     58  241  498  400  546   \n",
       "\n",
       "                                          path  prediction          gt  CER  \\\n",
       "0  ./data/yolo/images/val/1690279852_00054.jpg  [83, 2789]  [83, 2789]  0.0   \n",
       "1  ./data/yolo/images/val/1690279852_00055.jpg  [83, 2789]  [83, 2789]  0.0   \n",
       "2  ./data/yolo/images/val/1690279852_00056.jpg  [83, 2789]  [83, 2789]  0.0   \n",
       "3  ./data/yolo/images/val/1690279852_00057.jpg  [83, 2789]  [83, 2789]  0.0   \n",
       "4  ./data/yolo/images/val/1690279852_00058.jpg  [83, 2789]  [83, 2789]  0.0   \n",
       "\n",
       "   WER  \n",
       "0  0.0  \n",
       "1  0.0  \n",
       "2  0.0  \n",
       "3  0.0  \n",
       "4  0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open json file\n",
    "evaluation = None\n",
    "with open('evaluation.json') as f:\n",
    "    evaluation = json.load(f)\n",
    "#make datagframe\n",
    "df_idefics = pd.DataFrame(evaluation['idefics']['images'])\n",
    "df_idefics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>XTL</th>\n",
       "      <th>YTL</th>\n",
       "      <th>XBR</th>\n",
       "      <th>YBR</th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>543.251390</td>\n",
       "      <td>1228.155252</td>\n",
       "      <td>1021.255522</td>\n",
       "      <td>1417.614651</td>\n",
       "      <td>1162.310345</td>\n",
       "      <td>0.047857</td>\n",
       "      <td>0.152868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>345.559928</td>\n",
       "      <td>1159.209151</td>\n",
       "      <td>435.721364</td>\n",
       "      <td>1165.059902</td>\n",
       "      <td>473.340496</td>\n",
       "      <td>0.186367</td>\n",
       "      <td>0.590430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>250.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>618.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>499.000000</td>\n",
       "      <td>884.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>812.000000</td>\n",
       "      <td>2032.000000</td>\n",
       "      <td>1463.000000</td>\n",
       "      <td>2304.000000</td>\n",
       "      <td>1642.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1473.000000</td>\n",
       "      <td>3774.000000</td>\n",
       "      <td>1731.000000</td>\n",
       "      <td>3840.000000</td>\n",
       "      <td>1899.000000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             frame          XTL          YTL          XBR          YBR  \\\n",
       "count  6293.000000  6293.000000  6293.000000  6293.000000  6293.000000   \n",
       "mean    543.251390  1228.155252  1021.255522  1417.614651  1162.310345   \n",
       "std     345.559928  1159.209151   435.721364  1165.059902   473.340496   \n",
       "min       0.000000     0.000000   354.000000    57.000000   428.000000   \n",
       "25%     250.000000   136.000000   532.000000   410.000000   618.000000   \n",
       "50%     499.000000   884.000000  1063.000000  1080.000000  1208.000000   \n",
       "75%     812.000000  2032.000000  1463.000000  2304.000000  1642.000000   \n",
       "max    1473.000000  3774.000000  1731.000000  3840.000000  1899.000000   \n",
       "\n",
       "               CER          WER  \n",
       "count  6293.000000  6293.000000  \n",
       "mean      0.047857     0.152868  \n",
       "std       0.186367     0.590430  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.000000     0.000000  \n",
       "50%       0.000000     0.000000  \n",
       "75%       0.000000     0.000000  \n",
       "max       1.875000     4.500000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idefics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>XTL</th>\n",
       "      <th>YTL</th>\n",
       "      <th>XBR</th>\n",
       "      <th>YBR</th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>543.251390</td>\n",
       "      <td>1228.155252</td>\n",
       "      <td>1021.255522</td>\n",
       "      <td>1417.614651</td>\n",
       "      <td>1162.310345</td>\n",
       "      <td>0.562881</td>\n",
       "      <td>1.857461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>345.559928</td>\n",
       "      <td>1159.209151</td>\n",
       "      <td>435.721364</td>\n",
       "      <td>1165.059902</td>\n",
       "      <td>473.340496</td>\n",
       "      <td>0.325953</td>\n",
       "      <td>1.092048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>250.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>618.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>499.000000</td>\n",
       "      <td>884.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>812.000000</td>\n",
       "      <td>2032.000000</td>\n",
       "      <td>1463.000000</td>\n",
       "      <td>2304.000000</td>\n",
       "      <td>1642.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1473.000000</td>\n",
       "      <td>3774.000000</td>\n",
       "      <td>1731.000000</td>\n",
       "      <td>3840.000000</td>\n",
       "      <td>1899.000000</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             frame          XTL          YTL          XBR          YBR  \\\n",
       "count  6293.000000  6293.000000  6293.000000  6293.000000  6293.000000   \n",
       "mean    543.251390  1228.155252  1021.255522  1417.614651  1162.310345   \n",
       "std     345.559928  1159.209151   435.721364  1165.059902   473.340496   \n",
       "min       0.000000     0.000000   354.000000    57.000000   428.000000   \n",
       "25%     250.000000   136.000000   532.000000   410.000000   618.000000   \n",
       "50%     499.000000   884.000000  1063.000000  1080.000000  1208.000000   \n",
       "75%     812.000000  2032.000000  1463.000000  2304.000000  1642.000000   \n",
       "max    1473.000000  3774.000000  1731.000000  3840.000000  1899.000000   \n",
       "\n",
       "               CER          WER  \n",
       "count  6293.000000  6293.000000  \n",
       "mean      0.562881     1.857461  \n",
       "std       0.325953     1.092048  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.250000     1.000000  \n",
       "50%       0.625000     2.000000  \n",
       "75%       0.875000     2.500000  \n",
       "max       1.375000     4.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ocr = pd.DataFrame(evaluation['tesseract']['images'])\n",
    "df_ocr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>XTL</th>\n",
       "      <th>YTL</th>\n",
       "      <th>XBR</th>\n",
       "      <th>YBR</th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "      <td>6293.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>543.251390</td>\n",
       "      <td>1228.155252</td>\n",
       "      <td>1021.255522</td>\n",
       "      <td>1417.614651</td>\n",
       "      <td>1162.310345</td>\n",
       "      <td>0.329553</td>\n",
       "      <td>1.109248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>345.559928</td>\n",
       "      <td>1159.209151</td>\n",
       "      <td>435.721364</td>\n",
       "      <td>1165.059902</td>\n",
       "      <td>473.340496</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>1.108049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>250.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>618.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>499.000000</td>\n",
       "      <td>884.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>812.000000</td>\n",
       "      <td>2032.000000</td>\n",
       "      <td>1463.000000</td>\n",
       "      <td>2304.000000</td>\n",
       "      <td>1642.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1473.000000</td>\n",
       "      <td>3774.000000</td>\n",
       "      <td>1731.000000</td>\n",
       "      <td>3840.000000</td>\n",
       "      <td>1899.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             frame          XTL          YTL          XBR          YBR  \\\n",
       "count  6293.000000  6293.000000  6293.000000  6293.000000  6293.000000   \n",
       "mean    543.251390  1228.155252  1021.255522  1417.614651  1162.310345   \n",
       "std     345.559928  1159.209151   435.721364  1165.059902   473.340496   \n",
       "min       0.000000     0.000000   354.000000    57.000000   428.000000   \n",
       "25%     250.000000   136.000000   532.000000   410.000000   618.000000   \n",
       "50%     499.000000   884.000000  1063.000000  1080.000000  1208.000000   \n",
       "75%     812.000000  2032.000000  1463.000000  2304.000000  1642.000000   \n",
       "max    1473.000000  3774.000000  1731.000000  3840.000000  1899.000000   \n",
       "\n",
       "               CER          WER  \n",
       "count  6293.000000  6293.000000  \n",
       "mean      0.329553     1.109248  \n",
       "std       0.331522     1.108049  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.000000     0.000000  \n",
       "50%       0.250000     1.000000  \n",
       "75%       0.583333     2.000000  \n",
       "max       1.333333     4.500000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_easy = pd.DataFrame(evaluation['easyocr']['images'])\n",
    "df_easy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display images where the CER is higher than 0.5\n",
    "df_low_precision = df_idefics[df_idefics['CER'] > 0.5]\n",
    "for(index, row) in df_low_precision.iterrows():\n",
    "    image = cv2.imread(row[\"path\"])\n",
    "    #draw bounding box and draw prediiction and gt array \n",
    "    image = cv2.rectangle(image, (row[\"XTL\"], row[\"YTL\"]), (row[\"XBR\"], row[\"YBR\"]), (0, 255, 0), 2)\n",
    "    cv2.putText(image, f'Prediction: {row[\"prediction\"]}', (row[\"XTL\"], row[\"YTL\"] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    cv2.putText(image, f'GT: {row[\"gt\"]}', (row[\"XTL\"], row[\"YTL\"] - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    # Display the image in console\n",
    "    cv2.imshow(\"Image\", image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
