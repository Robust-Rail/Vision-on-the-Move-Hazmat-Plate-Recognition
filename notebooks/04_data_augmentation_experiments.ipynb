{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61feee27",
   "metadata": {},
   "source": [
    "# Data augmentation Experiments\n",
    "\n",
    "For this project we will implement different types of data augmentations as well as custom data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee70dc3",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96453461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradScaler is used for mixed precision training in PyTorch.\n",
    "from torch.amp import GradScaler\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve().parent / \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3942d992",
   "metadata": {},
   "source": [
    "Classes and augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ewald\\Documents\\un-number-detection\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ewald\\Documents\\un-number-detection\\.venv\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# Import necessary functions and classes\n",
    "from un_detector.data.datasets import HazmatDataset\n",
    "from un_detector.data.augmentation import (\n",
    "    Compose,\n",
    "    ToTensor,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomBrightnessCont,\n",
    "    RandomBlur,\n",
    "    RandomRotate,\n",
    "    RandomZoom,\n",
    "    get_augmented_transform,\n",
    "    visualize_augmentations\n",
    ")\n",
    "from un_detector.utils.file_io import save_json, tensor_to_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d770ccee",
   "metadata": {},
   "source": [
    "# Faster R-CNN Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cdf08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GradScaler for mixed precision training\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3172f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your augmentations\n",
    "augmentations = get_augmented_transform(train=True)\n",
    "\n",
    "# Create directories to save augmented data\n",
    "base_dir = 'data/processed/prorail_coco_format/augmented_data'\n",
    "augmented_images_dir = base_dir + '/images'\n",
    "# Define the path for augmented annotations\n",
    "augmented_annotations_file = base_dir + '/annotations/instances_aug.json'\n",
    "os.makedirs(augmented_images_dir, exist_ok=True)\n",
    "\n",
    "# Remove all files in the augmented_images_dir\n",
    "if os.path.exists(augmented_images_dir):\n",
    "    for file in os.listdir(augmented_images_dir):\n",
    "        file_path = os.path.join(augmented_images_dir, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # Remove the file or symlink\n",
    "            elif os.path.isdir(file_path):\n",
    "                os.rmdir(file_path)  # Remove the directory (if empty)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting file {file_path}: {e}\")\n",
    "else:\n",
    "    print(f\"Directory {augmented_images_dir} does not exist.\")\n",
    "\n",
    "\n",
    "# Check if the augmented annotations file exists, if not create it\n",
    "if os.path.exists(augmented_annotations_file):\n",
    "    os.remove(augmented_annotations_file)  # Delete the existing file\n",
    "    print(f\"Deleted existing annotations file: {augmented_annotations_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a default structure for the augmented annotations\n",
    "categories_list = [\n",
    "    {\"id\": 1, \"name\": \"hazmat code\"}\n",
    "]\n",
    "\n",
    "# Load the original dataset, we use train in this instance to apply augmentations \n",
    "# but we will not use it for training and you might actually want to concat the train, validation and test datasets for this use case.\n",
    "original_dataset = HazmatDataset(\n",
    "    root='data/processed/prorail_coco_format/formatted_data/train',\n",
    "    ann_file='annotations/instances_train.json',\n",
    "    transforms=augmentations,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "augmented_annotations = {\n",
    "    'images': [],\n",
    "    'annotations': [],\n",
    "    'categories': categories_list\n",
    "}\n",
    "save_json(augmented_annotations, augmented_annotations_file)\n",
    "print(f\"Created new annotations file: {augmented_annotations_file}\")\n",
    "\n",
    "\n",
    "# Counter for image and annotation IDs\n",
    "img_id = len(augmented_annotations['images'])\n",
    "ann_id = len(augmented_annotations['annotations'])\n",
    "\n",
    "# Number of augmented images to create\n",
    "num_images_to_generate = 1000  # Set this to your desired limit\n",
    "\n",
    "# Iterate through the dataset with progress tracking\n",
    "generated_count = 0  # Counter for how many images we have generated\n",
    "for idx in tqdm(range(len(original_dataset)), desc=\"Augmenting dataset\"):\n",
    "    if generated_count >= num_images_to_generate:\n",
    "        break  # Stop when the desired number of augmented images is created\n",
    "    \n",
    "    img, target = original_dataset[idx]\n",
    "    \n",
    "    # Apply augmentations\n",
    "    aug_img, aug_target = augmentations(img, target)\n",
    "    \n",
    "    # Save augmented image\n",
    "    aug_img_path = os.path.join(augmented_images_dir, f'aug_{img_id}.jpg')\n",
    "    if isinstance(aug_img, torch.Tensor):\n",
    "        aug_img = F.to_pil_image(aug_img)\n",
    "    aug_img.save(aug_img_path)\n",
    "    \n",
    "    # Update image annotation\n",
    "    augmented_annotations['images'].append({\n",
    "        'id': img_id,\n",
    "        'file_name': os.path.basename(aug_img_path),\n",
    "        'width': aug_img.width,\n",
    "        'height': aug_img.height\n",
    "    })\n",
    "    \n",
    "    # Update annotations\n",
    "    for box, label in zip(aug_target['boxes'], aug_target['labels']):\n",
    "        augmented_annotations['annotations'].append({\n",
    "            'id': ann_id,\n",
    "            'image_id': img_id,\n",
    "            'category_id': label.item(),\n",
    "            'bbox': [\n",
    "                box[0].item(), box[1].item(), \n",
    "                box[2].item() - box[0].item(), \n",
    "                box[3].item() - box[1].item()\n",
    "            ],\n",
    "            'area': (box[2] - box[0]) * (box[3] - box[1]),\n",
    "            'iscrowd': 0\n",
    "        })\n",
    "        ann_id += 1\n",
    "\n",
    "    # Update the image and annotation IDs\n",
    "    img_id += 1\n",
    "    generated_count += 1  # Increase the generated image counter\n",
    "\n",
    "# Convert tensors to lists before saving the annotations\n",
    "augmented_annotations = tensor_to_list(augmented_annotations)\n",
    "\n",
    "# Save augmented annotations\n",
    "save_json(augmented_annotations, augmented_annotations_file)\n",
    "\n",
    "print(f\"Generated {generated_count} augmented images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_augmentations(augmented_images_dir, augmented_annotations_file, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca7f49",
   "metadata": {},
   "source": [
    "# YOLOv11\n",
    "For Yolov11 we can use the standard ultralytics library for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7878367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of training a YOLOv11 model with the augmentations applied\n",
    "results = model.train(\n",
    "    data=path+'\\\\yolo\\\\dataset.yaml', \n",
    "    epochs=10,\n",
    "    scale=0.5,\n",
    "    shear=1.1,\n",
    "    device=device,\n",
    "    degrees=10.5,\n",
    "    perspective=0.5,\n",
    "    mosaic=0.5,\n",
    "    hsv_h=0.015,\n",
    "    hsv_s=0.7,\n",
    "    hsv_v=0.4,\n",
    "    multiscale=True,\n",
    "    )\n",
    "model.save(\"data\\\\yolo\\\\yolo11n_trained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d638af4",
   "metadata": {},
   "source": [
    "# Weather Augmentations\n",
    "This code is used to augment images with weather conditions, however the annotation algorithm has not been written yet, therefore this can only be used for human evaluation (e.g. visualising how the model performs under weather augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from typing import Union\n",
    "import cv2\n",
    "def generate_augmented_images(\n",
    "    input_image_path: str,\n",
    "    output_dir: str,\n",
    "    augmentation: Union[str, A.Compose],\n",
    "    num_augmentations: int = 5,\n",
    "    seed: int = None,\n",
    "    quality: int = 95,\n",
    "    prefix: str = \"aug\",\n",
    "    verbose: bool = True,\n",
    "    augmentation_presets: dict = None\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Generate and save augmented images to a specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_image_path: Path to source image (str)\n",
    "    - output_dir: Output directory path (str) - will be created if not exists\n",
    "    - augmentation: Albumentations transform or preset name ('rain', 'fog', etc.)\n",
    "    - num_augmentations: Number of augmented versions to create (int)\n",
    "    - seed: Optional random seed for reproducibility (int)\n",
    "    - quality: Output JPEG quality (1-100)\n",
    "    - prefix: Filename prefix for output images\n",
    "    - verbose: Print progress messages\n",
    "    \n",
    "    Returns:\n",
    "    List of saved file paths (list[str])\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not os.path.isfile(input_image_path):\n",
    "        raise FileNotFoundError(f\"Input image not found: {input_image_path}\")\n",
    "    \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load image\n",
    "    image = cv2.cvtColor(cv2.imread(input_image_path), cv2.COLOR_BGR2RGB)\n",
    "    base_name = os.path.splitext(os.path.basename(input_image_path))[0]\n",
    "    \n",
    "    # Only initialize augmentation presets if not provided\n",
    "    if augmentation_presets is None:\n",
    "        # Define default augmentation presets\n",
    "        augmentation_presets = {\n",
    "            'rain': A.RandomRain(\n",
    "                brightness_coefficient=0.9,  # Slightly darker rain\n",
    "                drop_width=3, # Thicker rain drops\n",
    "                blur_value=5, # Stronger blur effect\n",
    "                p=1, # Always apply rain\n",
    "                drop_length=20), # Longer rain streaks\n",
    "            'sun_flare': A.RandomSunFlare(\n",
    "                flare_roi=(0, 0, 1, 0.5), # Flare in the upper half of the image\n",
    "                angle_lower=0.5, # Angle of tronger sun flare\n",
    "                p=1), # Always apply sun flare\n",
    "            'shadow': A.RandomShadow(\n",
    "                num_shadows_lower=10, # More shadows\n",
    "                num_shadows_upper=15, \n",
    "                shadow_dimension=8, # Larger and darker shadows\n",
    "                shadow_roi=(0, 0, 1, 1), # Shadows across the entire image\n",
    "                p=1), # Always apply shadows\n",
    "            'fog': A.RandomFog(p=1), # Always apply fog\n",
    "            \n",
    "            # 'snow': A.RandomSnow(p=1) # Snow is really unrealistic and not recommended\n",
    "        }\n",
    "\n",
    "    \n",
    "    # Configure augmentation pipeline\n",
    "    if isinstance(augmentation, str):\n",
    "        if augmentation not in augmentation_presets:\n",
    "            raise ValueError(f\"Unknown preset: {augmentation}. Available: {list(augmentation_presets.keys())}\")\n",
    "        transform = A.Compose([augmentation_presets[augmentation]])\n",
    "    else:\n",
    "        transform = augmentation\n",
    "    \n",
    "    # Generate augmented images\n",
    "    saved_paths = []\n",
    "    for i in range(num_augmentations):\n",
    "        try:\n",
    "            augmented = transform(image=image)['image']\n",
    "            output_path = os.path.join(output_dir, f\"{augmentation}-{i}-{prefix}_{base_name}.jpg\")\n",
    "            \n",
    "            Image.fromarray(augmented).save(\n",
    "                output_path,\n",
    "                quality=quality,\n",
    "                optimize=True,\n",
    "                subsampling=0  # Keep highest chroma resolution\n",
    "            )\n",
    "            \n",
    "            saved_paths.append(output_path)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Saved: {output_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating augmentation {i+1}: {str(e)}\")\n",
    "    \n",
    "    return saved_paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
