{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import contextlib\n",
    "import io\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import ColorJitter, GaussianBlur\n",
    "import random\n",
    "import matplotlib.patches as patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HazmatDataset(Dataset):\n",
    "    def __init__(self, data_dir, annotations_file, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Load annotations\n",
    "        with open(annotations_file) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.images = {img['id']: img for img in data['images']}\n",
    "        self.annotations = data['annotations']\n",
    "\n",
    "        # Create image_id to annotations mapping\n",
    "        self.img_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_to_anns:\n",
    "                self.img_to_anns[img_id] = []\n",
    "            self.img_to_anns[img_id].append(ann)\n",
    "\n",
    "        self.ids = list(self.images.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.data_dir, 'images', img_info['file_name'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Get annotations\n",
    "        anns = self.img_to_anns.get(img_id, [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "\n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']\n",
    "            # Convert [x, y, w, h] to [x1, y1, x2, y2]\n",
    "            x_min = bbox[0]\n",
    "            y_min = bbox[1]\n",
    "            x_max = bbox[0] + bbox[2]\n",
    "            y_max = bbox[1] + bbox[3]\n",
    "\n",
    "            # Filter out degenerate boxes\n",
    "            if x_max > x_min and y_max > y_min:\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(ann['category_id'])\n",
    "                areas.append(ann['area'])\n",
    "                iscrowd.append(ann['iscrowd'])\n",
    "\n",
    "        # Convert to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': areas,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        # Convert PIL image to tensor\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_classifier_loss = 0\n",
    "    total_box_reg_loss = 0\n",
    "    total_objectness_loss = 0\n",
    "    total_rpn_box_reg_loss = 0\n",
    "\n",
    "    # Voeg tqdm toe om de voortgang te tonen\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=True)\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Wrap the forward pass in autocast\n",
    "        with autocast(device_type='cuda'):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Scale the loss and call backward\n",
    "        scaler.scale(losses).backward()\n",
    "        # Unscales the gradients and calls or skips optimizer.step()\n",
    "        scaler.step(optimizer)\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        # Bereken de totalen\n",
    "        total_loss += losses.item()\n",
    "        total_classifier_loss += loss_dict['loss_classifier'].item()\n",
    "        total_box_reg_loss += loss_dict['loss_box_reg'].item()\n",
    "        total_objectness_loss += loss_dict['loss_objectness'].item()\n",
    "        total_rpn_box_reg_loss += loss_dict['loss_rpn_box_reg'].item()\n",
    "\n",
    "        # Update tqdm-balk\n",
    "        progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{losses.item():.4f}\",\n",
    "            \"Classifier\": f\"{loss_dict['loss_classifier'].item():.4f}\",\n",
    "            \"BoxReg\": f\"{loss_dict['loss_box_reg'].item():.4f}\",\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_classifier_loss = total_classifier_loss / len(data_loader)\n",
    "    avg_box_reg_loss = total_box_reg_loss / len(data_loader)\n",
    "    avg_objectness_loss = total_objectness_loss / len(data_loader)\n",
    "    avg_rpn_box_reg_loss = total_rpn_box_reg_loss / len(data_loader)\n",
    "\n",
    "    return avg_loss, avg_classifier_loss, avg_box_reg_loss, avg_objectness_loss, avg_rpn_box_reg_loss\n",
    "\n",
    "\n",
    "\n",
    "# Load ground truth annotations\n",
    "coco_val = COCO('data/data_faster_rcnn/val/annotations/instances_val.json')\n",
    "\n",
    "# Prepare predictions in COCO format\n",
    "# Assuming you have a function to convert model outputs to COCO format\n",
    "# Conversion to COCO Format\n",
    "def convert_to_coco_format(outputs, image_ids):\n",
    "    coco_results = []\n",
    "    for output, image_id in zip(outputs, image_ids):\n",
    "        boxes = output['boxes'].cpu().numpy()\n",
    "        scores = output['scores'].cpu().numpy()\n",
    "        labels = output['labels'].cpu().numpy()\n",
    "        \n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            coco_results.append({\n",
    "                'image_id': image_id,\n",
    "                'category_id': int(label),\n",
    "                'bbox': [box[0], box[1], box[2] - box[0], box[3] - box[1]],\n",
    "                'score': float(score)\n",
    "            })\n",
    "    return coco_results\n",
    "\n",
    "# Validation Function\n",
    "def validate(model, data_loader, coco_gt, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    # Add tqdm\n",
    "    progress_bar = tqdm(data_loader, desc=\"Validation\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in progress_bar:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            image_ids = [target['image_id'].item() for target in targets]\n",
    "            coco_results = convert_to_coco_format(outputs, image_ids)\n",
    "            results.extend(coco_results)\n",
    "\n",
    "            # Update tqdm-bar\n",
    "            progress_bar.set_postfix({\"Processed\": len(results)})\n",
    "\n",
    "    if not results:\n",
    "        print(\"No predictions generated. Skipping evaluation.\")\n",
    "        return [0.0] * 6  # Return dummy metrics for empty results\n",
    "\n",
    "    # Suppress COCOeval output\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        coco_dt = coco_gt.loadRes(results)\n",
    "        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "    return coco_eval.stats\n",
    "\n",
    "\n",
    "# Custom backbone to return a dictionary of feature maps\n",
    "class BackboneWithChannels(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return {'0': x}\n",
    "    \n",
    "# Function to create a subset of the dataset\n",
    "def create_subset(dataset, percentage):\n",
    "    \"\"\"\n",
    "    Create a subset of the dataset based on the given percentage.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: The full dataset.\n",
    "    - percentage: The fraction of the dataset to use (value between 0.0 and 1.0).\n",
    "    \n",
    "    Returns:\n",
    "    - subset: A subset of the dataset containing the specified percentage of data.\n",
    "    \"\"\"\n",
    "    if not (0.0 < percentage <= 1.0):\n",
    "        raise ValueError(\"Percentage must be between 0.0 and 1.0.\")\n",
    "    \n",
    "    # Determine the subset size\n",
    "    total_samples = len(dataset)\n",
    "    subset_size = int(total_samples * percentage)\n",
    "    \n",
    "    # Shuffle and select a random subset of indices\n",
    "    indices = list(range(total_samples))\n",
    "    random.shuffle(indices)\n",
    "    subset_indices = indices[:subset_size]\n",
    "    \n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "def create_directory(base_path=\"data/models\"):\n",
    "    \"\"\"\n",
    "    Create a directory inside the base path named 'faster-rcnn-finetuned-{date}' \n",
    "    to store models and logs. The name includes the current date and time in the format 'DD-MM-YYYY HH:MM:SS'.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Base directory where the new directory will be created.\n",
    "\n",
    "    Returns:\n",
    "    - directory_path (str): Full path to the created directory.\n",
    "    \"\"\"\n",
    "    # Get the current date and time\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "    \n",
    "    # Define the full directory path\n",
    "    directory_name = f\"faster-rcnn-finetuned-{current_time}\"\n",
    "    directory_path = os.path.join(base_path, directory_name)\n",
    "    \n",
    "    # Create the directory\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Directory created: {directory_path}\")\n",
    "    return directory_path\n",
    "\n",
    "def train_model(directory, model, optimizer, train_loader, device, train_metrics_list, best_val_map, lr_scheduler, val_loader, coco_val, scaler, epoch):\n",
    "    \n",
    "    epoch+=1\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_classifier_loss, train_box_reg_loss, train_objectness_loss, train_rpn_box_reg_loss = train_one_epoch(\n",
    "        model, optimizer, train_loader, device, scaler)\n",
    "    \n",
    "    # Validate and get all COCO-metrics\n",
    "    val_metrics = validate(model, val_loader, coco_val, device)\n",
    "    val_map = val_metrics[0]  # mAP@IoU=0.50:0.95\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    \n",
    "    # Obtain the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Prepare data for logging\n",
    "    data = {\n",
    "        \"epoch\": epoch,\n",
    "        \"time_elapsed\": (int(minutes), int(seconds)),\n",
    "        \"learning_rate\": current_lr,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"classifier_loss\": train_classifier_loss,\n",
    "        \"box_reg_loss\": train_box_reg_loss,\n",
    "        \"objectness_loss\": train_objectness_loss,\n",
    "        \"rpn_box_reg_loss\": train_rpn_box_reg_loss,\n",
    "        \"val_metrics\": val_metrics\n",
    "    }\n",
    "    \n",
    "    # Append current epoch data to metrics list\n",
    "    train_metrics_list.append(data)\n",
    "    \n",
    "    # Print summary for this epoch\n",
    "    print(f\"📊 Epoch {epoch} | ⏳ Time: {int(minutes)}m {int(seconds)}s | 🔄 LR: {current_lr:.6f}\")\n",
    "    print(f\"📉 Train Loss: {train_loss:.4f} | 🎯 Classifier: {train_classifier_loss:.4f} | 📦 Box Reg: {train_box_reg_loss:.4f}\")\n",
    "    print(f\"🔍 Objectness: {train_objectness_loss:.4f} | 🗂️ RPN Box Reg: {train_rpn_box_reg_loss:.4f}\")\n",
    "    print(f\"🧪 mAP | 🟢 mAP@IoU=0.50:0.95: {val_metrics[0]:.4f} | 🔵 mAP@IoU=0.50: {val_metrics[1]:.4f} | 🟣 mAP@IoU=0.75: {val_metrics[2]:.4f}\")\n",
    "    print(f\"📏 Small mAP: {val_metrics[3]:.4f} | 📐 Medium mAP: {val_metrics[4]:.4f} | 📏 Large mAP: {val_metrics[5]:.4f}\")\n",
    "    \n",
    "    # Save epoch data to a log file\n",
    "    save_epoch_data(directory, data)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save the latest checkpoint with all metrics\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_map': val_map,\n",
    "        'train_metrics_list': train_metrics_list  # Save all metrics\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(directory, \"latest_model.pth\"))\n",
    "    \n",
    "    # Save the best model if the val_map is the highest so far\n",
    "    if val_map > best_val_map:\n",
    "        best_val_map = val_map\n",
    "        torch.save(checkpoint, os.path.join(directory, \"best_model.pth\"))\n",
    "    \n",
    "    return best_val_map\n",
    "        \n",
    "\n",
    "\n",
    "def save_epoch_data(directory, data):\n",
    "    \"\"\"\n",
    "    Save training statistics for each epoch in a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): Path to the directory.\n",
    "    - data (dict): Contains data on metrics such as epoch, losses, and validation metrics.\n",
    "    \"\"\"\n",
    "    log_file_path = os.path.join(directory, \"training_log.txt\")\n",
    "    \n",
    "    lines = [\n",
    "        f\"Epoch {data['epoch']} | Time: {data['time_elapsed'][0]}m {data['time_elapsed'][1]}s | LR: {data['learning_rate']:.10f}\\n\",\n",
    "        f\"Train Loss: {data['train_loss']:.4f} | Classifier: {data['classifier_loss']:.4f} | Box Reg: {data['box_reg_loss']:.4f}\\n\",\n",
    "        f\"Objectness: {data['objectness_loss']:.4f} | RPN Box Reg: {data['rpn_box_reg_loss']:.4f}\\n\",\n",
    "        f\"Validation Metrics: | mAP@IoU=0.50:0.95: {data['val_metrics'][0]:.4f} | mAP@IoU=0.50: {data['val_metrics'][1]:.4f} | mAP@IoU=0.75: {data['val_metrics'][2]:.4f}\\n\",\n",
    "        f\"Small mAP: {data['val_metrics'][3]:.4f} | Medium mAP: {data['val_metrics'][4]:.4f} | Large mAP: {data['val_metrics'][5]:.4f}\\n\\n\"\n",
    "    ]\n",
    "    with open(log_file_path, \"a\") as log_file:\n",
    "        log_file.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        # Check image type\n",
    "        if not isinstance(image, (torch.Tensor, Image.Image)):\n",
    "            raise TypeError(f\"Unsupported image type: {type(image)}. Expected torch.Tensor or PIL.Image.\")\n",
    "        \n",
    "        if torch.rand(1) < self.prob:\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                width = image.shape[-1]\n",
    "                image = F.hflip(image)\n",
    "            else:\n",
    "                width, _ = image.size\n",
    "                image = F.hflip(image)\n",
    "            \n",
    "            # Flip bounding boxes\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]  # Flip x-coordinates\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n",
    "\n",
    "class RandomBrightnessCont(object):\n",
    "    def __init__(self, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5):\n",
    "        self.color_jitter = ColorJitter(brightness, contrast, saturation, hue)\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            image = self.color_jitter(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomBlur(object):\n",
    "    def __init__(self, kernel_size=3, p=0.5):\n",
    "        self.blur = GaussianBlur(kernel_size)\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            image = self.blur(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomRotate(object):\n",
    "    def __init__(self, angle_range=10, p=0.5):\n",
    "        self.angle_range = angle_range\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        # Check image type\n",
    "        if not isinstance(image, (torch.Tensor, Image.Image)):\n",
    "            raise TypeError(f\"Unsupported image type: {type(image)}. Expected torch.Tensor or PIL.Image.\")\n",
    "\n",
    "        if random.random() < self.p:\n",
    "            angle = random.uniform(-self.angle_range, self.angle_range)\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                image = F.to_pil_image(image)\n",
    "                image = F.rotate(image, angle)\n",
    "                image = F.to_tensor(image)\n",
    "            else:\n",
    "                image = F.rotate(image, angle)\n",
    "\n",
    "            # Rotate bounding boxes (as in your code)\n",
    "            boxes = target['boxes']\n",
    "            if len(boxes) > 0:\n",
    "                # Rotate logic here\n",
    "                pass  # (Keep your rotation logic)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    \n",
    "class RandomZoom(object):\n",
    "    def __init__(self, zoom_range=(1.0, 2.0), p=0.5):\n",
    "        self.zoom_range = zoom_range\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            boxes = target['boxes']\n",
    "            if len(boxes) == 0:\n",
    "                return image, target\n",
    "\n",
    "            box_idx = random.randint(0, len(boxes) - 1)\n",
    "            x1, y1, x2, y2 = boxes[box_idx].numpy()\n",
    "\n",
    "            width, height = image.size\n",
    "            center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "            box_width, box_height = x2 - x1, y2 - y1\n",
    "\n",
    "            zoom_factor = random.uniform(*self.zoom_range)\n",
    "            crop_width = box_width / zoom_factor\n",
    "            crop_height = box_height / zoom_factor\n",
    "\n",
    "            crop_x1 = max(0, center_x - crop_width / 2)\n",
    "            crop_y1 = max(0, center_y - crop_height / 2)\n",
    "            crop_x2 = min(width, center_x + crop_width / 2)\n",
    "            crop_y2 = min(height, center_y + crop_height / 2)\n",
    "\n",
    "            image = image.crop((int(crop_x1), int(crop_y1), int(crop_x2), int(crop_y2)))\n",
    "            target['boxes'][:, [0, 2]] -= crop_x1\n",
    "            target['boxes'][:, [1, 3]] -= crop_y1\n",
    "            target['boxes'][:, [0, 2]] = target['boxes'][:, [0, 2]].clamp(0, crop_x2 - crop_x1)\n",
    "            target['boxes'][:, [1, 3]] = target['boxes'][:, [1, 3]].clamp(0, crop_y2 - crop_y1)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    \n",
    "def get_augmented_transform(train):\n",
    "    \"\"\"\n",
    "    Get transform pipeline with augmentations for training or validation\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    \n",
    "    if train:\n",
    "        # Applies a series of data augmentations specifically for the training set\n",
    "        transforms.extend([\n",
    "            RandomHorizontalFlip(0.5),  # Horizontally flips the image with a 50% probability\n",
    "            RandomBrightnessCont(  # Adjusts brightness, contrast, saturation, and hue with specified ranges\n",
    "                brightness=0.3, \n",
    "                contrast=0.4, \n",
    "                saturation=0.5, \n",
    "                hue=0.5, \n",
    "                p=0.5  # Applies these adjustments with a 50% probability\n",
    "            ),\n",
    "            RandomBlur(kernel_size=3, p=0.3),  # Applies Gaussian blur with a kernel size of 3, 30% chance\n",
    "            # RandomRotate(angle_range=10, p=0.3),  # Rotates the image by -10 to +10 degrees, 30% chance\n",
    "            RandomZoom(zoom_range=(0.05, 0.9), p=0.6)  # Zooms the image by a factor between 0.05 and 0.9, 60% chance\n",
    "        ])\n",
    "\n",
    "    # Converts the image to a tensor for model input\n",
    "    transforms.append(ToTensor())\n",
    "    \n",
    "    return Compose(transforms)\n",
    "\n",
    "def visualize_augmentations(dataset, num_samples=5):\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 4 * num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "        orig_img, orig_target = dataset[idx]\n",
    "        \n",
    "        if isinstance(orig_img, torch.Tensor):\n",
    "            orig_img_np = orig_img.permute(1, 2, 0).numpy()\n",
    "        else:\n",
    "            orig_img_np = np.array(orig_img)\n",
    "        \n",
    "        axes[i, 0].imshow(orig_img_np)\n",
    "        axes[i, 0].set_title('Original')\n",
    "        \n",
    "        aug_img, aug_target = dataset[idx]\n",
    "        aug_img_np = aug_img.permute(1, 2, 0).numpy() if isinstance(aug_img, torch.Tensor) else np.array(aug_img)\n",
    "        \n",
    "        axes[i, 1].imshow(aug_img_np)\n",
    "        axes[i, 1].set_title('Augmented')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your augmentations\n",
    "augmentations = get_augmented_transform(train=True)\n",
    "\n",
    "# Create directories to save augmented data\n",
    "augmented_images_dir = 'data/data_faster_rcnn/train_aug/images'\n",
    "os.makedirs(augmented_images_dir, exist_ok=True)\n",
    "\n",
    "# Remove all files in the augmented_images_dir\n",
    "if os.path.exists(augmented_images_dir):\n",
    "    for file in os.listdir(augmented_images_dir):\n",
    "        file_path = os.path.join(augmented_images_dir, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # Remove the file or symlink\n",
    "            elif os.path.isdir(file_path):\n",
    "                os.rmdir(file_path)  # Remove the directory (if empty)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting file {file_path}: {e}\")\n",
    "else:\n",
    "    print(f\"Directory {augmented_images_dir} does not exist.\")\n",
    "\n",
    "# Define the path for augmented annotations\n",
    "augmented_annotations_file = 'data/data_faster_rcnn/train_aug/annotations/instances_aug.json'\n",
    "\n",
    "# Function to save dictionaries as JSON files\n",
    "def save_json(data, json_path):\n",
    "    with open(json_path, \"w\") as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "# Function to convert tensors to lists for JSON serialization\n",
    "def tensor_to_list(obj):\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.tolist()  # Convert Tensor to list\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in obj.items()}  # Recursively convert dicts\n",
    "    elif isinstance(obj, list):\n",
    "        return [tensor_to_list(item) for item in obj]  # Recursively convert lists\n",
    "    else:\n",
    "        return obj  # Return other types as is\n",
    "\n",
    "# Check if the augmented annotations file exists, if not create it\n",
    "if os.path.exists(augmented_annotations_file):\n",
    "    os.remove(augmented_annotations_file)  # Delete the existing file\n",
    "    print(f\"Deleted existing annotations file: {augmented_annotations_file}\")\n",
    "    \n",
    "    \n",
    "# Create a default structure for the augmented annotations\n",
    "categories_list = [\n",
    "    {\"id\": 1, \"name\": \"hazmat code\"}\n",
    "]\n",
    "\n",
    "augmented_annotations = {\n",
    "    'images': [],\n",
    "    'annotations': [],\n",
    "    'categories': categories_list\n",
    "}\n",
    "save_json(augmented_annotations, augmented_annotations_file)\n",
    "print(f\"Created new annotations file: {augmented_annotations_file}\")\n",
    "\n",
    "\n",
    "# Counter for image and annotation IDs\n",
    "img_id = len(augmented_annotations['images'])\n",
    "ann_id = len(augmented_annotations['annotations'])\n",
    "\n",
    "# Number of augmented images to create\n",
    "num_images_to_generate = 1000  # Set this to your desired limit\n",
    "\n",
    "# Iterate through the dataset with progress tracking\n",
    "generated_count = 0  # Counter for how many images we have generated\n",
    "for idx in tqdm(range(len(original_dataset)), desc=\"Augmenting dataset\"):\n",
    "    if generated_count >= num_images_to_generate:\n",
    "        break  # Stop when the desired number of augmented images is created\n",
    "    \n",
    "    img, target = original_dataset[idx]\n",
    "    \n",
    "    # Apply augmentations\n",
    "    aug_img, aug_target = augmentations(img, target)\n",
    "    \n",
    "    # Save augmented image\n",
    "    aug_img_path = os.path.join(augmented_images_dir, f'aug_{img_id}.jpg')\n",
    "    if isinstance(aug_img, torch.Tensor):\n",
    "        aug_img = F.to_pil_image(aug_img)\n",
    "    aug_img.save(aug_img_path)\n",
    "    \n",
    "    # Update image annotation\n",
    "    augmented_annotations['images'].append({\n",
    "        'id': img_id,\n",
    "        'file_name': os.path.basename(aug_img_path),\n",
    "        'width': aug_img.width,\n",
    "        'height': aug_img.height\n",
    "    })\n",
    "    \n",
    "    # Update annotations\n",
    "    for box, label in zip(aug_target['boxes'], aug_target['labels']):\n",
    "        augmented_annotations['annotations'].append({\n",
    "            'id': ann_id,\n",
    "            'image_id': img_id,\n",
    "            'category_id': label.item(),\n",
    "            'bbox': [\n",
    "                box[0].item(), box[1].item(), \n",
    "                box[2].item() - box[0].item(), \n",
    "                box[3].item() - box[1].item()\n",
    "            ],\n",
    "            'area': (box[2] - box[0]) * (box[3] - box[1]),\n",
    "            'iscrowd': 0\n",
    "        })\n",
    "        ann_id += 1\n",
    "\n",
    "    # Update the image and annotation IDs\n",
    "    img_id += 1\n",
    "    generated_count += 1  # Increase the generated image counter\n",
    "\n",
    "# Convert tensors to lists before saving the annotations\n",
    "augmented_annotations = tensor_to_list(augmented_annotations)\n",
    "\n",
    "# Save augmented annotations\n",
    "save_json(augmented_annotations, augmented_annotations_file)\n",
    "\n",
    "print(f\"Generated {generated_count} augmented images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Step\n",
    "def visualize_augmented_data(annotations_file, images_dir, num_to_display=5):\n",
    "    \"\"\"\n",
    "    Visualizes augmented images with bounding boxes.\n",
    "    \"\"\"\n",
    "    # Load annotations\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        augmented_annotations = json.load(f)\n",
    "    \n",
    "    # Get image and annotation mapping\n",
    "    images = {img['id']: img for img in augmented_annotations['images']}\n",
    "    annotations = augmented_annotations['annotations']\n",
    "    \n",
    "    # Group annotations by image_id\n",
    "    annotations_by_image = {}\n",
    "    for ann in annotations:\n",
    "        img_id = ann['image_id']\n",
    "        if img_id not in annotations_by_image:\n",
    "            annotations_by_image[img_id] = []\n",
    "        annotations_by_image[img_id].append(ann)\n",
    "    \n",
    "    # Visualize specified number of images\n",
    "    for img_id, img_info in list(images.items())[:num_to_display]:\n",
    "        img_path = os.path.join(images_dir, img_info['file_name'])\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Plot image\n",
    "        fig, ax = plt.subplots(1, figsize=(10, 8))\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Image ID: {img_id}\")\n",
    "        \n",
    "        # Plot bounding boxes\n",
    "        for ann in annotations_by_image.get(img_id, []):\n",
    "            bbox = ann['bbox']\n",
    "            rect = patches.Rectangle(\n",
    "                (bbox[0], bbox[1]), bbox[2], bbox[3], \n",
    "                linewidth=2, edgecolor='red', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(\n",
    "                bbox[0], bbox[1] - 5, f\"Hazmat Code\", \n",
    "                color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.8)\n",
    "            )\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Call the visualization function\n",
    "visualize_augmented_data(\n",
    "    annotations_file=augmented_annotations_file,\n",
    "    images_dir=augmented_images_dir,\n",
    "    num_to_display=200  # Adjust this number to control how many images to display\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mock = {\n",
    "    \"epoch\": 5,\n",
    "    \"time_elapsed\": [12, 34],  # 12 minutes, 34 seconds\n",
    "    \"learning_rate\": 0.000123,\n",
    "    \"train_loss\": 0.5678,\n",
    "    \"classifier_loss\": 0.1234,\n",
    "    \"box_reg_loss\": 0.2345,\n",
    "    \"objectness_loss\": 0.3456,\n",
    "    \"rpn_box_reg_loss\": 0.4567,\n",
    "    \"val_metrics\": [0.6543, 0.8765, 0.7654, 0.1234, 0.2345, 0.3456]  # Mocked mAP values\n",
    "}\n",
    "\n",
    "save_epoch_data(\"./data\", data_mock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 36090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "print(f\"Loading model on {device}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/train_aug',\n",
    "    annotations_file='data/data_faster_rcnn/train/annotations/instances_train.json',\n",
    "    transforms=get_augmented_transform(train=True)\n",
    ")\n",
    "\n",
    "val_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/val',\n",
    "    annotations_file='data/data_faster_rcnn/val/annotations/instances_val.json',\n",
    "    transforms=get_augmented_transform(train=False)\n",
    ")\n",
    "\n",
    "# Set the percentage of the training dataset to use (e.g. 0.x to 1)\n",
    "train_percentage = 0.01\n",
    "\n",
    "# Create a subset of the training dataset\n",
    "train_dataset_subset = create_subset(train_dataset, train_percentage)\n",
    "\n",
    "# Set the percentage of the val dataset to use (e.g. 0.x to 1)\n",
    "val_percentage = 1\n",
    "\n",
    "# Create a subset of the training dataset\n",
    "val_dataset_subset = create_subset(val_dataset, val_percentage)\n",
    "\n",
    "# amount of cpu cores\n",
    "workers = 2\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_subset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_subset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 2  # hazmat code and background\n",
    "\n",
    "# Create ResNet-101 backbone with FPN\n",
    "backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
    "\n",
    "# Define anchor generator for FPN\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32,), (64,), (128,), (256,), (512,)),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    ")\n",
    "\n",
    "# Multi-scale RoI pooling for FPN\n",
    "roi_pooler = MultiScaleRoIAlign(\n",
    "    featmap_names=['0', '1', '2', '3', '4'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2\n",
    ")\n",
    "\n",
    "print(\"initializing model...\")\n",
    "# Initialize Faster R-CNN with ResNet-101-FPN\n",
    "model = FasterRCNN(\n",
    "    backbone=backbone,\n",
    "    num_classes=num_classes,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ")\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "print(f\"Training model on {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# visualize_augmentations(train_dataset, num_samples=5)\n",
    "\n",
    "# save the model as file so I can train it later and then destroy kernel session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kill -9 48787\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_data = torch.device('cuda:1')\n",
    "print(f\"Training model on {device}\")\n",
    "scaler = GradScaler()\n",
    "# Initialize optimizer and scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "train_metrics_map = []\n",
    "best_val_map = float('-inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Create directory to store models and logs\n",
    "directory_finetuned_model = create_directory()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    best_val_map = train_model(\n",
    "        directory=directory_finetuned_model, \n",
    "        model=model, optimizer=optimizer, train_loader=train_loader, device=device, \n",
    "        train_metrics_list=train_metrics_map, best_val_map=best_val_map, lr_scheduler=lr_scheduler, \n",
    "        val_loader=val_loader, coco_val=coco_val, scaler=scaler, epoch=epoch\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dev)",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
