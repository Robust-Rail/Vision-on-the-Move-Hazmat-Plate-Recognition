{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import contextlib\n",
    "import io\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import ColorJitter, GaussianBlur\n",
    "import random\n",
    "import matplotlib.patches as patches\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if cuda is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HazmatDataset(Dataset):\n",
    "    def __init__(self, data_dir, annotations_file, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Load annotations\n",
    "        with open(annotations_file) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.images = {img['id']: img for img in data['images']}\n",
    "        self.annotations = data['annotations']\n",
    "\n",
    "        # Create image_id to annotations mapping\n",
    "        self.img_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_to_anns:\n",
    "                self.img_to_anns[img_id] = []\n",
    "            self.img_to_anns[img_id].append(ann)\n",
    "\n",
    "        self.ids = list(self.images.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.data_dir, 'images', img_info['file_name'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Get annotations\n",
    "        anns = self.img_to_anns.get(img_id, [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "\n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']\n",
    "            # Convert [x, y, w, h] to [x1, y1, x2, y2]\n",
    "            x_min = bbox[0]\n",
    "            y_min = bbox[1]\n",
    "            x_max = bbox[0] + bbox[2]\n",
    "            y_max = bbox[1] + bbox[3]\n",
    "\n",
    "            # Filter out degenerate boxes\n",
    "            if x_max > x_min and y_max > y_min:\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(ann['category_id'])\n",
    "                areas.append(ann['area'])\n",
    "                iscrowd.append(ann['iscrowd'])\n",
    "\n",
    "        # Convert to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': areas,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        # Convert PIL image to tensor\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_classifier_loss = 0\n",
    "    total_box_reg_loss = 0\n",
    "    total_objectness_loss = 0\n",
    "    total_rpn_box_reg_loss = 0\n",
    "\n",
    "    # Voeg tqdm toe om de voortgang te tonen\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=True)\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Wrap the forward pass in autocast\n",
    "        with autocast(device_type='cuda'):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Scale the loss and call backward\n",
    "        scaler.scale(losses).backward()\n",
    "        # Unscales the gradients and calls or skips optimizer.step()\n",
    "        scaler.step(optimizer)\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        # Bereken de totalen\n",
    "        total_loss += losses.item()\n",
    "        total_classifier_loss += loss_dict['loss_classifier'].item()\n",
    "        total_box_reg_loss += loss_dict['loss_box_reg'].item()\n",
    "        total_objectness_loss += loss_dict['loss_objectness'].item()\n",
    "        total_rpn_box_reg_loss += loss_dict['loss_rpn_box_reg'].item()\n",
    "\n",
    "        # Update tqdm-balk\n",
    "        progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{losses.item():.4f}\",\n",
    "            \"Classifier\": f\"{loss_dict['loss_classifier'].item():.4f}\",\n",
    "            \"BoxReg\": f\"{loss_dict['loss_box_reg'].item():.4f}\",\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_classifier_loss = total_classifier_loss / len(data_loader)\n",
    "    avg_box_reg_loss = total_box_reg_loss / len(data_loader)\n",
    "    avg_objectness_loss = total_objectness_loss / len(data_loader)\n",
    "    avg_rpn_box_reg_loss = total_rpn_box_reg_loss / len(data_loader)\n",
    "\n",
    "    return avg_loss, avg_classifier_loss, avg_box_reg_loss, avg_objectness_loss, avg_rpn_box_reg_loss\n",
    "\n",
    "\n",
    "\n",
    "# Load ground truth annotations\n",
    "coco_val = COCO('data/data_faster_rcnn/val/annotations/instances_val.json')\n",
    "\n",
    "# Prepare predictions in COCO format\n",
    "# Assuming you have a function to convert model outputs to COCO format\n",
    "# Conversion to COCO Format\n",
    "def convert_to_coco_format(outputs, image_ids):\n",
    "    coco_results = []\n",
    "    for output, image_id in zip(outputs, image_ids):\n",
    "        boxes = output['boxes'].cpu().numpy()\n",
    "        scores = output['scores'].cpu().numpy()\n",
    "        labels = output['labels'].cpu().numpy()\n",
    "        \n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            coco_results.append({\n",
    "                'image_id': image_id,\n",
    "                'category_id': int(label),\n",
    "                'bbox': [box[0], box[1], box[2] - box[0], box[3] - box[1]],\n",
    "                'score': float(score)\n",
    "            })\n",
    "    return coco_results\n",
    "\n",
    "# Validation Function\n",
    "def validate(model, data_loader, coco_gt, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    # Add tqdm\n",
    "    progress_bar = tqdm(data_loader, desc=\"Validation\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in progress_bar:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            image_ids = [target['image_id'].item() for target in targets]\n",
    "            coco_results = convert_to_coco_format(outputs, image_ids)\n",
    "            results.extend(coco_results)\n",
    "\n",
    "            # Update tqdm-bar\n",
    "            progress_bar.set_postfix({\"Processed\": len(results)})\n",
    "\n",
    "    if not results:\n",
    "        print(\"No predictions generated. Skipping evaluation.\")\n",
    "        return [0.0] * 6  # Return dummy metrics for empty results\n",
    "\n",
    "    # Suppress COCOeval output\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        coco_dt = coco_gt.loadRes(results)\n",
    "        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "    return coco_eval.stats\n",
    "\n",
    "\n",
    "# Custom backbone to return a dictionary of feature maps\n",
    "class BackboneWithChannels(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return {'0': x}\n",
    "    \n",
    "# Function to create a subset of the dataset\n",
    "def create_subset(dataset, percentage):\n",
    "    \"\"\"\n",
    "    Create a subset of the dataset based on the given percentage.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: The full dataset.\n",
    "    - percentage: The fraction of the dataset to use (value between 0.0 and 1.0).\n",
    "    \n",
    "    Returns:\n",
    "    - subset: A subset of the dataset containing the specified percentage of data.\n",
    "    \"\"\"\n",
    "    if not (0.0 < percentage <= 1.0):\n",
    "        raise ValueError(\"Percentage must be between 0.0 and 1.0.\")\n",
    "    \n",
    "    # Determine the subset size\n",
    "    total_samples = len(dataset)\n",
    "    subset_size = int(total_samples * percentage)\n",
    "    \n",
    "    # Shuffle and select a random subset of indices\n",
    "    indices = list(range(total_samples))\n",
    "    random.shuffle(indices)\n",
    "    subset_indices = indices[:subset_size]\n",
    "    \n",
    "    return Subset(dataset, subset_indices)\n",
    "import os\n",
    "import re\n",
    "\n",
    "def sanitize_directory_name(name):\n",
    "    \"\"\"Vervang ongeldige tekens in mapnamen voor Windows\"\"\"\n",
    "    # Lijst van verboden tekens: < > : \" / \\ | ? *\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '-', name)\n",
    "\n",
    "def create_directory(base_path=\"data/models/\"):\n",
    "    \"\"\"\n",
    "    Create a directory inside the base path named 'faster-rcnn-finetuned-{date}' \n",
    "    to store models and logs. The name includes the current date and time in the format 'DD-MM-YYYY HH:MM:SS'.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Base directory where the new directory will be created.\n",
    "\n",
    "    Returns:\n",
    "    - directory_path (str): Full path to the created directory.\n",
    "    \"\"\"\n",
    "    # Get the current date and time\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "    \n",
    "    # Define the full directory path\n",
    "    directory_name = f\"faster-rcnn-finetuned-{current_time}\"\n",
    "    directory_name = sanitize_directory_name(directory_name)\n",
    "    directory_path = os.path.join(base_path, directory_name)\n",
    "\n",
    "    # Create the directory\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Directory created: {directory_path}\")\n",
    "    return directory_path\n",
    "\n",
    "def train_model(directory, model, optimizer, train_loader, device, train_metrics_list, best_val_map, lr_scheduler, val_loader, coco_val, scaler, epoch):\n",
    "    \n",
    "    epoch+=1\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_classifier_loss, train_box_reg_loss, train_objectness_loss, train_rpn_box_reg_loss = train_one_epoch(\n",
    "        model, optimizer, train_loader, device, scaler)\n",
    "    \n",
    "    # Validate and get all COCO-metrics\n",
    "    val_metrics = validate(model, val_loader, coco_val, device)\n",
    "    val_map = val_metrics[0]  # mAP@IoU=0.50:0.95\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    \n",
    "    # Obtain the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Prepare data for logging\n",
    "    data = {\n",
    "        \"epoch\": epoch,\n",
    "        \"time_elapsed\": (int(minutes), int(seconds)),\n",
    "        \"learning_rate\": current_lr,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"classifier_loss\": train_classifier_loss,\n",
    "        \"box_reg_loss\": train_box_reg_loss,\n",
    "        \"objectness_loss\": train_objectness_loss,\n",
    "        \"rpn_box_reg_loss\": train_rpn_box_reg_loss,\n",
    "        \"val_metrics\": val_metrics\n",
    "    }\n",
    "    \n",
    "    # Append current epoch data to metrics list\n",
    "    train_metrics_list.append(data)\n",
    "    \n",
    "    # Print summary for this epoch\n",
    "    print(f\"📊 Epoch {epoch} | ⏳ Time: {int(minutes)}m {int(seconds)}s | 🔄 LR: {current_lr:.6f}\")\n",
    "    print(f\"📉 Train Loss: {train_loss:.4f} | 🎯 Classifier: {train_classifier_loss:.4f} | 📦 Box Reg: {train_box_reg_loss:.4f}\")\n",
    "    print(f\"🔍 Objectness: {train_objectness_loss:.4f} | 🗂️ RPN Box Reg: {train_rpn_box_reg_loss:.4f}\")\n",
    "    print(f\"🧪 mAP | 🟢 mAP@IoU=0.50:0.95: {val_metrics[0]:.4f} | 🔵 mAP@IoU=0.50: {val_metrics[1]:.4f} | 🟣 mAP@IoU=0.75: {val_metrics[2]:.4f}\")\n",
    "    print(f\"📏 Small mAP: {val_metrics[3]:.4f} | 📐 Medium mAP: {val_metrics[4]:.4f} | 📏 Large mAP: {val_metrics[5]:.4f}\")\n",
    "    \n",
    "    # Save epoch data to a log file\n",
    "    save_epoch_data(directory, data)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save the latest checkpoint with all metrics\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_map': val_map,\n",
    "        'train_metrics_list': train_metrics_list  # Save all metrics\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(directory, \"latest_model.pth\"))\n",
    "    \n",
    "    # Save the best model if the val_map is the highest so far\n",
    "    if val_map > best_val_map:\n",
    "        best_val_map = val_map\n",
    "        torch.save(checkpoint, os.path.join(directory, \"best_model.pth\"))\n",
    "    \n",
    "    return best_val_map\n",
    "        \n",
    "\n",
    "\n",
    "def save_epoch_data(directory, data):\n",
    "    \"\"\"\n",
    "    Save training statistics for each epoch in a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): Path to the directory.\n",
    "    - data (dict): Contains data on metrics such as epoch, losses, and validation metrics.\n",
    "    \"\"\"\n",
    "    log_file_path = os.path.join(directory, \"training_log.txt\")\n",
    "    \n",
    "    lines = [\n",
    "        f\"Epoch {data['epoch']} | Time: {data['time_elapsed'][0]}m {data['time_elapsed'][1]}s | LR: {data['learning_rate']:.10f}\\n\",\n",
    "        f\"Train Loss: {data['train_loss']:.4f} | Classifier: {data['classifier_loss']:.4f} | Box Reg: {data['box_reg_loss']:.4f}\\n\",\n",
    "        f\"Objectness: {data['objectness_loss']:.4f} | RPN Box Reg: {data['rpn_box_reg_loss']:.4f}\\n\",\n",
    "        f\"Validation Metrics: | mAP@IoU=0.50:0.95: {data['val_metrics'][0]:.4f} | mAP@IoU=0.50: {data['val_metrics'][1]:.4f} | mAP@IoU=0.75: {data['val_metrics'][2]:.4f}\\n\",\n",
    "        f\"Small mAP: {data['val_metrics'][3]:.4f} | Medium mAP: {data['val_metrics'][4]:.4f} | Large mAP: {data['val_metrics'][5]:.4f}\\n\\n\"\n",
    "    ]\n",
    "    with open(log_file_path, \"a\") as log_file:\n",
    "        log_file.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        # Check image type\n",
    "        if not isinstance(image, (torch.Tensor, Image.Image)):\n",
    "            raise TypeError(f\"Unsupported image type: {type(image)}. Expected torch.Tensor or PIL.Image.\")\n",
    "        \n",
    "        if torch.rand(1) < self.prob:\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                width = image.shape[-1]\n",
    "                image = F.hflip(image)\n",
    "            else:\n",
    "                width, _ = image.size\n",
    "                image = F.hflip(image)\n",
    "            \n",
    "            # Flip bounding boxes\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]  # Flip x-coordinates\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n",
    "\n",
    "class RandomBrightnessCont(object):\n",
    "    def __init__(self, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5):\n",
    "        self.color_jitter = ColorJitter(brightness, contrast, saturation, hue)\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            image = self.color_jitter(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomBlur(object):\n",
    "    def __init__(self, kernel_size=3, p=0.5):\n",
    "        self.blur = GaussianBlur(kernel_size)\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            image = self.blur(image)\n",
    "        return image, target\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "class RandomRotate(object):\n",
    "    def __init__(self, angle_range=10, p=0.5):\n",
    "        self.angle_range = angle_range\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() >= self.p:\n",
    "            return image, target\n",
    "\n",
    "        angle = random.uniform(-self.angle_range, self.angle_range)\n",
    "        original_width, original_height = self._get_image_size(image)\n",
    "\n",
    "        # Rotate image with expansion to get new dimensions\n",
    "        image_pil = image if isinstance(image, Image.Image) else F.to_pil_image(image)\n",
    "        image_pil_rotated = F.rotate(image_pil, angle, expand=True)\n",
    "        new_width, new_height = image_pil_rotated.size\n",
    "\n",
    "        # Convert back to tensor if needed\n",
    "        image = F.to_tensor(image_pil_rotated) if isinstance(image, torch.Tensor) else image_pil_rotated\n",
    "\n",
    "        # Rotate bounding boxes\n",
    "        boxes = target['boxes']\n",
    "        if len(boxes) == 0:\n",
    "            return image, target\n",
    "\n",
    "        # Compute rotation matrix with expansion offset\n",
    "        cx_orig = original_width / 2\n",
    "        cy_orig = original_height / 2\n",
    "\n",
    "        # Calculate expansion offset (min_x, min_y)\n",
    "        corners_original = torch.tensor([\n",
    "            [0, 0],\n",
    "            [original_width, 0],\n",
    "            [original_width, original_height],\n",
    "            [0, original_height]\n",
    "        ])\n",
    "        corners_rotated = self._rotate_points(corners_original, -angle, (cx_orig, cy_orig))\n",
    "        min_x = corners_rotated[:, 0].min()\n",
    "        min_y = corners_rotated[:, 1].min()\n",
    "\n",
    "        # Rotate and translate box corners\n",
    "        boxes_rotated = []\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            corners = torch.tensor([\n",
    "                [x1, y1], [x2, y1], [x2, y2], [x1, y2]\n",
    "            ])\n",
    "            corners_rot = self._rotate_points(corners, -angle, (cx_orig, cy_orig))\n",
    "            corners_rot -= torch.tensor([[min_x, min_y]])  # Adjust for expansion\n",
    "\n",
    "            # Clamp to new image bounds\n",
    "            x_min = max(0.0, corners_rot[:, 0].min().item())\n",
    "            y_min = max(0.0, corners_rot[:, 1].min().item())\n",
    "            x_max = min(new_width, corners_rot[:, 0].max().item())\n",
    "            y_max = min(new_height, corners_rot[:, 1].max().item())\n",
    "\n",
    "            if x_max > x_min and y_max > y_min:\n",
    "                boxes_rotated.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        target['boxes'] = torch.tensor(boxes_rotated, dtype=torch.float32) if boxes_rotated else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        return image, target\n",
    "\n",
    "    def _rotate_points(self, points, angle, center):\n",
    "        angle_rad = math.radians(angle)\n",
    "        cos_theta = math.cos(angle_rad)\n",
    "        sin_theta = math.sin(angle_rad)\n",
    "        cx, cy = center\n",
    "\n",
    "        # Translate points to origin\n",
    "        translated = points - torch.tensor([[cx, cy]])\n",
    "\n",
    "        # Apply rotation\n",
    "        x_rot = translated[:, 0] * cos_theta - translated[:, 1] * sin_theta\n",
    "        y_rot = translated[:, 0] * sin_theta + translated[:, 1] * cos_theta\n",
    "\n",
    "        # Translate back\n",
    "        rotated_points = torch.stack([x_rot + cx, y_rot + cy], dim=1)\n",
    "        return rotated_points\n",
    "\n",
    "    def _get_image_size(self, image):\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            return image.shape[-1], image.shape[-2]\n",
    "        elif isinstance(image, Image.Image):\n",
    "            return image.size\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported image type.\")\n",
    "    \n",
    "class RandomZoom(object):\n",
    "    def __init__(self, zoom_range=(1.0, 2.0), p=0.5):\n",
    "        self.zoom_range = zoom_range\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            boxes = target['boxes']\n",
    "            if len(boxes) == 0:\n",
    "                return image, target\n",
    "\n",
    "            box_idx = random.randint(0, len(boxes) - 1)\n",
    "            x1, y1, x2, y2 = boxes[box_idx].numpy()\n",
    "\n",
    "            width, height = image.size\n",
    "            center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "            box_width, box_height = x2 - x1, y2 - y1\n",
    "\n",
    "            zoom_factor = random.uniform(*self.zoom_range)\n",
    "            crop_width = box_width / zoom_factor\n",
    "            crop_height = box_height / zoom_factor\n",
    "\n",
    "            crop_x1 = max(0, center_x - crop_width / 2)\n",
    "            crop_y1 = max(0, center_y - crop_height / 2)\n",
    "            crop_x2 = min(width, center_x + crop_width / 2)\n",
    "            crop_y2 = min(height, center_y + crop_height / 2)\n",
    "\n",
    "            image = image.crop((int(crop_x1), int(crop_y1), int(crop_x2), int(crop_y2)))\n",
    "            target['boxes'][:, [0, 2]] -= crop_x1\n",
    "            target['boxes'][:, [1, 3]] -= crop_y1\n",
    "            target['boxes'][:, [0, 2]] = target['boxes'][:, [0, 2]].clamp(0, crop_x2 - crop_x1)\n",
    "            target['boxes'][:, [1, 3]] = target['boxes'][:, [1, 3]].clamp(0, crop_y2 - crop_y1)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    \n",
    "def get_augmented_transform(train):\n",
    "    \"\"\"\n",
    "    Get transform pipeline with augmentations for training or validation\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    \n",
    "    if train:\n",
    "        # Applies a series of data augmentations specifically for the training set\n",
    "        transforms.extend([\n",
    "            RandomHorizontalFlip(0.5),  # Horizontally flips the image with a 50% probability\n",
    "            RandomBrightnessCont(  # Adjusts brightness, contrast, saturation, and hue with specified ranges\n",
    "                brightness=0.3, \n",
    "                contrast=0.4, \n",
    "                saturation=0.5, \n",
    "                hue=0.5, \n",
    "                p=0.5  # Applies these adjustments with a 50% probability\n",
    "            ),\n",
    "            RandomBlur(kernel_size=3, p=0.5),  # Applies Gaussian blur with a kernel size of 3, 30% chance\n",
    "            RandomRotate(angle_range=50, p=0.5),  # Rotates the image by -10 to +10 degrees, 30% chance\n",
    "            RandomZoom(zoom_range=(0.05, 0.99), p=0.6)  # Zooms the image by a factor between 0.05 and 0.9, 60% chance\n",
    "        ])\n",
    "\n",
    "    # Converts the image to a tensor for model input\n",
    "    transforms.append(ToTensor())\n",
    "    \n",
    "    return Compose(transforms)\n",
    "\n",
    "def visualize_augmentations(dataset, num_samples=5):\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 4 * num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "        orig_img, orig_target = dataset[idx]\n",
    "        \n",
    "        if isinstance(orig_img, torch.Tensor):\n",
    "            orig_img_np = orig_img.permute(1, 2, 0).numpy()\n",
    "        else:\n",
    "            orig_img_np = np.array(orig_img)\n",
    "        \n",
    "        axes[i, 0].imshow(orig_img_np)\n",
    "        axes[i, 0].set_title('Original')\n",
    "        \n",
    "        aug_img, aug_target = dataset[idx]\n",
    "        aug_img_np = aug_img.permute(1, 2, 0).numpy() if isinstance(aug_img, torch.Tensor) else np.array(aug_img)\n",
    "        \n",
    "        axes[i, 1].imshow(aug_img_np)\n",
    "        axes[i, 1].set_title('Augmented')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make a function to visualise the data augmentation\n",
    "dataset = HazmatDataset('data/data_faster_rcnn/train', 'data/data_faster_rcnn/train/annotations/instances_train.json', get_augmented_transform(train=True))\n",
    "visualize_augmentations(dataset, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.patches import Rectangle\n",
    "dataset = HazmatDataset('data/data_faster_rcnn/train', 'data/data_faster_rcnn/train/annotations/instances_train.json', get_augmented_transform(train=True))\n",
    "def visualize_augmentations(\n",
    "    dataset, \n",
    "    num_samples=5, \n",
    "    bbox_format='xyxy', \n",
    "    denormalize_boxes=False, \n",
    "    box_color='red', \n",
    "    line_width=2, \n",
    "    get_boxes=lambda target: target.get('boxes', []), \n",
    "    denormalize_img=None, \n",
    "    seed=None,\n",
    "    figsize=(15, 8)\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize original and augmented images with annotations.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset: Dataset object returning tuples (image, target).\n",
    "        num_samples: Number of sample pairs to display.\n",
    "        bbox_format: Bounding box format ('xyxy' or 'xywh').\n",
    "        denormalize_boxes: Whether to denormalize box coordinates.\n",
    "        box_color: Color for bounding boxes.\n",
    "        line_width: Line width for bounding boxes.\n",
    "        get_boxes: Function to extract boxes from target.\n",
    "        denormalize_img: Tuple (mean, std) to reverse image normalization.\n",
    "        seed: Random seed for reproducibility.\n",
    "        figsize: Figure size.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=figsize)\n",
    "    fig.suptitle('Original vs Augmented Images with Annotations', fontsize=16, y=1.02)\n",
    "    \n",
    "    # Handle single sample case\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "        \n",
    "        original_dataset = HazmatDataset('data/data_faster_rcnn/test', 'data/data_faster_rcnn/test/annotations/instances_test.json', get_augmented_transform(train=False))\n",
    "        # Get data pairs\n",
    "        orig_img, orig_target = original_dataset[idx]\n",
    "        aug_img, aug_target = dataset[idx]\n",
    "        \n",
    "        # Process original image\n",
    "        orig_img_np = _process_image(orig_img, denormalize_img)\n",
    "        _draw_image_and_boxes(axes[i, 0], orig_img_np, orig_target, \n",
    "                             f'Sample {i+1} - Original', \n",
    "                             get_boxes, bbox_format, denormalize_boxes, \n",
    "                             box_color, line_width)\n",
    "        \n",
    "        # Process augmented image\n",
    "        aug_img_np = _process_image(aug_img, denormalize_img)\n",
    "        _draw_image_and_boxes(axes[i, 1], aug_img_np, aug_target, \n",
    "                             f'Sample {i+1} - Augmented', \n",
    "                             get_boxes, bbox_format, denormalize_boxes, \n",
    "                             box_color, line_width)\n",
    "        \n",
    "        # Clean up axes\n",
    "        axes[i, 0].axis('off')\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def _process_image(img, denormalize_params):\n",
    "    \"\"\"Convert tensor to numpy and denormalize if needed\"\"\"\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "    else:\n",
    "        img_np = np.array(img)\n",
    "    \n",
    "    if denormalize_params is not None:\n",
    "        mean, std = denormalize_params\n",
    "        img_np = img_np * std + mean\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "    \n",
    "    return img_np\n",
    "\n",
    "def _draw_image_and_boxes(ax, img_np, target, title, \n",
    "                         get_boxes, bbox_format, denormalize_boxes,\n",
    "                         box_color, line_width):\n",
    "    \"\"\"Helper to draw image and bounding boxes\"\"\"\n",
    "    ax.imshow(img_np)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    boxes = get_boxes(target)\n",
    "    if len(boxes) == 0:\n",
    "        return\n",
    "    \n",
    "    h, w = img_np.shape[:2]\n",
    "    boxes = np.array(boxes)\n",
    "    \n",
    "    if denormalize_boxes:\n",
    "        if bbox_format == 'xyxy':\n",
    "            boxes *= np.array([w, h, w, h])\n",
    "        elif bbox_format == 'xywh':\n",
    "            x_center, y_center = boxes[:,0] * w, boxes[:,1] * h\n",
    "            width, height = boxes[:,2] * w, boxes[:,3] * h\n",
    "            x1 = x_center - width/2\n",
    "            y1 = y_center - height/2\n",
    "            boxes = np.stack([x1, y1, width, height], axis=1)\n",
    "            bbox_format = 'xywh'  # Update format for conversion\n",
    "    \n",
    "    # Convert all boxes to xyxy format\n",
    "    if bbox_format == 'xywh':\n",
    "        boxes[:,0] = boxes[:,0] - boxes[:,2]/2\n",
    "        boxes[:,1] = boxes[:,1] - boxes[:,3]/2\n",
    "        boxes[:,2] = boxes[:,0] + boxes[:,2]\n",
    "        boxes[:,3] = boxes[:,1] + boxes[:,3]\n",
    "    \n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = Rectangle(\n",
    "            (x1, y1), x2 - x1, y2 - y1,\n",
    "            linewidth=line_width,\n",
    "            edgecolor=box_color,\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "dataset = HazmatDataset('data/data_faster_rcnn/test', 'data/data_faster_rcnn/test/annotations/instances_test.json', get_augmented_transform(train=True))\n",
    "# Visualize augmentations\n",
    "visualize_augmentations(\n",
    "    dataset, \n",
    "    num_samples=10, \n",
    "    bbox_format='xyxy', \n",
    "    denormalize_boxes=False, \n",
    "    box_color='blue', \n",
    "    line_width=2, \n",
    "    get_boxes=lambda target: target.get('boxes', []), \n",
    "    figsize=(30, 30)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def adjust_bbox_for_transforms(original_bbox, img_width, img_height, transforms_applied):\n",
    "    \"\"\"\n",
    "    Adjust bounding box coordinates based on applied augmentations.\n",
    "    \"\"\"\n",
    "    x, y, w, h = original_bbox\n",
    "    x1, y1 = x, y\n",
    "    x2, y2 = x + w, y + h\n",
    "\n",
    "    for transform in transforms_applied:\n",
    "        if transform[\"name\"] == \"RandomHorizontalFlip\" and transform[\"applied\"]:\n",
    "            # Flip coordinates horizontally\n",
    "            x1 = img_width - x2\n",
    "            x2 = img_width - x\n",
    "            x, w = x1, x2 - x1\n",
    "\n",
    "        elif transform[\"name\"] == \"RandomZoom\" and transform[\"applied\"]:\n",
    "            # Zoom-specific adjustments\n",
    "            zoom_factor = transform[\"zoom_factor\"]\n",
    "            new_width = int(img_width * zoom_factor)\n",
    "            new_height = int(img_height * zoom_factor)\n",
    "            \n",
    "            # Calculate crop coordinates (assuming center crop)\n",
    "            left = (new_width - img_width) // 2\n",
    "            top = (new_height - img_height) // 2\n",
    "            right = left + img_width\n",
    "            bottom = top + img_height\n",
    "            \n",
    "            # Adjust coordinates for zoom and crop\n",
    "            x1 = max(0, x1 * zoom_factor - left)\n",
    "            y1 = max(0, y1 * zoom_factor - top)\n",
    "            x2 = min(img_width, x2 * zoom_factor - left)\n",
    "            y2 = min(img_height, y2 * zoom_factor - top)\n",
    "            \n",
    "            x, y, w, h = x1, y1, x2 - x1, y2 - y1\n",
    "\n",
    "    return [x, y, w, h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # Added tqdm import\n",
    "\n",
    "def export_augmented_dataset(\n",
    "    original_img_dir,\n",
    "    original_json_path,\n",
    "    output_dir,\n",
    "    num_augmentations=3\n",
    "):\n",
    "    # Load original COCO data\n",
    "    with open(original_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(os.path.join(output_dir, \"images\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, \"annotations\"), exist_ok=True)\n",
    "\n",
    "    # Initialize new dataset structure\n",
    "    new_data = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": coco_data[\"categories\"]\n",
    "    }\n",
    "\n",
    "    # Track IDs\n",
    "    max_img_id = max(img[\"id\"] for img in coco_data[\"images\"]) if coco_data[\"images\"] else 0\n",
    "    max_ann_id = max(ann[\"id\"] for ann in coco_data[\"annotations\"]) if coco_data[\"annotations\"] else 0\n",
    "\n",
    "    # Prepare augmentation transforms (without ToTensor())\n",
    "    augmentation_pipeline = get_augmented_transform(train=True)\n",
    "    # Remove ToTensor() from pipeline for image saving\n",
    "    augmentation_pipeline.transforms = [t for t in augmentation_pipeline.transforms \n",
    "                                      if not isinstance(t, ToTensor)]\n",
    "\n",
    "    # Main progress bar for images\n",
    "    for orig_img_info in tqdm(coco_data[\"images\"], desc=\"Processing images\", unit=\"img\"):\n",
    "        # Load original image\n",
    "        img_path = os.path.join(original_img_dir, orig_img_info[\"file_name\"])\n",
    "        original_image = Image.open(img_path).convert(\"RGB\")\n",
    "        orig_width, orig_height = original_image.size\n",
    "\n",
    "        # Get corresponding annotations\n",
    "        original_annots = [ann for ann in coco_data[\"annotations\"] \n",
    "                         if ann[\"image_id\"] == orig_img_info[\"id\"]]\n",
    "\n",
    "        # Save original image and annotations to new dataset\n",
    "        original_output_path = os.path.join(output_dir, \"images\", orig_img_info[\"file_name\"])\n",
    "        original_image.save(original_output_path)\n",
    "        new_data[\"images\"].append(copy.deepcopy(orig_img_info))\n",
    "        for ann in original_annots:\n",
    "            new_ann = copy.deepcopy(ann)\n",
    "            new_data[\"annotations\"].append(new_ann)\n",
    "\n",
    "        # Convert COCO bboxes to x1y1x2y2 format\n",
    "        boxes = []\n",
    "        for ann in original_annots:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "\n",
    "        # Create target dictionary\n",
    "        original_target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor([ann[\"category_id\"] for ann in original_annots], dtype=torch.int64),\n",
    "            \"iscrowd\": torch.tensor([ann[\"iscrowd\"] for ann in original_annots], dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        # Augmentation progress bar\n",
    "        for aug_idx in tqdm(range(num_augmentations), desc=\"Augmenting\", leave=False, unit=\"aug\"):\n",
    "            # Create copies for augmentation\n",
    "            aug_image = original_image.copy()\n",
    "            aug_target = copy.deepcopy(original_target)\n",
    "\n",
    "            # Apply augmentation pipeline\n",
    "            for transform in augmentation_pipeline.transforms:\n",
    "                aug_image, aug_target = transform(aug_image, aug_target)\n",
    "\n",
    "            # Get new dimensions from PIL Image\n",
    "            aug_width, aug_height = aug_image.size\n",
    "\n",
    "            # Generate new filename\n",
    "            base_name = os.path.splitext(orig_img_info[\"file_name\"])[0]\n",
    "            new_filename = f\"{base_name}_aug{aug_idx}.jpg\"\n",
    "            new_img_path = os.path.join(output_dir, \"images\", new_filename)\n",
    "            aug_image.save(new_img_path)\n",
    "\n",
    "            # Create new image entry\n",
    "            max_img_id += 1\n",
    "            new_img_info = {\n",
    "                \"id\": max_img_id,\n",
    "                \"file_name\": new_filename,\n",
    "                \"width\": aug_width,\n",
    "                \"height\": aug_height\n",
    "            }\n",
    "            new_data[\"images\"].append(new_img_info)\n",
    "\n",
    "            # Process annotations\n",
    "            valid_boxes = []\n",
    "            for box, label, iscrowd in zip(aug_target[\"boxes\"].numpy(),\n",
    "                                         aug_target[\"labels\"].numpy(),\n",
    "                                         aug_target[\"iscrowd\"].numpy()):\n",
    "                # Convert back to COCO format\n",
    "                x1, y1, x2, y2 = box\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "\n",
    "                # Filter invalid boxes\n",
    "                if w > 0 and h > 0 and x1 < aug_width and y1 < aug_height:\n",
    "                    valid_boxes.append({\n",
    "                        \"id\": max_ann_id + 1,\n",
    "                        \"image_id\": max_img_id,\n",
    "                        \"category_id\": int(label),\n",
    "                        \"bbox\": [float(x1), float(y1), float(w), float(h)],\n",
    "                        \"area\": float(w * h),\n",
    "                        \"iscrowd\": int(iscrowd)\n",
    "                    })\n",
    "                    max_ann_id += 1\n",
    "\n",
    "            new_data[\"annotations\"].extend(valid_boxes)\n",
    "\n",
    "    # Save new annotations\n",
    "    output_json_path = os.path.join(output_dir, \"annotations\", \"instances_augmented.json\")\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(new_data, f, indent=2)\n",
    "\n",
    "    print(f\"\\nExported {len(new_data['images'])} images with {len(new_data['annotations'])} annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def export_augmented_dataset(\n",
    "    original_img_dir,\n",
    "    original_json_path,\n",
    "    output_dir,\n",
    "    num_augmentations=3\n",
    "):\n",
    "    # Load original COCO data\n",
    "    with open(original_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(os.path.join(output_dir, \"images\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, \"annotations\"), exist_ok=True)\n",
    "\n",
    "    # Initialize new dataset structure\n",
    "    new_data = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": coco_data[\"categories\"]\n",
    "    }\n",
    "\n",
    "    # Track IDs\n",
    "    max_img_id = max(img[\"id\"] for img in coco_data[\"images\"]) if coco_data[\"images\"] else 0\n",
    "    max_ann_id = max(ann[\"id\"] for ann in coco_data[\"annotations\"]) if coco_data[\"annotations\"] else 0\n",
    "\n",
    "    # Prepare augmentation transforms (without ToTensor())\n",
    "    augmentation_pipeline = get_augmented_transform(train=True)\n",
    "    # Remove ToTensor() from pipeline for image saving\n",
    "    augmentation_pipeline.transforms = [t for t in augmentation_pipeline.transforms \n",
    "                                      if not isinstance(t, ToTensor)]\n",
    "\n",
    "    for orig_img_info in coco_data[\"images\"]:\n",
    "        # Load original image\n",
    "        img_path = os.path.join(original_img_dir, orig_img_info[\"file_name\"])\n",
    "        original_image = Image.open(img_path).convert(\"RGB\")\n",
    "        orig_width, orig_height = original_image.size\n",
    "\n",
    "        # Get corresponding annotations\n",
    "        original_annots = [ann for ann in coco_data[\"annotations\"] \n",
    "                         if ann[\"image_id\"] == orig_img_info[\"id\"]]\n",
    "\n",
    "        # Save original image and annotations to new dataset\n",
    "        original_output_path = os.path.join(output_dir, \"images\", orig_img_info[\"file_name\"])\n",
    "        original_image.save(original_output_path)\n",
    "        new_data[\"images\"].append(copy.deepcopy(orig_img_info))\n",
    "        for ann in original_annots:\n",
    "            new_ann = copy.deepcopy(ann)\n",
    "            new_data[\"annotations\"].append(new_ann)\n",
    "\n",
    "        # Convert COCO bboxes to x1y1x2y2 format\n",
    "        boxes = []\n",
    "        for ann in original_annots:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "\n",
    "        # Create target dictionary\n",
    "        original_target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor([ann[\"category_id\"] for ann in original_annots], dtype=torch.int64),\n",
    "            \"iscrowd\": torch.tensor([ann[\"iscrowd\"] for ann in original_annots], dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        for aug_idx in range(num_augmentations):\n",
    "            # Create copies for augmentation\n",
    "            aug_image = original_image.copy()\n",
    "            aug_target = copy.deepcopy(original_target)\n",
    "\n",
    "            # Apply augmentation pipeline\n",
    "            for transform in augmentation_pipeline.transforms:\n",
    "                aug_image, aug_target = transform(aug_image, aug_target)\n",
    "\n",
    "            # Get new dimensions from PIL Image\n",
    "            aug_width, aug_height = aug_image.size\n",
    "\n",
    "            # Generate new filename\n",
    "            base_name = os.path.splitext(orig_img_info[\"file_name\"])[0]\n",
    "            new_filename = f\"{base_name}_aug{aug_idx}.jpg\"\n",
    "            new_img_path = os.path.join(output_dir, \"images\", new_filename)\n",
    "            aug_image.save(new_img_path)\n",
    "\n",
    "            # Create new image entry\n",
    "            max_img_id += 1\n",
    "            new_img_info = {\n",
    "                \"id\": max_img_id,\n",
    "                \"file_name\": new_filename,\n",
    "                \"width\": aug_width,\n",
    "                \"height\": aug_height\n",
    "            }\n",
    "            new_data[\"images\"].append(new_img_info)\n",
    "\n",
    "            # Process annotations\n",
    "            valid_boxes = []\n",
    "            for box, label, iscrowd in zip(aug_target[\"boxes\"].numpy(),\n",
    "                                         aug_target[\"labels\"].numpy(),\n",
    "                                         aug_target[\"iscrowd\"].numpy()):\n",
    "                # Convert back to COCO format\n",
    "                x1, y1, x2, y2 = box\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "\n",
    "                # Filter invalid boxes\n",
    "                if w > 0 and h > 0 and x1 < aug_width and y1 < aug_height:\n",
    "                    valid_boxes.append({\n",
    "                        \"id\": max_ann_id + 1,\n",
    "                        \"image_id\": max_img_id,\n",
    "                        \"category_id\": int(label),\n",
    "                        \"bbox\": [float(x1), float(y1), float(w), float(h)],\n",
    "                        \"area\": float(w * h),\n",
    "                        \"iscrowd\": int(iscrowd)\n",
    "                    })\n",
    "                    max_ann_id += 1\n",
    "\n",
    "            new_data[\"annotations\"].extend(valid_boxes)\n",
    "\n",
    "    # Save new annotations\n",
    "    output_json_path = os.path.join(output_dir, \"annotations\", \"instances_augmented.json\")\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(new_data, f, indent=2)\n",
    "\n",
    "    print(f\"Exported {len(new_data['images'])} images with {len(new_data['annotations'])} annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_augmented_dataset(\n",
    "    original_img_dir=\"data/data_faster_rcnn/train/images\",\n",
    "    original_json_path=\"data/data_faster_rcnn/train/annotations/instances_train.json\",\n",
    "    output_dir=\"data/data_faster_rcnn/augmented_examplev43\",\n",
    "    num_augmentations=4  # Number of augmented versions per image\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get function to see if annotations are right\n",
    "import cv2\n",
    "# read the json files\n",
    "def check_images_in_annotations(destination_dir, destination_dir_for_images, max_images=10):\n",
    "    with open(destination_dir) as f:\n",
    "        data = json.load(f)\n",
    "        annotations = data[\"annotations\"]\n",
    "        #shffle\n",
    "        random.shuffle(annotations)\n",
    "        for annotation in annotations[:max_images]:\n",
    "            image_id = annotation[\"image_id\"]\n",
    "            images = data[\"images\"]\n",
    "            image = next((image for image in images if image[\"id\"] == image_id), None)\n",
    "\n",
    "            if image is None:\n",
    "                print(f\"Image not found for annotation: {annotation}\")\n",
    "            else:\n",
    "                # show the image with bounding box\n",
    "                # Read the image\n",
    "                path_im = destination_dir_for_images + \"/\"+ image[\"file_name\"]\n",
    "                file_name = image[\"file_name\"]\n",
    "                image = cv2.imread(path_im)\n",
    "                if image is None:\n",
    "                    print(f\"Image not found: {path_im}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert BGR (OpenCV format) to RGB (Matplotlib format)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Extract bounding box coordinates\n",
    "                x, y, w, h = annotation[\"bbox\"]\n",
    "                x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "\n",
    "                # Plot the image\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.imshow(image)\n",
    "\n",
    "                # Draw the bounding box\n",
    "                plt.gca().add_patch(plt.Rectangle((x, y), w, h, edgecolor='green', facecolor='none', linewidth=2))\n",
    "\n",
    "                \n",
    "                # Display the image with the bounding box\n",
    "                plt.title(f\"Image file: {file_name}\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "annotations_file = \"data/data_faster_rcnn/augmented_examplev12/annotations/instances_augmented.json\"\n",
    "images_dir = \"data/data_faster_rcnn/augmented_examplev12/images\"\n",
    "check_images_in_annotations(annotations_file,images_dir, max_images=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#see if torch is available\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 25180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda')\n",
    "print(f\"Training model on {device}\")\n",
    "from torchvision.models.detection.backbone_utils import mobilenet_backbone\n",
    "# Create datasets\n",
    "train_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/augmented_trainv2',\n",
    "    annotations_file='data/data_faster_rcnn/augmented_trainv2/annotations/instances_augmented.json',\n",
    "    transforms=get_augmented_transform(train=False)\n",
    ")\n",
    "\n",
    "val_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/val',\n",
    "    annotations_file='data/data_faster_rcnn/val/annotations/instances_val.json',\n",
    "    transforms=get_augmented_transform(train=False)\n",
    ")\n",
    "\n",
    "# amount of cpu cores\n",
    "workers = 0\n",
    "batchsize = 4\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batchsize,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batchsize,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 2  # hazmat code and background\n",
    "backbone = mobilenet_backbone('mobilenet_v2', pretrained=True)\n",
    "# Create ResNet-101 backbone with FPN\n",
    "# backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
    "\n",
    "# Define anchor generator for FPN\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32,), (64,), (128,), (256,), (512,)),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    ")\n",
    "\n",
    "# Multi-scale RoI pooling for FPN\n",
    "roi_pooler = MultiScaleRoIAlign(\n",
    "    featmap_names=['0', '1', '2', '3', '4'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2\n",
    ")\n",
    "\n",
    "print(\"initializing model...\")\n",
    "# Initialize Faster R-CNN with ResNet-101-FPN\n",
    "model = FasterRCNN(\n",
    "    backbone=backbone,\n",
    "    num_classes=num_classes,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "train_metrics_map = []\n",
    "best_val_map = float('-inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Create directory to store models and logs\n",
    "directory_finetuned_model = create_directory()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    best_val_map = train_model(\n",
    "        directory=directory_finetuned_model, \n",
    "        model=model, optimizer=optimizer, train_loader=train_loader, device=device, \n",
    "        train_metrics_list=train_metrics_map, best_val_map=best_val_map, lr_scheduler=lr_scheduler, \n",
    "        val_loader=val_loader, coco_val=coco_val, scaler=scaler, epoch=epoch\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "device = torch.device('cuda:0')\n",
    "print(f\"Training model on {device}\")\n",
    "\n",
    "data_dir_train = \"augmented_examplev12\"\n",
    "annotations_d = \"augmented\"\n",
    "\n",
    "    \n",
    "# Create datasets\n",
    "train_dataset = HazmatDataset(\n",
    "    data_dir=f'data/data_faster_rcnn/{data_dir_train}',\n",
    "    annotations_file=f'data/data_faster_rcnn/{data_dir_train}/annotations/instances_{annotations_d}.json',\n",
    "    transforms=get_augmented_transform(train=False)\n",
    ")\n",
    "\n",
    "val_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/val',\n",
    "    annotations_file='data/data_faster_rcnn/val/annotations/instances_val.json',\n",
    "    transforms=get_augmented_transform(train=False)\n",
    ")\n",
    "\n",
    "# Set the percentage of the training dataset to use (e.g. 0.x to 1)\n",
    "train_percentage = 1\n",
    "\n",
    "# Create a subset of the training dataset\n",
    "train_dataset_subset = create_subset(train_dataset, train_percentage)\n",
    "\n",
    "# Set the percentage of the val dataset to use (e.g. 0.x to 1)\n",
    "val_percentage = 1\n",
    "\n",
    "# Create a subset of the training dataset\n",
    "val_dataset_subset = create_subset(val_dataset, val_percentage)\n",
    "\n",
    "# amount of cpu cores\n",
    "workers = 2\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_subset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_subset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 2  # hazmat code and background\n",
    "\n",
    "# Create ResNet-101 backbone with FPN\n",
    "backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
    "\n",
    "# Define anchor generator for FPN\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32,), (64,), (128,), (256,), (512,)),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    ")\n",
    "\n",
    "# Multi-scale RoI pooling for FPN\n",
    "roi_pooler = MultiScaleRoIAlign(\n",
    "    featmap_names=['0', '1', '2', '3', '4'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2\n",
    ")\n",
    "\n",
    "print(\"initializing model...\")\n",
    "# Initialize Faster R-CNN with ResNet-101-FPN\n",
    "model = FasterRCNN(\n",
    "    backbone=backbone,\n",
    "    num_classes=num_classes,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "directory_finetuned_model = os.path.join(\"data\", \"models\")\n",
    "device = torch.device('cuda')\n",
    "model_path = os.path.join(directory_finetuned_model, 'best_model.pth')\n",
    "checkpoint = torch.load(model_path, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "val_map = checkpoint['val_map']\n",
    "epoch = checkpoint['epoch']\n",
    "#latest\n",
    "# latest_model_path = os.path.join(directory_finetuned_model, 'latest_model.pth')\n",
    "# checkpoint_latest = torch.load(latest_model_path, map_location=device)\n",
    "# val_map_latest = checkpoint_latest['val_map']\n",
    "# epoch_latest = checkpoint_latest['epoch']\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# model = torch.load('data/models/best_model.pth', map_location=torch.device('cpu'))\n",
    "def load_image(image_path, transforms=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transforms:\n",
    "        for transform in transforms:\n",
    "            image, _ = transform(image, target=None)  # No target during inference\n",
    "    return image\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # Convert PIL image to tensor\n",
    "    transforms.append(ToTensor())\n",
    "    if train:\n",
    "        # Add training augmentations here if needed\n",
    "        transforms.append(RandomHorizontalFlip(0.5))\n",
    "    return transforms\n",
    "\n",
    "# Define preprocessing transforms\n",
    "test_transforms = get_transform(train=False)\n",
    "\n",
    "# Load the image\n",
    "image_path = 'images/hazard_plate.jpg'  # Replace with your image path\n",
    "image = load_image(image_path, transforms=test_transforms)\n",
    "image = image.to(device)\n",
    "# Wrap the image in a list as the model expects a batch\n",
    "with torch.no_grad():\n",
    "    predictions = model([image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_predictions(image, predictions, threshold=0.5, classes=['background', 'hazmat']):\n",
    "    # Convert image from tensor to numpy array\n",
    "    image = image.cpu().permute(1, 2, 0).numpy()\n",
    "    image = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Filter predictions based on confidence threshold\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if label == 1:  # Only plot hazmat codes\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color='blue', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            label_name = classes[label]\n",
    "            ax.text(x1, y1, f'{label_name}: {score:.2f}', color='white', backgroundcolor='blue', fontsize=12)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, threshold=0.5):\n",
    "    # List of class names\n",
    "    classes = ['background', 'hazmat']\n",
    "    \n",
    "    # Load the image\n",
    "    image = load_image(image_path, transforms=test_transforms)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # Wrap the image in a list as the model expects a batch\n",
    "    with torch.no_grad():\n",
    "        predictions = model([image])\n",
    "    \n",
    "    # Filter predictions based on threshold\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Apply threshold filter\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    # Print the predictions\n",
    "    if len(boxes) == 0:\n",
    "        print(\"No predictions meet the threshold.\")\n",
    "    else:\n",
    "        print(\"Predictions:\")\n",
    "        for label, score in zip(labels, scores):\n",
    "            class_name = classes[label]\n",
    "            print(f\"  {class_name}: {score:.2f}\")\n",
    "        # Display the predictions\n",
    "        draw_predictions(image, predictions, threshold=threshold, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Function to predict all images in a directory (including subdirectories)\n",
    "def predict_all_images(directory, threshold=0):\n",
    "    image_extensions = ('*.jpg', '*.jpeg', '*.png', '*.webp')  # Add more extensions if needed\n",
    "    image_files = []\n",
    "    \n",
    "    # Collect all images from the directory and subdirectories\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(directory, '**', ext), recursive=True))\n",
    "    \n",
    "    # Run predictions on each image\n",
    "    for image_path in image_files:\n",
    "        print(f\"Predicting: {image_path}\")\n",
    "        predict_image(image_path, threshold=threshold)\n",
    "\n",
    "# Call the function with your images folder\n",
    "predict_all_images('images', threshold=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image('data/data_faster_rcnn/val/images/1690281365_00595.jpg', threshold=0.29)\n",
    "predict_image('images/hazard_plate.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/1.webp', threshold=0)\n",
    "predict_image('images/un_numbers_test/2.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/3.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/4.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/6.webp', threshold=0)\n",
    "predict_image('images/un_numbers_test/7.jpg', threshold=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
