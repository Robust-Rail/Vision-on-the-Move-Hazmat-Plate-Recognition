{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importing libraries...\")\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image\n",
    "import contextlib\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "# Define the dataset class\n",
    "class HazmatDataset(Dataset):\n",
    "    def __init__(self, data_dir, annotations_file, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Load annotations\n",
    "        with open(annotations_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.images = {img['id']: img for img in data['images']}\n",
    "        self.annotations = data['annotations']\n",
    "        \n",
    "        # Create image_id to annotations mapping\n",
    "        self.img_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_to_anns:\n",
    "                self.img_to_anns[img_id] = []\n",
    "            self.img_to_anns[img_id].append(ann)\n",
    "        \n",
    "        self.ids = list(self.images.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.data_dir, 'images', img_info['file_name'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Get annotations\n",
    "        anns = self.img_to_anns.get(img_id, [])\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']\n",
    "            # Convert [x, y, w, h] to [x1, y1, x2, y2]\n",
    "            boxes.append([\n",
    "                bbox[0],\n",
    "                bbox[1],\n",
    "                bbox[0] + bbox[2],\n",
    "                bbox[1] + bbox[3]\n",
    "            ])\n",
    "            labels.append(ann['category_id'])\n",
    "            areas.append(ann['area'])\n",
    "            iscrowd.append(ann['iscrowd'])\n",
    "        \n",
    "        # Convert to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': areas,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            for transform in self.transforms:\n",
    "                img, target = transform(img, target)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        # Convert PIL image to tensor\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = F.hflip(image)\n",
    "            # Flip bounding boxes\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]  # Flip x-coordinates\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # Convert PIL image to tensor\n",
    "    transforms.append(ToTensor())\n",
    "    if train:\n",
    "        # Add training augmentations here if needed\n",
    "        transforms.append(RandomHorizontalFlip(0.5))\n",
    "    return transforms\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_classifier_loss = 0\n",
    "    total_box_reg_loss = 0\n",
    "    total_objectness_loss = 0\n",
    "    total_rpn_box_reg_loss = 0\n",
    "\n",
    "    # Voeg tqdm toe om de voortgang te tonen\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=True)\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Wrap the forward pass in autocast\n",
    "        with autocast():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Scale the loss and call backward\n",
    "        scaler.scale(losses).backward()\n",
    "        # Unscales the gradients and calls or skips optimizer.step()\n",
    "        scaler.step(optimizer)\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        # Bereken de totalen\n",
    "        total_loss += losses.item()\n",
    "        total_classifier_loss += loss_dict['loss_classifier'].item()\n",
    "        total_box_reg_loss += loss_dict['loss_box_reg'].item()\n",
    "        total_objectness_loss += loss_dict['loss_objectness'].item()\n",
    "        total_rpn_box_reg_loss += loss_dict['loss_rpn_box_reg'].item()\n",
    "\n",
    "        # Update tqdm-balk\n",
    "        progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{losses.item():.4f}\",\n",
    "            \"Classifier\": f\"{loss_dict['loss_classifier'].item():.4f}\",\n",
    "            \"BoxReg\": f\"{loss_dict['loss_box_reg'].item():.4f}\",\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_classifier_loss = total_classifier_loss / len(data_loader)\n",
    "    avg_box_reg_loss = total_box_reg_loss / len(data_loader)\n",
    "    avg_objectness_loss = total_objectness_loss / len(data_loader)\n",
    "    avg_rpn_box_reg_loss = total_rpn_box_reg_loss / len(data_loader)\n",
    "\n",
    "    return avg_loss, avg_classifier_loss, avg_box_reg_loss, avg_objectness_loss, avg_rpn_box_reg_loss\n",
    "\n",
    "\n",
    "\n",
    "# Load ground truth annotations\n",
    "coco_val = COCO('data/data_faster_rcnn/val/annotations/instances_val.json')\n",
    "\n",
    "# Prepare predictions in COCO format\n",
    "# Assuming you have a function to convert model outputs to COCO format\n",
    "# Conversion to COCO Format\n",
    "def convert_to_coco_format(outputs, image_ids):\n",
    "    coco_results = []\n",
    "    for output, image_id in zip(outputs, image_ids):\n",
    "        boxes = output['boxes'].cpu().numpy()\n",
    "        scores = output['scores'].cpu().numpy()\n",
    "        labels = output['labels'].cpu().numpy()\n",
    "        \n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            coco_results.append({\n",
    "                'image_id': image_id,\n",
    "                'category_id': int(label),\n",
    "                'bbox': [box[0], box[1], box[2] - box[0], box[3] - box[1]],\n",
    "                'score': float(score)\n",
    "            })\n",
    "    return coco_results\n",
    "\n",
    "# Validation Function\n",
    "def validate(model, data_loader, coco_gt, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    # Add tqdm\n",
    "    progress_bar = tqdm(data_loader, desc=\"Validation\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in progress_bar:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            image_ids = [target['image_id'].item() for target in targets]\n",
    "            coco_results = convert_to_coco_format(outputs, image_ids)\n",
    "            results.extend(coco_results)\n",
    "\n",
    "            # Update tqdm-bar\n",
    "            progress_bar.set_postfix({\"Processed\": len(results)})\n",
    "\n",
    "    if not results:\n",
    "        print(\"No predictions generated. Skipping evaluation.\")\n",
    "        return [0.0] * 6  # Return dummy metrics for empty results\n",
    "\n",
    "    # Suppress COCOeval output\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        coco_dt = coco_gt.loadRes(results)\n",
    "        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "    return coco_eval.stats\n",
    "\n",
    "\n",
    "# Custom backbone to return a dictionary of feature maps\n",
    "class BackboneWithChannels(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return {'0': x}\n",
    "    \n",
    "# Function to create a subset of the dataset\n",
    "def create_subset(dataset, percentage):\n",
    "    \"\"\"\n",
    "    Create a subset of the dataset based on the given percentage.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: The full dataset.\n",
    "    - percentage: The fraction of the dataset to use (value between 0.0 and 1.0).\n",
    "    \n",
    "    Returns:\n",
    "    - subset: A subset of the dataset containing the specified percentage of data.\n",
    "    \"\"\"\n",
    "    if not (0.0 < percentage <= 1.0):\n",
    "        raise ValueError(\"Percentage must be between 0.0 and 1.0.\")\n",
    "    \n",
    "    # Determine the subset size\n",
    "    total_samples = len(dataset)\n",
    "    subset_size = int(total_samples * percentage)\n",
    "    \n",
    "    # Shuffle and select a random subset of indices\n",
    "    indices = list(range(total_samples))\n",
    "    random.shuffle(indices)\n",
    "    subset_indices = indices[:subset_size]\n",
    "    \n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "def create_directory(base_path=\"data/models\"):\n",
    "    \"\"\"\n",
    "    Create a directory inside the base path named 'faster-rcnn-finetuned-{date}' \n",
    "    to store models and logs. The name includes the current date and time in the format 'DD-MM-YYYY HH:MM:SS'.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Base directory where the new directory will be created.\n",
    "\n",
    "    Returns:\n",
    "    - directory_path (str): Full path to the created directory.\n",
    "    \"\"\"\n",
    "    # Get the current date and time\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "    \n",
    "    # Define the full directory path\n",
    "    directory_name = f\"faster-rcnn-finetuned-{current_time}\"\n",
    "    directory_path = os.path.join(base_path, directory_name)\n",
    "    \n",
    "    # Create the directory\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Directory created: {directory_path}\")\n",
    "    return directory_path\n",
    "\n",
    "def train_model(directory, model, optimizer, train_loader, device, train_metrics_list, best_val_map, lr_scheduler, val_loader, coco_val, scaler, epoch):\n",
    "    \n",
    "    epoch+=1\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_classifier_loss, train_box_reg_loss, train_objectness_loss, train_rpn_box_reg_loss = train_one_epoch(\n",
    "        model, optimizer, train_loader, device, scaler)\n",
    "    \n",
    "    # Validate and get all COCO-metrics\n",
    "    val_metrics = validate(model, val_loader, coco_val, device)\n",
    "    val_map = val_metrics[0]  # mAP@IoU=0.50:0.95\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    \n",
    "    # Obtain the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Prepare data for logging\n",
    "    data = {\n",
    "        \"epoch\": epoch,\n",
    "        \"time_elapsed\": (int(minutes), int(seconds)),\n",
    "        \"learning_rate\": current_lr,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"classifier_loss\": train_classifier_loss,\n",
    "        \"box_reg_loss\": train_box_reg_loss,\n",
    "        \"objectness_loss\": train_objectness_loss,\n",
    "        \"rpn_box_reg_loss\": train_rpn_box_reg_loss,\n",
    "        \"val_metrics\": val_metrics\n",
    "    }\n",
    "    \n",
    "    # Append current epoch data to metrics list\n",
    "    train_metrics_list.append(data)\n",
    "    \n",
    "    # Print summary for this epoch\n",
    "    print(f\"ðŸ“Š Epoch {epoch} | â³ Time: {int(minutes)}m {int(seconds)}s | ðŸ”„ LR: {current_lr:.6f}\")\n",
    "    print(f\"ðŸ“‰ Train Loss: {train_loss:.4f} | ðŸŽ¯ Classifier: {train_classifier_loss:.4f} | ðŸ“¦ Box Reg: {train_box_reg_loss:.4f}\")\n",
    "    print(f\"ðŸ” Objectness: {train_objectness_loss:.4f} | ðŸ—‚ï¸ RPN Box Reg: {train_rpn_box_reg_loss:.4f}\")\n",
    "    print(f\"ðŸ§ª mAP | ðŸŸ¢ mAP@IoU=0.50:0.95: {val_metrics[0]:.4f} | ðŸ”µ mAP@IoU=0.50: {val_metrics[1]:.4f} | ðŸŸ£ mAP@IoU=0.75: {val_metrics[2]:.4f}\")\n",
    "    print(f\"ðŸ“ Small mAP: {val_metrics[3]:.4f} | ðŸ“ Medium mAP: {val_metrics[4]:.4f} | ðŸ“ Large mAP: {val_metrics[5]:.4f}\")\n",
    "    \n",
    "    # Save epoch data to a log file\n",
    "    save_epoch_data(directory, data)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save the latest checkpoint with all metrics\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_map': val_map,\n",
    "        'train_metrics_list': train_metrics_list  # Save all metrics\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(directory, \"latest_model.pth\"))\n",
    "    \n",
    "    # Save the best model if the val_map is the highest so far\n",
    "    if val_map > best_val_map:\n",
    "        best_val_map = val_map\n",
    "        torch.save(checkpoint, os.path.join(directory, \"best_model.pth\"))\n",
    "    \n",
    "    return best_val_map\n",
    "        \n",
    "\n",
    "\n",
    "def save_epoch_data(directory, data):\n",
    "    \"\"\"\n",
    "    Save training statistics for each epoch in a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): Path to the directory.\n",
    "    - data (dict): Contains data on metrics such as epoch, losses, and validation metrics.\n",
    "    \"\"\"\n",
    "    log_file_path = os.path.join(directory, \"training_log.txt\")\n",
    "    \n",
    "    with open(log_file_path, \"a\") as log_file:\n",
    "        log_file.write(f\"ðŸ“Š Epoch {data['epoch']} | â³ Time: {data['time_elapsed'][0]}m {data['time_elapsed'][1]}s | ðŸ”„ LR: {data['learning_rate']:.6f}\\n\")\n",
    "        log_file.write(f\"ðŸ“‰ Train Loss: {data['train_loss']:.4f} | ðŸŽ¯ Classifier: {data['classifier_loss']:.4f} | ðŸ“¦ Box Reg: {data['box_reg_loss']:.4f}\\n\")\n",
    "        log_file.write(f\"ðŸ” Objectness: {data['objectness_loss']:.4f} | ðŸ—‚ï¸ RPN Box Reg: {data['rpn_box_reg_loss']:.4f}\\n\")\n",
    "        log_file.write(f\"ðŸ§ª Validation Metrics | ðŸŸ¢ mAP@IoU=0.50:0.95: {data['val_metrics'][0]:.4f} | ðŸ”µ mAP@IoU=0.50: {data['val_metrics'][1]:.4f} | ðŸŸ£ mAP@IoU=0.75: {data['val_metrics'][2]:.4f}\\n\")\n",
    "        log_file.write(f\"ðŸ“ Small mAP: {data['val_metrics'][3]:.4f} | ðŸ“ Medium mAP: {data['val_metrics'][4]:.4f} | ðŸ“ Large mAP: {data['val_metrics'][5]:.4f}\\n\")\n",
    "        log_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "print(f\"Training model on {device}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/train',\n",
    "    annotations_file='data/data_faster_rcnn/train/annotations/instances_train.json',\n",
    "    transforms=get_transform(train=True)\n",
    ")\n",
    "\n",
    "val_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/val',\n",
    "    annotations_file='data/data_faster_rcnn/val/annotations/instances_val.json',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "# Set the percentage of the training dataset to use (e.g. 0.x to 1)\n",
    "train_percentage = 1\n",
    "\n",
    "# Create a subset of the training dataset\n",
    "train_dataset_subset = create_subset(train_dataset, train_percentage)\n",
    "\n",
    "# Set the percentage of the val dataset to use (e.g. 0.x to 1)\n",
    "val_percentage = 1\n",
    "\n",
    "# Create a subset of the training dataset\n",
    "val_dataset_subset = create_subset(val_dataset, val_percentage)\n",
    "\n",
    "# amount of cpu cores\n",
    "workers = 2\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_subset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_subset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 2  # hazmat code and background\n",
    "\n",
    "# Create ResNet-101 backbone with FPN\n",
    "backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
    "\n",
    "# Define anchor generator for FPN\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32,), (64,), (128,), (256,), (512,)),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    ")\n",
    "\n",
    "# Multi-scale RoI pooling for FPN\n",
    "roi_pooler = MultiScaleRoIAlign(\n",
    "    featmap_names=['0', '1', '2', '3', '4'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2\n",
    ")\n",
    "\n",
    "print(\"initializing model...\")\n",
    "# Initialize Faster R-CNN with ResNet-101-FPN\n",
    "model = FasterRCNN(\n",
    "    backbone=backbone,\n",
    "    num_classes=num_classes,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 7710"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 23\n",
    "train_metrics_map = []\n",
    "best_val_map = float('-inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Create directory to store models and logs\n",
    "directory_finetuned_model = create_directory()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    best_val_map = train_model(\n",
    "        directory=directory_finetuned_model, \n",
    "        model=model, optimizer=optimizer, train_loader=train_loader, device=device, \n",
    "        train_metrics_list=train_metrics_map, best_val_map=best_val_map, lr_scheduler=lr_scheduler, \n",
    "        val_loader=val_loader, coco_val=coco_val, scaler=scaler, epoch=epoch\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "directory_finetuned_model = \"data/models\"\n",
    "device = torch.device('gpu:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = os.path.join(directory_finetuned_model, 'best_model.pth')\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "val_map = checkpoint['val_map']\n",
    "epoch = checkpoint['epoch']\n",
    "#latest\n",
    "latest_model_path = os.path.join(directory_finetuned_model, 'latest_checkpoint.pth')\n",
    "checkpoint_latest = torch.load(latest_model_path, map_location=device)\n",
    "val_map_latest = checkpoint_latest['val_map']\n",
    "epoch_latest = checkpoint_latest['epoch']\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(f\"Validation mAP best model: {val_map:.4f}\")\n",
    "print(f\"Epoch best model: {epoch}\")\n",
    "\n",
    "print(f\"Validation mAP latest model: {val_map_latest:.4f}\")\n",
    "print(f\"Epoch latest model: {epoch_latest}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(checkpoint_path, title=\"Training and Validation Metrics over Epochs\"):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics from a given model checkpoint.\n",
    "    \n",
    "    Parameters:\n",
    "    - checkpoint_path (str): Path to the model checkpoint file (e.g., 'latest_model.pth').\n",
    "    - title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    train_metrics_list = checkpoint['train_metrics_list']\n",
    "    \n",
    "    # Extract metrics per epoch\n",
    "    epochs = [data['epoch'] for data in train_metrics_list]\n",
    "    train_loss_list = [data['train_loss'] for data in train_metrics_list]\n",
    "    classifier_loss_list = [data['classifier_loss'] for data in train_metrics_list]\n",
    "    box_reg_loss_list = [data['box_reg_loss'] for data in train_metrics_list]\n",
    "    objectness_loss_list = [data['objectness_loss'] for data in train_metrics_list]\n",
    "    rpn_box_reg_loss_list = [data['rpn_box_reg_loss'] for data in train_metrics_list]\n",
    "\n",
    "    # Extract validation mAP metrics\n",
    "    val_map_list = [data['val_metrics'][0] for data in train_metrics_list]  # mAP@IoU=0.50:0.95\n",
    "    val_map_50_list = [data['val_metrics'][1] for data in train_metrics_list]  # mAP@IoU=0.50\n",
    "    val_map_75_list = [data['val_metrics'][2] for data in train_metrics_list]  # mAP@IoU=0.75\n",
    "    val_map_small_list = [data['val_metrics'][3] for data in train_metrics_list]  # Small mAP\n",
    "    val_map_medium_list = [data['val_metrics'][4] for data in train_metrics_list]  # Medium mAP\n",
    "    val_map_large_list = [data['val_metrics'][5] for data in train_metrics_list]  # Large mAP\n",
    "\n",
    "    # Initialize the plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    # Plot training losses\n",
    "    plt.plot(epochs, train_loss_list, label='Training Loss', marker='o')\n",
    "    #     plt.plot(epochs, classifier_loss_list, label='Classifier Loss', marker='o')\n",
    "    #     plt.plot(epochs, box_reg_loss_list, label='Box Regression Loss', marker='o')\n",
    "    #     plt.plot(epochs, objectness_loss_list, label='Objectness Loss', marker='o')\n",
    "    #     plt.plot(epochs, rpn_box_reg_loss_list, label='RPN Box Regression Loss', marker='o')\n",
    "\n",
    "    # Plot validation mAP metrics\n",
    "    plt.plot(epochs, val_map_list, label='Validation mAP (IoU=0.50:0.95)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_50_list, label='Validation mAP (IoU=0.50)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_75_list, label='Validation mAP (IoU=0.75)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_small_list, label='Validation mAP (Small)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_medium_list, label='Validation mAP (Medium)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_large_list, label='Validation mAP (Large)', linestyle='--', marker='x')\n",
    "\n",
    "    # Set x-axis ticks to start from 1\n",
    "    plt.xticks(range(1, len(epochs) + 1))\n",
    "\n",
    "    # Set plot details\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest model checkpoint\n",
    "latest_model_path = os.path.join(directory_finetuned_model, 'latest_model.pth')\n",
    "plot_metrics(latest_model_path, \"Training and validation over epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transforms=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transforms:\n",
    "        for transform in transforms:\n",
    "            image, _ = transform(image, target=None)  # No target during inference\n",
    "    return image\n",
    "\n",
    "# Define preprocessing transforms\n",
    "test_transforms = get_transform(train=False)\n",
    "\n",
    "# Load the image\n",
    "image_path = 'images/hazard_plate.jpg'  # Replace with your image path\n",
    "image = load_image(image_path, transforms=test_transforms)\n",
    "image = image.to(device)\n",
    "# Wrap the image in a list as the model expects a batch\n",
    "with torch.no_grad():\n",
    "    predictions = model([image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_with_opacity(score):\n",
    "    \"\"\"\n",
    "    Get a color with opacity based on the confidence score.\n",
    "    Higher confidence = more red and higher opacity.\n",
    "    Lower confidence = random color and lower opacity.\n",
    "    \"\"\"\n",
    "    if score > 0.75:\n",
    "        # High confidence: Red with high opacity\n",
    "        color = (1, 0, 0, min(1.0, 0.3 + score))  # Red with opacity based on score\n",
    "    else:\n",
    "        # Low confidence: Random color with lower opacity\n",
    "        color = (random.random(), random.random(), random.random(), max(0.3, score))\n",
    "    return color\n",
    "\n",
    "def draw_predictions(image, predictions, threshold=0.5, classes=['background', 'hazmat']):\n",
    "    # Convert image from tensor to numpy array\n",
    "    image = image.cpu().permute(1, 2, 0).numpy()\n",
    "    image = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Filter predictions based on confidence threshold\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if label == 1:  # Only plot hazmat codes\n",
    "            x1, y1, x2, y2 = box\n",
    "            color = get_color_with_opacity(score)\n",
    "            \n",
    "            # Draw rectangle with opacity\n",
    "            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, \n",
    "                                 edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add text label with confidence score\n",
    "            label_name = classes[label]\n",
    "            ax.text(x1, y1, f'{label_name}: {score:.2f}', \n",
    "                    color='white', \n",
    "                    bbox=dict(facecolor=color[:3], alpha=0.6), \n",
    "                    fontsize=12)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, threshold=0.5):\n",
    "    # List of class names\n",
    "    classes = ['background', 'hazmat']\n",
    "    \n",
    "    # Load the image\n",
    "    image = load_image(image_path, transforms=test_transforms)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Wrap the image in a list as the model expects a batch\n",
    "    with torch.no_grad():\n",
    "        predictions = model([image])\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    prediction_time = end_time - start_time\n",
    "    print(f\"Prediction time: {prediction_time:.4f} seconds\")\n",
    "    \n",
    "    # Filter predictions based on threshold\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Apply threshold filter\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    # Print the predictions\n",
    "    if len(boxes) == 0:\n",
    "        print(\"No predictions meet the threshold.\")\n",
    "    else:\n",
    "        print(\"Predictions:\")\n",
    "        for label, score in zip(labels, scores):\n",
    "            class_name = classes[label]\n",
    "            print(f\"  {class_name}: {score:.2f}\")\n",
    "        # Display the predictions\n",
    "        draw_predictions(image, predictions, threshold=threshold, classes=classes)\n",
    "\n",
    "\n",
    "def get_color_with_opacity(score):\n",
    "    \"\"\"\n",
    "    Get a color with opacity based on the confidence score.\n",
    "    Higher confidence = more red and higher opacity.\n",
    "    Lower confidence = random color and lower opacity.\n",
    "    \"\"\"\n",
    "    if score > 0.75:\n",
    "        # High confidence: Red with high opacity\n",
    "        color = (1, 0, 0, min(1.0, 0.3 + score))  # Red with opacity based on score\n",
    "    else:\n",
    "        # Low confidence: Random color with lower opacity\n",
    "        color = (random.random(), random.random(), random.random(), max(0.3, score))\n",
    "    return color\n",
    "\n",
    "def draw_predictions(image, predictions, threshold=0.5, classes=['background', 'hazmat']):\n",
    "    # Convert image from tensor to numpy array\n",
    "    image = image.cpu().permute(1, 2, 0).numpy()\n",
    "    image = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Filter predictions based on confidence threshold\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if label == 1:  # Only plot hazmat codes\n",
    "            x1, y1, x2, y2 = box\n",
    "            color = get_color_with_opacity(score)\n",
    "            \n",
    "            # Draw rectangle with opacity\n",
    "            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, \n",
    "                                 edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add text label with confidence score\n",
    "            label_name = classes[label]\n",
    "            ax.text(x1, y1, f'{label_name}: {score:.2f}', \n",
    "                    color='white', \n",
    "                    bbox=dict(facecolor=color[:3], alpha=0.6), \n",
    "                    fontsize=12)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_image('data/data_faster_rcnn/val/images/1690281365_00595.jpg', threshold=0.29)\n",
    "predict_image('images/hazard_plate.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/close_up_number.webp', threshold=0)\n",
    "predict_image('images/un_numbers_test/2.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/3.jpg', threshold=0)\n",
    "predict_image('images/two_signs_different_distance.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/6.webp', threshold=0)\n",
    "predict_image('images/no_signs.jpg', threshold=0)\n",
    "predict_image('images/africalane_closed_off.jpg', threshold=0)\n",
    "predict_image('images/bikes_get_off.jpg', threshold=0)\n",
    "predict_image('images/gevaarlijke_stoffen_route.jpg', threshold=0)\n",
    "predict_image('images/great_britain_nb.jpeg', threshold=0)\n",
    "predict_image('images/priority-road-sign.webp', threshold=0)\n",
    "predict_image('images/reflective_un_number_on_truck.jpg', threshold=0)\n",
    "predict_image('images/traffic signs.jpg', threshold=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "test_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/test',\n",
    "    annotations_file='data/data_faster_rcnn/test/annotations/instances_test.json',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "# Create test data loader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Load the best model checkpoint\n",
    "\n",
    "model_path = os.path.join(directory_finetuned_model, 'best_model.pth')\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "# Load ground truth annotations for test set\n",
    "coco_test = COCO('data/data_faster_rcnn/test/annotations/instances_test.json')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = validate(model, test_loader, coco_test, device)\n",
    "\n",
    "# Print test metrics\n",
    "print(f\"Test Metrics - mAP: {test_metrics[0]:.4f}\")\n",
    "print(f\"mAP@0.5: {test_metrics[1]:.4f}, mAP@0.75: {test_metrics[2]:.4f}\")\n",
    "print(f\"mAP medium: {test_metrics[4]:.4f}, mAP large: {test_metrics[5]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /tmp/ipykernel_2090903/2616491437.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
    "#   checkpoint = torch.load(best_checkpoint_path)\n",
    "# loading annotations into memory...\n",
    "# Done (t=0.00s)\n",
    "# creating index...\n",
    "# index created!\n",
    "# Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [02:38<00:00,  2.56s/it, Processed=1095]\n",
    "# Test Metrics - mAP: 0.5634\n",
    "# mAP@0.5: 0.9892, mAP@0.75: 0.4738\n",
    "# mAP small: -1.0000, mAP medium: 0.4648, mAP large: 0.5724"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep for augmented weather evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop trhough all images from test and predict\n",
    "test_images_path = \"data/data_faster_rcnn/test/images\"\n",
    "\n",
    "# frames available\n",
    "test_images_path_list = os.listdir(test_images_path)\n",
    "random.shuffle(test_images_path_list)\n",
    "\n",
    "# Predict on the first 30 images\n",
    "for count, image_name in enumerate(test_images_path_list[:20]):\n",
    "    image_path = os.path.join(test_images_path, image_name)\n",
    "    predict_image(image_path, threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize(image):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "\n",
    "test_images_path = \"data/data_faster_rcnn/test/images\"\n",
    "\n",
    "# frames available\n",
    "test_images_path_list = os.listdir(test_images_path)\n",
    "random.shuffle(test_images_path_list)\n",
    "path = os.path.join(test_images_path, test_images_path_list[0])\n",
    "image = cv2.imread(path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "transform = A.Compose(\n",
    "    [A.RandomRain(brightness_coefficient=0.9, drop_width=1, blur_value=5, p=1)],\n",
    ")\n",
    "random.seed(7)\n",
    "transformed = transform(image=image)\n",
    "visualize(transformed['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [A.RandomSunFlare(flare_roi=(0, 0, 1,0.5), angle_lower=1, p=1)],\n",
    ")\n",
    "random.seed(7)\n",
    "transformed = transform(image=image)\n",
    "visualize(transformed['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [A.RandomShadow(num_shadows_lower=1, num_shadows_upper=1, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=1)],\n",
    ")\n",
    "random.seed(7)\n",
    "transformed = transform(image=image)\n",
    "visualize(transformed['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [A.RandomFog(fog_coef_lower=0.7, fog_coef_upper=0.8, alpha_coef=0.1, p=1)],\n",
    ")\n",
    "random.seed(7)\n",
    "transformed = transform(image=image)\n",
    "visualize(transformed['image'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "# frames available\n",
    "train_images_path = \"data/data_faster_rcnn/train/images\"\n",
    "train_list = os.listdir(train_images_path)\n",
    "random.shuffle(train_list)\n",
    "\n",
    "def add_to_dataset(image, dataset_name):\n",
    "    \"\"\"\n",
    "    Add an image to the specified dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - image (numpy.ndarray): The image to add (as a NumPy array).\n",
    "    - dataset_name (str): The name of the dataset to add the image to.\n",
    "    \"\"\"\n",
    "    # Get the dataset directory\n",
    "    dataset_dir = os.path.join('data/augmented_images', dataset_name)\n",
    "    \n",
    "    # Create the dataset directory if it doesn't exist\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    \n",
    "    # Get the image filename\n",
    "    image_id = len(os.listdir(dataset_dir)) + 1\n",
    "    image_filename = f\"{image_id}.jpg\"\n",
    "    \n",
    "    # Convert NumPy array to PIL image\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Save the image\n",
    "    image_pil.save(os.path.join(dataset_dir, image_filename))\n",
    "\n",
    "\n",
    "# Loop over train list\n",
    "for count, image_name in enumerate(train_list):\n",
    "    image_path = os.path.join(train_images_path, image_name)\n",
    "    \n",
    "    # Load image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    #check if we have max images per set\n",
    "    random_string = random.choice([\"rain\", \"sunflare\", \"shadow\", \"fog\"])\n",
    "\n",
    "    \n",
    "\n",
    "    # Apply augmentation based on the random choice\n",
    "    if random_string == \"rain\":\n",
    "        transform = A.Compose(\n",
    "            [A.RandomRain(brightness_coefficient=0.9, drop_width=1, blur_value=5, p=1)],\n",
    "        )\n",
    "\n",
    "    elif random_string == \"sunflare\":\n",
    "        transform = A.Compose(\n",
    "            [A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=1, p=1)],\n",
    "        )\n",
    "\n",
    "    elif random_string == \"shadow\":\n",
    "        transform = A.Compose(\n",
    "            [A.RandomShadow(num_shadows_lower=1, num_shadows_upper=1, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=1)],\n",
    "        )\n",
    "\n",
    "    elif random_string == \"fog\":\n",
    "        transform = A.Compose(\n",
    "            [A.RandomFog(fog_coef_lower=0.7, fog_coef_upper=0.8, alpha_coef=0.1, p=1)],\n",
    "        )\n",
    "\n",
    "    # Apply transformation\n",
    "    transformed = transform(image=image)\n",
    "    \n",
    "    # Save transformed image\n",
    "    add_to_dataset(transformed['image'], random_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go over each set rain, sunflare, shadow, fog and predict the images\n",
    "\n",
    "# base path\n",
    "base_path = \"data/augmented_images\"\n",
    "\n",
    "# loop over each set\n",
    "\n",
    "for set_name in [\"rain\", \"sunflare\", \"shadow\", \"fog\"]:\n",
    "    # Get all images in the set\n",
    "    set_path = os.path.join(base_path, set_name)\n",
    "\n",
    "    # loop over each image and use the predict_image function\n",
    "    for count, image_name in enumerate(os.listdir(set_path)):\n",
    "        image_path = os.path.join(set_path, image_name)\n",
    "        predict_image(image_path, threshold=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model metrics\n",
    "\n",
    "<img src=\"data/models/validation.png\" alt=\"validation over epochs\" width=1000>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open training log file\n",
    "\n",
    "log_file_path = os.path.join(\"data\",\"models\", \"training_log.txt\")\n",
    "\n",
    "with open(log_file_path, \"r\") as log_file:\n",
    "    print(log_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fintuned model Faster R-CNN has been trained for 18 epochs and reached a these maximal metrics:\n",
    "\n",
    "- **mAP@IoU=0.50:0.95 (overall mAP)** â†’ Epoch 9, value: 0.5303\n",
    "- **mAP@IoU=0.50** â†’ Epoch 7 and 12 to 18 value: 0.9792\n",
    "- **mAP@IoU=0.75** â†’ Epoch 6, value: 0.4170\n",
    "- **Medium mAP** â†’ Epoch 8, value: 0.4431\n",
    "- **Large mAP** â†’ Epoch 6, value: 0.5395\n",
    "- **RPN Box Reg** â†’ Epoch 5 to 18, value: 0.0008\n",
    "- **Objectness** â†’ Epoch 5 to 18, value: 0.0007\n",
    "- **Box Reg** â†’ Epoch 18, value: 0.0341\n",
    "- **Classifier** â†’ Epoch 4 and 5, value: 0.0182\n",
    "- **Train loss** â†’ Epoch 18, 12, 10, 7, value: 0.0540\n",
    "\n",
    "\n",
    "The model which was chosen as the best model was the model with the highest mAP@IoU=0.50:0.95 (overall mAP), so the checkpoint at epoch 9\n",
    "\n",
    "When evaluating the best model on the test set we get these metrics:\n",
    "- mAP@=0.50:0.95: 0.5634\n",
    "- mAP@0.5: 0.9892\n",
    "- mAP@0.75: 0.4738\n",
    "- mAP medium: 0.4648\n",
    "- mAP large: 0.5724\n",
    "\n",
    "Which are slightly better than the results on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training analysis\n",
    "Based on the analysis of the training and validation metrics, it can be concluded that additional training with the current configuration (data and hyperparameters) yields diminishing returns. The training loss remains stable at approximately 0.0540 over the last 7 to 18 epochs, indicating little improvement with further training. Additionally, the validation mAP scores have plateaued between epochs 4 and 18, showing no significant change. \n",
    "\n",
    "Therefore, it can be inferred that training for around 7 epochs provides near-optimal results while minimizing the time spent on training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False positives\n",
    "\n",
    "Predictions:\n",
    "  - hazmat: 0.88\n",
    "  - hazmat: 0.15\n",
    "  - hazmat: 0.14\n",
    "  - hazmat: 0.06\n",
    "\n",
    "<br>\n",
    "<img src=\"images/predictions/bmw_prediction.png\" alt=\"Model prediction on a BMW licence plate\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions:\n",
    "- hazmat: 0.82\n",
    "- hazmat: 0.81\n",
    "- hazmat: 0.14\n",
    "- hazmat: 0.11\n",
    "- hazmat: 0.06\n",
    "- hazmat: 0.06\n",
    "- hazmat: 0.06\n",
    "- hazmat: 0.06\n",
    "- hazmat: 0.05\n",
    "- hazmat: 0.05\n",
    "\n",
    "<img src=\"images/predictions/trafficsignpred.png\" alt=\"Model prediction on a traffic sign\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model appears to produce some false positives, mistakenly identifying certain objects as hazmat placards when they are not. This issue is particularly prevalent with objects that are square and have colors such as yellow, red, or orange. Expanding the training dataset and applying data augmentation techniques should help mitigate this problem by improving the model's ability to differentiate between actual placards and visually similar objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False negatives\n",
    "\n",
    "<img src=\"images/predictions/hazmatclose.png\" width=600>\n",
    "\n",
    "In certain cases, the model failed to detect UN number placards in high-resolution images. This issue likely arises because the training dataset primarily consists of images where UN number placards were captured from a distance. As a result, the model may have overfitted to the assumption that UN numbers appear relatively small in images. To address this, applying targeted data augmentation techniquesâ€”such as zooming in on UN numbers, rotating them (e.g., upside down), and varying their orientationâ€”can help the model generalize better to different scales and perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Conditions\n",
    "\n",
    "For this project, it was essential to develop a robust model capable of performing well under various weather conditions. However, our dataset primarily consisted of images captured under different lighting conditions, such as day and night, without significant weather variations.  \n",
    "\n",
    "To address this limitation, we evaluated the model using augmented images generated with the **Albumentations** library, applying weather-related transformations such as **rain, sunflare, shadow, and fog.** The model performed reasonably well on these augmented images, successfully identifying objects in most cases. However, it is important to note that data augmentation does not always provide a fully realistic representation of real-world weather conditions.  \n",
    "\n",
    "While the results indicate that the model can likely handle different weather conditions to some extent, further improvements are needed to ensure robust performance in real-life scenarios. Incorporating actual weather-diverse data, along with advanced augmentation techniques that simulate real-world complexity more accurately, would enhance the model's generalization capabilities.\n",
    "\n",
    "### Predictions on Augmented Images:\n",
    "\n",
    "<h4> Rain </h4>\n",
    "<img src=\"images/predictions/rainpred.png\" alt=\"Augmented image with rain effect\" width=\"400\"/>\n",
    "<h4> Flare </h4>\n",
    "<img src=\"images/predictions/flare.png\" alt=\"Augmented image with sunflare effect\" width=\"400\"/>\n",
    "<h4> Shadows </h4>\n",
    "<img src=\"images/predictions/shadowspred.png\" alt=\"Augmented image with shadow effect\" width=\"400\"/>\n",
    "<h4> Fog </h4>\n",
    "<img src=\"images/predictions/fog_pred.png\" alt=\"Augmented image with fog effect\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection on Data Mining Goals\n",
    "\n",
    "### **Primary Data Mining Goal:**\n",
    "Create and train an object detection model capable of identifying and interpreting UN number hazard plates on freight wagons in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "### **Specific Data Mining Goals:**\n",
    "\n",
    "#### **1. Object Detection and Localization:**\n",
    "Develop a model that achieves a high AP score for accurately detecting and localizing hazard plates on freight wagons within each video frame.\n",
    "\n",
    "**Approach and Outcome:**  \n",
    "To accomplish this goal, we finetuned a Faster R-CNN model, which demonstrated promising results in localizing hazard placards. However, the model is not yet fully robust and occasionally produces false positives. These false detections often occur when objects with similar visual characteristicsâ€”such as square shapes and colors resembling hazard placards (e.g., yellow, red, or orange)â€”are present in the scene.\n",
    "\n",
    "**Improvement Strategies:**\n",
    "- **1.1: Expanding the training dataset** with a greater variety of real-world scenarios to improve generalization.\n",
    "- **1.2: Advanced data augmentation**, such as applying transformations that simulate real-life conditions (e.g., partial occlusion, varying angles, and different lighting conditions).\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Robustness Across Variable Conditions:**\n",
    "Enhance the modelâ€™s robustness by training it on datasets representing diverse lighting and weather conditions, with a goal to maintain high AP scores across these environments.\n",
    "\n",
    "**Approach and Outcome:**  \n",
    "While we did not have access to datasets covering a wide range of weather conditions, we leveraged data captured at different times of the day, covering various lighting conditions such as daytime, nighttime, and low-light scenarios. The model demonstrated strong performance across these lighting variations, indicating a certain level of robustness in this aspect.\n",
    "\n",
    "To evaluate its performance under different weather conditions, we used data augmentation techniques from the **Albumentations** library. Augmentations like rain, sunflare, shadow, and fog were introduced to simulate adverse weather conditions. Although the model performed reasonably well on these augmented images, it is important to acknowledge that synthetic augmentations do not fully replicate real-world conditions.\n",
    "\n",
    "**Further Improvements:**\n",
    "To improve the modelâ€™s robustness, it would be beneficial to collect and incorporate real-world data by filming freight wagons across different seasons and weather conditions. This would ensure the model can generalize better to practical scenarios. Additional techniques to enhance robustness, such as advanced augmentation strategies, are discussed in sections **1.1** and **1.2**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Optimization for Real-Time Processing:**\n",
    "Implement real-time object detection and OCR capabilities to ensure the model operates at a frame rate suitable for analyzing images from moving trains.\n",
    "\n",
    "**Approach and Outcome:**  \n",
    "Currently, the model does not perform in real-time. Inference can take up to **2.8 seconds per frame**, and reading the hazard placard (OCR) requires an additional **2 seconds**, making the total processing time **at least 4.8 seconds per frame**. This delay is far from real-time performance requirements, which typically demand processing speeds of **30 frames per second (FPS)** or faster, depending on the train's speed and camera setup.\n",
    "\n",
    "**Optimization Strategies:**\n",
    "To enhance processing speed, several optimization strategies can be considered:\n",
    "- **4.1: Model Quantization:** Reducing the precision of model parameters (e.g., from 32-bit floating point to 8-bit integers) to speed up computations with minimal accuracy loss.\n",
    "- **4.2: Efficient Attention Mechanisms:** Using lightweight attention models to focus computational resources on relevant regions, improving both speed and accuracy.\n",
    "- **4.3: Model Pruning:** Removing redundant weights and layers to reduce computation overhead.\n",
    "- **4.4: Hardware Acceleration:** Leveraging GPUs, TPUs, or edge AI devices for faster inference.\n",
    "\n",
    "#### Data Mining Success Criteria evaluation\n",
    "\n",
    "- **Object Detection AP**: Achieve a Mean Average Precision (mAP) of at least 0.70 for detecting and localizing hazard plates across varied conditions.  \n",
    "  **Outcome:** Not achieved, with an mAP of 0.5303.\n",
    "\n",
    "- **OCR Precision for UN Numbers**: Ensure the Tesseract OCR module achieves high accuracy in reading UN numbers, even under challenging conditions, with a target precision score above 0.95.  \n",
    "  **Outcome:** Tesseract was unable to consistently recognize codes in difficult conditions, so we switched to using the **idefics2 VLM**, which performed significantly better, even with low-quality images. However, accuracy metrics for idefics2 have not been formally evaluated.\n",
    "\n",
    "- **Processing Speed**: Ensure the model achieves a processing time per frame under 100 milliseconds to maintain real-time functionality.  \n",
    "  **Outcome:** Not achieved. The model takes longer to process predictions, and the OCR stage, which involves **idefics2**, also contributes to longer processing times, resulting in a total time greater than 100 milliseconds.\n",
    "\n",
    "- **Environmental Robustness**: Maintain consistent mAP scores across different lighting and weather conditions.  \n",
    "  **Outcome:** Partly achieved. Lighting conditions were well represented in the dataset, and the model performed consistently across different lighting variations. Weather conditions, however, were not included in the dataset, but the model performed reasonably well when evaluated with augmented data. The mAP score was not measured for augmented weather conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
