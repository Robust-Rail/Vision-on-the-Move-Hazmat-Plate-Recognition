{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "\n",
    "The goal is to prepare the data in the videos with labels_dataframe.csv to make it ready for finetuning Faster-R CNN\n",
    "we first take a look at how faster r-cnn finetune data is structered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use the coco json annotation:\n",
    "```dataset/\n",
    "├── train/\n",
    "│   ├── images/\n",
    "│   │   ├── image1.jpg\n",
    "│   │   ├── image2.jpg\n",
    "│   └── annotations/\n",
    "│       └── instances_train.json\n",
    "├── val/\n",
    "│   ├── images/\n",
    "│   │   ├── image1.jpg\n",
    "│   │   ├── image2.jpg\n",
    "│   └── annotations/\n",
    "│       └── instances_val.json\n",
    "\n",
    "Sample COCO JSON annotation for one image:\n",
    "{\n",
    "    \"images\": [\n",
    "        {\"id\": 1, \"file_name\": \"image1.jpg\", \"height\": 480, \"width\": 640},\n",
    "        \n",
    "    ],\n",
    "    \"annotations\": [\n",
    "        {\"id\": 1, \"image_id\": 1, \"category_id\": 1, \"bbox\": [100, 200, 50, 50], \"area\": 2500, \"iscrowd\": 0}\n",
    "    ],\n",
    "    \"categories\": [\n",
    "        {\"id\": 1, \"name\": \"object_class\"}\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = [('train', 0.8), ('test', 0.1), ('val', 0.1)]\n",
    "len(distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data\"\n",
    "video_directory = \"/deepstore/datasets/dmb/ComputerVision/ProRail/Ivg/Videos\"\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(\"Path to video files:\", video_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + \"/labels_dataframe.csv\")\n",
    "videos = df[\"Source\"].unique()\n",
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/labels_dataframe.csv')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns we need for COCO are:\n",
    "\n",
    "images:\n",
    "- ID\n",
    "- File name\n",
    "- height/width of the picture\n",
    "\n",
    "annotations:\n",
    "- ID\n",
    "- Image_id\n",
    "- category ID\n",
    "- bounding box (coordinates)\n",
    "- area (oppervlakte van bounding box)\n",
    "- iscrowd (used to indicate whether an object is part of a \"crowd\" or a group of objects that cannot be easily separated)\n",
    "\n",
    "categories:\n",
    "- ID\n",
    "- Category class (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_videos = os.listdir(video_directory)\n",
    "available_videos = [video for video in available_videos if video.endswith(\".mp4\")]\n",
    "available_videos = [video for video in available_videos if video in videos]\n",
    "available_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = df[df[\"Source\"].isin(available_videos)]\n",
    "total_frames = total_frames[\"Absolute Frame\"].count()\n",
    "total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_path = \"./data/data_faster_rcnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frame(frame, video, frame_number, path, subpath, overwrite=True):\n",
    "    \"\"\"\n",
    "    Save a frame to disk with a formatted filename.\n",
    "\n",
    "    Args:\n",
    "        frame (numpy.ndarray): The frame to save.\n",
    "        video (str): The name of the video file (without extension).\n",
    "        frame_number (int): The frame number.\n",
    "        path (str): The directory to save the frame.\n",
    "        overwrite (bool): Whether to overwrite existing files.\n",
    "    \"\"\"\n",
    "    new_path = os.path.join(path, subpath, \"images\")\n",
    "    os.makedirs(new_path, exist_ok=True)\n",
    "    formatted_frame_number = f\"{frame_number:05d}\"\n",
    "    image_path = f\"{new_path}/{video}_{formatted_frame_number}.jpg\"\n",
    "    # print(f\"🎞️ Saving frame {image_path}...\")\n",
    "    if not overwrite and os.path.exists(image_path):\n",
    "        return\n",
    "    cv2.imwrite(image_path, frame)\n",
    "\n",
    "\n",
    "coco_categories = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"hazmat\",\n",
    "    }\n",
    "]\n",
    "coco_images = {\"train\": [], \"test\": [], \"val\": []}\n",
    "coco_annotations = {\"train\": [], \"test\": [], \"val\": []}\n",
    "\n",
    "\n",
    "def add_annotation(video, dist, frame_number, video_h, video_w, annotations):\n",
    "    global coco_annotations\n",
    "    global coco_images\n",
    "    global coco_categories\n",
    "    image_id = len(coco_images[dist]) + 1\n",
    "    if dist not in coco_images:\n",
    "        coco_images[dist] = []\n",
    "    coco_images[dist].append(\n",
    "        {\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": f\"{video}_{frame_number:05d}.jpg\",\n",
    "            \"width\": video_w,\n",
    "            \"height\": video_h,\n",
    "        }\n",
    "    )\n",
    "    for index, annotation in annotations.iterrows():\n",
    "        x_left_top = annotation[\"XTL\"]\n",
    "        y_left_top = annotation[\"YTL\"]\n",
    "        x_right_bottom = annotation[\"XBR\"]\n",
    "        y_right_bottom = annotation[\"YBR\"]\n",
    "        # get the bounding box\n",
    "        width = x_right_bottom - x_left_top\n",
    "        height = y_right_bottom - y_left_top\n",
    "        # calculate the area\n",
    "        area = width * height\n",
    "        # make the bbox\n",
    "        bbox = [x_left_top, y_left_top, width, height]\n",
    "        if dist not in coco_annotations:\n",
    "            coco_annotations[dist] = []\n",
    "        coco_annotations[dist].append(\n",
    "            {\n",
    "                \"id\": len(coco_annotations[dist]) + 1,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": 1,\n",
    "                \"bbox\": bbox,\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "def get_rnd_distribution():\n",
    "    new_dist = distribution.copy()\n",
    "    while len(new_dist) > 0:\n",
    "        rnd_dist = random.choice(new_dist)\n",
    "        required_amount = int(total_frames * rnd_dist[1])\n",
    "        if rnd_dist[0] not in coco_annotations:\n",
    "            coco_annotations[rnd_dist[0]] = []\n",
    "        if required_amount >= len(coco_annotations[rnd_dist[0]]):\n",
    "            return rnd_dist\n",
    "        else:\n",
    "            new_dist.remove(rnd_dist)\n",
    "    \n",
    "    return distribution[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=total_frames) as pbar:\n",
    "    for video in available_videos:\n",
    "        video_path = f\"{video_directory}/{video}\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        video_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        video_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        number_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_number = 0\n",
    "        video_name = video.split('.')[0]\n",
    "        while frame_number < number_frames:\n",
    "            ret, frame = cap.read()\n",
    "            pbar.set_description(f\"Processing {video}, frame {frame_number}/{number_frames}\")\n",
    "            if not ret:\n",
    "                break\n",
    "            annotations = df[(df[\"Source\"] == video) & (df[\"Relative Frame\"] == frame_number)]\n",
    "            if not annotations.empty:\n",
    "                sub_path = get_rnd_distribution()[0]\n",
    "                # take rnd distribution if not full and then store in it.\n",
    "                save_frame(frame, video_name, frame_number, frames_path,sub_path)\n",
    "                add_annotation(video_name, sub_path, frame_number, video_h, video_w, annotations)\n",
    "                pbar.update(annotations.shape[0])\n",
    "            frame_number += 1\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(coco_annotations['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dist in distribution:\n",
    "    path = f\"{frames_path}/{name}/annotations\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(f\"{frames_path}/{name}/annotations/instances_{name}.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"images\": coco_images[name],\n",
    "                \"annotations\": coco_annotations[name],\n",
    "                \"categories\": coco_categories,\n",
    "            },\n",
    "            f,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_annotations(annotation,dist_name):\n",
    "    # coco_images is array of dictionaries with keys: id, file_name, width, height\n",
    "    image_name = [\n",
    "        image[\"file_name\"]\n",
    "        for image in coco_images[dist_name]\n",
    "        if image[\"id\"] == annotation[\"image_id\"]\n",
    "    ]\n",
    "    # open the image\n",
    "    image = cv2.imread(f\"{frames_path}/{dist_name}/{image_name}/{image_name[0]}\")\n",
    "    # get the bbox\n",
    "    bbox = annotation[\"bbox\"]\n",
    "    # draw the bbox\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    # show the image\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_random_annotation():\n",
    "    dist_name = random.choice(distribution)[0]\n",
    "    return random.choice(coco_annotations[dist_name]), dist_name\n",
    "# check if all the images are in the right folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read the json files\n",
    "def check_images_in_annotations(destination_dir, destination_dir_for_images, max_images=10):\n",
    "    with open(destination_dir) as f:\n",
    "        data = json.load(f)\n",
    "        annotations = data[\"annotations\"]\n",
    "        #shffle\n",
    "        random.shuffle(annotations)\n",
    "        for annotation in annotations[:max_images]:\n",
    "            image_id = annotation[\"image_id\"]\n",
    "            images = data[\"images\"]\n",
    "            image = next((image for image in images if image[\"id\"] == image_id), None)\n",
    "\n",
    "            if image is None:\n",
    "                print(f\"Image not found for annotation: {annotation}\")\n",
    "            else:\n",
    "                # show the image with bounding box\n",
    "                # Read the image\n",
    "                path_im = destination_dir_for_images + \"/\"+ image[\"file_name\"]\n",
    "                image = cv2.imread(path_im)\n",
    "                if image is None:\n",
    "                    print(f\"Image not found: {path_im}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert BGR (OpenCV format) to RGB (Matplotlib format)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Extract bounding box coordinates\n",
    "                x, y, w, h = annotation[\"bbox\"]\n",
    "                x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "\n",
    "                # Plot the image\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.imshow(image)\n",
    "\n",
    "                # Draw the bounding box\n",
    "                plt.gca().add_patch(plt.Rectangle((x, y), w, h, edgecolor='green', facecolor='none', linewidth=2))\n",
    "\n",
    "                # Display the image with the bounding box\n",
    "                plt.title(f\"Image ID: {image_id}\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "for name, dist in distribution:\n",
    "    print(f\"Checking {name}\")\n",
    "    check_images_in_annotations(\n",
    "        frames_path + f\"/{name}/annotations/instances_{name}.json\", \n",
    "        frames_path + f\"/{name}/images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def count_files_in_directory(directory_path):\n",
    "    try:\n",
    "        # List all items in the directory\n",
    "        all_items = os.listdir(directory_path)\n",
    "        # Filter only files\n",
    "        files = [item for item in all_items if os.path.isfile(os.path.join(directory_path, item))]\n",
    "        return len(files)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Example usage\n",
    "img_path = \"./data/data_faster_rcnn\"\n",
    "print(count_files_in_directory(img_path+ \"/train/images\"))\n",
    "print(count_files_in_directory(img_path+ \"/val/images\"))\n",
    "print(count_files_in_directory(img_path+ \"/test/images\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rnd_distribution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
