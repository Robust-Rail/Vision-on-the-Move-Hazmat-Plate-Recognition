{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OCR techniques used in the research: Tesseract, EasyOCR and Idefics2 need to be evaluated. The metrics used for that are CER(Character Error Rate) and WER (Word Error Rate). According the fact that in the usecases there are no words but codes, the ground truth will be compared with the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import easyocr\n",
    "from tqdm import tqdm\n",
    "from transformers import Idefics2Processor, Idefics2ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "import os\n",
    "import Levenshtein\n",
    "import json\n",
    "import easyocr\n",
    "import pytesseract\n",
    "import regex as re\n",
    "import numpy as np\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download('stanislavlevendeev/haz-mat-signs')\n",
    "df_dataset = pd.read_csv(path + '/public_dataset.csv')\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models and processors\n",
    "# Init  idefics2\n",
    "processor = Idefics2Processor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "print('Processor loaded')\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "    \"HuggingFaceM4/idefics2-8b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device,\n",
    "    quantization_config=quantization_config,   \n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the image and extract two key values:\n",
    "\n",
    "    The UN number visible on the upper part of the placard.\n",
    "    The code visible on the lower part of the placard, located below the horizontal line separating the two sections.\n",
    "\n",
    "Both codes are printed in black. If either the upper or lower part cannot be detected, replace the missing value with \"0\" Output the extracted values as plain text, separated by a comma if multiple codes are present. No additional context or formatting is needed.\n",
    "\n",
    "Input Examples:\n",
    "\n",
    "    {98 {line} 4567}\n",
    "    (not found, {line}, 8901)\n",
    "    {101 {line} 3345}\n",
    "    (not found, {line}, {not found})\n",
    "    {45 {line} 2789}\n",
    "    {22 {line} 5678}\n",
    "\n",
    "Desired Output:\n",
    "\n",
    "    98, 4567\n",
    "    00, 8901\n",
    "    101, 3345\n",
    "    00, 0000\n",
    "    45, 2789\n",
    "    22, 5678\n",
    "\n",
    "Expected Transformation:\n",
    "\n",
    "    For each input example, extract the UN number and the code below the horizontal line.\n",
    "    If either part is missing (i.e., \"not found\"), replace it with 0.\n",
    "    Output the extracted values as plain text, separated by a comma, without any additional context or formatting.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "text = processor.apply_chat_template(messages, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluation = {\n",
    "    \"idefics\": {\n",
    "        \"images\": [],\n",
    "        \"WER\": 0,\n",
    "        \"CER\": 0,\n",
    "    },\n",
    "    \"easyocr\": {\n",
    "        \"images\": [],\n",
    "        \"WER\": 0,\n",
    "        \"CER\": 0,\n",
    "    },\n",
    "    \"tesseract\": {\n",
    "        \"images\": [],\n",
    "        \"WER\": 0,\n",
    "        \"CER\": 0,\n",
    "    },\n",
    "}\n",
    "\n",
    "def initEvaluation():\n",
    "    global evaluation \n",
    "    evaluation = {\n",
    "        \"idefics\": {\n",
    "            \"images\": [],\n",
    "            \"WER\": 0,\n",
    "            \"CER\": 0,\n",
    "        },\n",
    "        \"easyocr\": {\n",
    "            \"images\": [],\n",
    "            \"WER\": 0,\n",
    "            \"CER\": 0,\n",
    "        },\n",
    "        \"tesseract\": {\n",
    "            \"images\": [],\n",
    "            \"WER\": 0,\n",
    "            \"CER\": 0,\n",
    "        },\n",
    "    }\n",
    "\n",
    "def read_image(source, frame):\n",
    "    source = source.split('.')[0]\n",
    "    frame = str(frame).zfill(5)\n",
    "    path = f'{annotations_path}/train/{source}_{frame}.jpg'\n",
    "    image = None\n",
    "    if os.path.exists(path):\n",
    "        image = Image.open(path)\n",
    "    path = f'{annotations_path}/test/{source}_{frame}.jpg'\n",
    "    if os.path.exists(path):\n",
    "        image = Image.open(path)\n",
    "    path = f'{annotations_path}/val/{source}_{frame}.jpg'\n",
    "    if os.path.exists(path):\n",
    "        image = Image.open(path)\n",
    "    if image is None:\n",
    "        print(f'Image not found: {path}')\n",
    "    return image\n",
    "# Define the method\n",
    "def extract_bounding_box(self, bbox):\n",
    "    \"\"\"\n",
    "    Extract a bounding box from the image.\n",
    "    \n",
    "    :param bbox: A tuple of (left, top, right, bottom) coordinates.\n",
    "    :return: A new Image object containing the cropped region.\n",
    "    \"\"\"\n",
    "    return self.crop(bbox)\n",
    "\n",
    "def perform_ocr(image): \n",
    "    \n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "    generated_text = model.generate(**inputs, max_new_tokens=500)\n",
    "    generated_text = processor.batch_decode(generated_text, skip_special_tokens=True)[0]\n",
    "    assistant_output = generated_text.split(\"Assistant:\")[1].strip()\n",
    "    # Split the output by comma to get the individual numbers\n",
    "    numbers = assistant_output.split(\",\")\n",
    "    # Strip any leading or trailing whitespace from the numbers\n",
    "    numbers = [number.strip().replace('.','') for number in numbers]\n",
    "    numbers.append('0000') \n",
    "    numbers.append('0000')\n",
    "    un_number, hin_number = numbers[:2]\n",
    "    return un_number, hin_number\n",
    "def get_text_from_image_easyocr(image):\n",
    "    # Initialize the reader for digits\n",
    "    reader = easyocr.Reader([\"en\"])\n",
    "    result = reader.readtext(image, allowlist=\"0123456789\",detail=0)\n",
    "    h,w = None, None\n",
    "    try:\n",
    "        h, w = image.shape\n",
    "    except:\n",
    "        h,w,_ = image.shape\n",
    "    image_un = image[0:int(h/2), 0:w]\n",
    "    image_hin = image[int(h/2):h, 0:w]\n",
    "    result_un = reader.readtext(image_un, allowlist=\"0123456789\",detail=0)\n",
    "    result_hin = reader.readtext(image_hin, allowlist=\"0123456789\",detail=0)\n",
    "    result_un = result_un[0] if len(result_un) > 0 else '00'\n",
    "    result_hin = result_hin[0] if len(result_hin) > 0 else '0000'\n",
    "    return result_un,result_hin,result\n",
    "def extract_un_number(text):\n",
    "    un = re.findall(r'\\d{2,}', text)\n",
    "    un = un[0] if len(un) > 0 else '00'\n",
    "    return un\n",
    "\n",
    "def extract_hin_number(text):\n",
    "    hin = re.findall(r'\\d{4,}', text)\n",
    "    hin = hin[0] if len(hin) > 0 else '0000'\n",
    "    return hin\n",
    "def get_text_from_image_ocr(image):\n",
    "    #split image horizontally in two pieces\n",
    "    h, w = image.shape[:2]\n",
    "    image_upper = image[0:int(h/2), 0:w]\n",
    "    image_lower = image[int(h/2):h, 0:w]\n",
    "    psm = 6\n",
    "    option = f\"--psm {psm}\"\n",
    "    text_un = pytesseract.image_to_string(image_upper, config=option)\n",
    "    text_hin = pytesseract.image_to_string(image_lower, config=option)\n",
    "    \n",
    "    return extract_un_number(text_un), extract_hin_number(text_hin)\n",
    "def calculate_cer(gt, ocr):\n",
    "    return Levenshtein.distance(gt, ocr) / max(1, len(gt))\n",
    "def calculate_wer(gt, ocr):\n",
    "    gt_words = gt.split()\n",
    "    ocr_words = ocr.split()\n",
    "    return Levenshtein.distance(\" \".join(gt_words), \" \".join(ocr_words)) / max(1, len(gt_words))\n",
    "def to_cv2(self):\n",
    "    \"\"\"\n",
    "    Convert a PIL Image to an OpenCV image (numpy array) in grayscale.\n",
    "    \n",
    "    :return: Grayscale OpenCV image (numpy array)\n",
    "    \"\"\"\n",
    "    # Convert PIL Image to numpy array\n",
    "    numpy_image = np.array(self)\n",
    "    \n",
    "    # Convert RGB to BGR (OpenCV format)\n",
    "    opencv_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Convert BGR to Grayscale\n",
    "    grayscale_image = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    return grayscale_image\n",
    "def store_evaluation(lable_info, ocr_technique, prediction, gt):\n",
    "    cer_un = calculate_cer(gt[0], prediction[0])\n",
    "    wer_un = calculate_wer(gt[0], prediction[0])\n",
    "    cer_hin = calculate_cer(gt[1], prediction[1])\n",
    "    wer_hin = calculate_wer(gt[1], prediction[1])\n",
    "    evaluation[ocr_technique]['WER'] += (wer_un + wer_hin)/2\n",
    "    evaluation[ocr_technique]['CER'] += (cer_un + cer_hin)/2\n",
    "    evaluation[ocr_technique]['images'].append({\n",
    "        **lable_info,\n",
    "        'prediction': prediction,\n",
    "        'gt': gt,\n",
    "        'CER': (cer_un + cer_hin)/2,\n",
    "        'WER' : (wer_un + wer_hin)/2,\n",
    "    })\n",
    "def print_results():\n",
    "    number_lables = len(evaluation['idefics']['images'])\n",
    "    print('Evaluation  Idefics:')\n",
    "    print('Number of lables:', number_lables)\n",
    "    print ('Raw CER:', evaluation['idefics']['CER'])\n",
    "    print ('Raw WER:',  evaluation['idefics']['WER'])\n",
    "    print('CER:',  evaluation['idefics']['CER']/number_lables)\n",
    "    print('WER:',  evaluation['idefics']['WER']/number_lables)\n",
    "    print('Evaluation  EasyOCR:')\n",
    "    print('Number of lables:', number_lables)\n",
    "    print ('Raw CER:', evaluation['easyocr']['CER'])\n",
    "    print ('Raw WER:',  evaluation['easyocr']['WER'])\n",
    "    print('CER:',  evaluation['easyocr']['CER']/number_lables)\n",
    "    print('WER:',  evaluation['easyocr']['WER']/number_lables)\n",
    "    print('Evaluation  Tesseract:')\n",
    "    print('Number of lables:', number_lables)\n",
    "    print ('Raw CER:',  evaluation['tesseract']['CER'])\n",
    "    print ('Raw WER:', evaluation['tesseract']['WER'])\n",
    "    print('CER:', evaluation['tesseract']['CER']/number_lables)\n",
    "    print('WER:', evaluation['tesseract']['WER']/number_lables)\n",
    "\n",
    "# Add the method to the Image class\n",
    "Image.Image.extract_bounding_box = extract_bounding_box\n",
    "Image.Image.to_cv2 = to_cv2\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_result(res):\n",
    "    print('Saving evaluation')\n",
    "    with open('evaluation.json', 'w') as f:\n",
    "        json.dump(res, f)\n",
    "    print('Evaluation saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on private dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotations_path = kagglehub.dataset_download(\"stanislavlevendeev/hazmat-detection\")\n",
    "annotations_path = annotations_path.replace('\\\\', '/')\n",
    "path_video = os.environ['PATH_TO_DATA']\n",
    "data_path = annotations_path + '/labels_dataframe.csv'\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv(data_path)\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_sources = [f for f in os.listdir(path_video) if f.endswith('.mp4')]\n",
    "available_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by in daaataframe\n",
    "df_labels_grouped = df_labels.groupby(['Source', 'Relative Frame']).size().reset_index().iloc[:, :4]\n",
    "df_labels_grouped = df_labels_grouped[df_labels_grouped['Source'].isin(available_sources)]\n",
    "df_labels_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop throuw grouped df_lables_grouped\n",
    "annotations_path = annotations_path + '/yolo/images'\n",
    "annotations_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lables = 0\n",
    "\n",
    "for index, frames in tqdm(df_labels_grouped.head(100).iterrows(), total=df_labels_grouped.shape[0]):\n",
    "\n",
    "    # Read image\n",
    "\n",
    "    image = read_image(frames['Source'], frames['Relative Frame'])\n",
    "\n",
    "    if image is None:\n",
    "\n",
    "        continue\n",
    "\n",
    "    data_annotation = df_labels[(df_labels['Source'] == frames['Source']) & (df_labels['Relative Frame'] == frames['Relative Frame'])]\n",
    "\n",
    "    #for loop\n",
    "\n",
    "    for index, lable in data_annotation.iterrows():\n",
    "\n",
    "        number_lables +=1\n",
    "\n",
    "        # Crop image\n",
    "\n",
    "        cropped = image.extract_bounding_box((int(lable['XTL']), int(lable['YTL']), int(lable['XBR']), int(lable['YBR'])))\n",
    "\n",
    "        # OCR\n",
    "        lable_info = {\n",
    "            'video': frames['Source'],\n",
    "            'frame': frames['Relative Frame'],\n",
    "            'XTL': int(lable['XTL']),\n",
    "            'YTL': int(lable['YTL']),\n",
    "            'XBR': int(lable['XBR']),\n",
    "            'YBR': int(lable['YBR']),\n",
    "        }\n",
    "        actual_code = str(lable['Code']).split('/')\n",
    "        cropped_cv2 = cropped.to_cv2()\n",
    "        # EasyOCR\n",
    "        un, hin, ocr = get_text_from_image_easyocr(cropped_cv2)\n",
    "        prediction_easyocr = (un, hin)\n",
    "        store_evaluation(lable_info, 'easyocr', prediction_easyocr, actual_code)\n",
    "        # Tesseract\n",
    "        un, hin = get_text_from_image_ocr(cropped_cv2)\n",
    "        prediction_ocr = (un, hin)\n",
    "        store_evaluation(lable_info, 'tesseract', prediction_ocr, actual_code)\n",
    "        # Idefics\n",
    "        prediction_idefics = perform_ocr(cropped)\n",
    "        store_evaluation(lable_info, 'idefics', prediction_idefics, actual_code)\n",
    "\n",
    "print('Number of lables:', number_lables)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_evaluation(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_data_annotation = kagglehub.dataset_download(\"stanislavlevendeev/haz-mat-signs\")\n",
    "public_data_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_labels = pd.read_csv(os.path.join(public_data_annotation, 'images_with_boxes.csv'))\n",
    "df_public_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list fiels from a directory\n",
    "files = os.listdir(os.path.join(public_data_annotation, 'images'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through files using progress bar\n",
    "for file in tqdm(files):\n",
    "    image = Image.open(os.path.join(public_data_annotation, 'images', file))\n",
    "    # get the lables \n",
    "    image_id = int(file.split('.')[0])\n",
    "    print(image_id)\n",
    "    data_annotation = df_public_labels[df_public_labels['image_id'] == image_id]\n",
    "    for index, lable in data_annotation.iterrows():\n",
    "        # Crop image\n",
    "        xtl, ytl, xbr, ybr = int(lable['box_xtl']), int(lable['box_ytl']), int(lable['box_xbr']), int(lable['box_ybr'])\n",
    "        cropped = image.extract_bounding_box((xtl, ytl, xbr, ybr))\n",
    "        # display cropped image\n",
    "        cropped.show()\n",
    "        lable_info = {\n",
    "            'image_path': file,\n",
    "            'XTL': xtl,\n",
    "            'YTL': ytl,\n",
    "            'XBR': xbr,\n",
    "            'YBR': ybr,\n",
    "        }\n",
    "        actual_code = str(lable['code']).split('/')\n",
    "        cropped_cv2 = cropped.to_cv2()\n",
    "        # OCR\n",
    "        # EasyOCR\n",
    "        un, hin, ocr = get_text_from_image_easyocr(cropped_cv2)\n",
    "        prediction_easyocr = (un, hin)\n",
    "        store_evaluation(lable_info, 'easyocr', prediction_easyocr, actual_code)\n",
    "        # Tesseract\n",
    "        un, hin = get_text_from_image_ocr(cropped_cv2)\n",
    "        prediction_ocr = (un, hin)\n",
    "        store_evaluation(lable_info, 'tesseract', prediction_ocr, actual_code)\n",
    "        # Idefics\n",
    "        prediction_idefics = perform_ocr(cropped)\n",
    "        store_evaluation(lable_info, 'idefics', prediction_idefics, actual_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results()\n",
    "evaluation['idefics']['images']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open json file\n",
    "evaluation = None\n",
    "with open('evaluation.json') as f:\n",
    "    evaluation = json.load(f)\n",
    "#make datagframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idefics = pd.DataFrame(evaluation['idefics']['images'])\n",
    "# sort by CER\n",
    "df_idefics = df_idefics.sort_values(by=['CER'], ascending=False)\n",
    "df_idefics.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idefics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocr = pd.DataFrame(evaluation['tesseract']['images'])\n",
    "df_ocr = df_ocr.sort_values(by=['CER'], ascending=True)\n",
    "df_ocr.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_easy = pd.DataFrame(evaluation['easyocr']['images'])\n",
    "df_easy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display images where the CER is higher than 0.5\n",
    "df_low_precision = df_idefics[df_idefics['CER'] > 0.5]\n",
    "for(index, row) in df_low_precision.iterrows():\n",
    "    image = cv2.imread(row[\"path\"])\n",
    "    #draw bounding box and draw prediiction and gt array \n",
    "    image = cv2.rectangle(image, (row[\"XTL\"], row[\"YTL\"]), (row[\"XBR\"], row[\"YBR\"]), (0, 255, 0), 2)\n",
    "    cv2.putText(image, f'Prediction: {row[\"prediction\"]}', (row[\"XTL\"], row[\"YTL\"] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    cv2.putText(image, f'GT: {row[\"gt\"]}', (row[\"XTL\"], row[\"YTL\"] - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    # Display the image in console\n",
    "    cv2.imshow(\"Image\", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_public_image(image_id):\n",
    "    file = str(image_id) + '.png'\n",
    "    # c heck if file exists\n",
    "    if not os.path.exists(os.path.join(public_data_annotation, 'images', file)):\n",
    "        print('File not found:', file)\n",
    "        file = str(image_id) + '.jpg'\n",
    "    \n",
    "    image = Image.open(os.path.join(public_data_annotation, 'images', file))\n",
    "    image = image.convert(\"RGB\")\n",
    "    data_annotation = df_public_labels[df_public_labels['image_id'] == image_id]\n",
    "    for index, lable in data_annotation.iterrows():\n",
    "        # Crop image\n",
    "        xtl, ytl, xbr, ybr = int(lable['box_xtl']), int(lable['box_ytl']), int(lable['box_xbr']), int(lable['box_ybr'])\n",
    "        cropped = image.extract_bounding_box((xtl, ytl, xbr, ybr))\n",
    "        # display cropped image\n",
    "        cropped.save('./data/labels/cropped_' + str(index) +file)\n",
    "        lable_info = {\n",
    "            'image_path': file,\n",
    "            'XTL': xtl,\n",
    "            'YTL': ytl,\n",
    "            'XBR': xbr,\n",
    "            'YBR': ybr,\n",
    "        }\n",
    "        \n",
    "        actual_code = str(lable['code']).split('/')\n",
    "        cropped_cv2 = cropped.to_cv2()\n",
    "        # OCR\n",
    "        # EasyOCR\n",
    "        un, hin, ocr = get_text_from_image_easyocr(cropped_cv2)\n",
    "        prediction_easyocr = (un, hin)\n",
    "        # Tesseract\n",
    "        un, hin = get_text_from_image_ocr(cropped_cv2)\n",
    "        prediction_ocr = (un, hin)\n",
    "        prediction_idefics = perform_ocr(cropped)\n",
    "        # print the results and ground truth\n",
    "        print('Tesseract:', prediction_ocr)\n",
    "        print('EasyOCR:', prediction_easyocr)\n",
    "        print('Idefics:', prediction_idefics)\n",
    "        print('Ground Truth:', actual_code)\n",
    "        # print author\n",
    "        adder, author = get_author(image_id)\n",
    "        print('Adder:', adder)\n",
    "        print(\"Author:\", author)\n",
    "        \n",
    "def get_author(imageid):\n",
    "    # get row from df \n",
    "    row = df_dataset[df_dataset['Image_id'] == imageid]\n",
    "    author = row['Author'].values[0]\n",
    "    # get adder\n",
    "    adder = row['Added_by'].values[0]\n",
    "    return adder, author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_public_image(227) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adder, author = get_author(218)\n",
    "print('Adder:', adder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
