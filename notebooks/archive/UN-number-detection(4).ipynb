{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# UN number detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "For this project, we have been tasked with developing a machine learning model capable of recognizing UN number hazard plates. These plates, commonly displayed on freight train wagons, indicate the types of hazardous materials being transported. The successful implementation of this model will contribute to a more efficient and secure railway system across the EU.\n",
    "\n",
    "<img src=\"images/hazard_plate.jpg\" alt=\"Hazard Plate\" width=\"500\"/>\n",
    "\n",
    "The hazard plates play a crucial role in ensuring the safety of transportation by providing essential information about the nature of the substances on board, such as flammability, toxicity, or corrosiveness. By automating the recognition process with machine learning, the handling and tracking of these hazardous materials can be streamlined, reducing manual labor and minimizing potential human errors.\n",
    " Determine business objectives\n",
    "\n",
    "### Determine business objectives\n",
    "#### Background\n",
    "The specific expectations and objectives of the EU for this project are not yet fully defined, but the initiative's roots are clear. This project is spearheaded by the University of Twente, with researcher Mellisa Tijink serving as our supervisor. Our team, composed of pre-master's Computer Science students, has been tasked with developing the machine learning model. Mellisa Tijink plays a pivotal role as the intermediary between our team and major stakeholders, including ProRail, the EU, and other experts in the field.\n",
    "\n",
    "This project is part of a broader initiative aimed at enhancing rail freight operations within Europe, aligning with the EU’s goals for improved efficiency and safety. More information on the initiative can be found on the official project site: [EU Rail FP5](https://projects.rail-research.europa.eu/eurail-fp5/).\n",
    "\n",
    "Flagship Project 5: *TRANS4M-R aims to establish rail freight as the backbone of a low-emission, resilient European logistics chain that meets end-user needs. It focuses on two main technological clusters: 'Full Digital Freight Train Operation' and 'Seamless Freight Operation', which will develop and demonstrate solutions to increase rail capacity, efficiency, and cross-border coordination. By integrating Digital Automatic Coupler (DAC) solutions with software-defined systems, the project seeks to optimize network management and enhance cooperation among infrastructure managers. The ultimate goal is to create an EU-wide, interoperable rail freight framework with unified technologies and seamless operations across borders and various stakeholders, boosting the EU transport and logistics sector.*\n",
    "\n",
    "#### Business objectives\n",
    "\n",
    "**Primary Objective:** Develop an object detection model for UN number hazard plates on freight wagons.\n",
    "\n",
    "**Sub-objectives:**\n",
    "1. Detect and identify UN number hazard plates: Ensure the model can accurately locate hazard plates on freight wagons. \n",
    "2. Read and interpret the UN numbers: Implement recognition capabilities to accurately read the numbers on the detected plates.\n",
    "3. Ensure model robustness and accuracy: Train the model to achieve high accuracy and reliability under various conditions (e.g., different lighting, weather).\n",
    "4. Optimize model for speed: Make sure the model runs efficiently and in real-time to function on moving trains.\n",
    "5. Adapt the model for moving environments: Design and test the model to handle the unique challenges of detecting and reading plates on trains in motion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Assess Situation\n",
    "\n",
    "#### Inventory of resources\n",
    "\n",
    "**Business Experts:** Our team currently lacks extensive expertise in this area. We can consult Melissa for some questions, and we have an upcoming interview with a Swedish expert in the field of UN number hazard plates.\n",
    "\n",
    "**Data Mining Team:** \n",
    "- Melissa Tijink (Researcher in Data Management & Biometrics/Electrical Engineering, Mathematics, and Computer Science)\n",
    "- Ewaldo Nieuwenhuis (Pre-master student in Computer Science)\n",
    "- Stanislav Levendeev (Pre-master student in Computer Science)\n",
    "\n",
    "**Data:**\n",
    "1. **Video Data of Freight Trains:** This consists of video footage of moving freight trains, where the freight wagons should display the UN numbers.\n",
    "2. **Line Scan Camera Pictures:** These are high-resolution images of the train, but they are very spread out. It is still uncertain if these will be useful.\n",
    "3. **Photos of ADR Warning Signs:** These are images of ADR signs on freight trains. However, this is not exactly what we need since our objective is to build a model that recognizes UN numbers.\n",
    "\n",
    "**Computing Resources:** We have access to a cluster from the University of Twente, which we can use to train or fine-tune our model.\n",
    "\n",
    "**Software:** We will use Python, Jupyter Notebook, Keras, PyTorch, and TensorFlow for analyzing, cleaning, preparing the data, and modeling. For data labeling, we will use [CVAT](https://www.cvat.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Requirements, assumptions, and constraints\n",
    "\n",
    "##### Requirements\n",
    "- Object detection capability for UN number hazard plates.\n",
    "- Text recognition to read and extract UN numbers.\n",
    "- High accuracy and precision in detection and recognition.\n",
    "- Robust performance under varying conditions (weather, lighting, speed).\n",
    "- Speed optimization for fast processing with minimal lag\n",
    "- Real-time processing for operation on moving trains.\n",
    "\n",
    "##### Assumptions\n",
    "- Consistent access to a high-performance computational cluster for model training and testing.\n",
    "- The high-performance cluster is necessary due to the heavy processing demands of deep learning models.\n",
    "- Local machines are not sufficient for the required high computational tasks.\n",
    "- Project-specific data, including images and videos of freight trains with hazard plates, will be provided as planned.\n",
    "- Data will include varied conditions (different lighting and weather) to ensure robustness.\n",
    "- Access to diverse data is essential for creating a model that generalizes well to real-world scenarios.\n",
    "- If the planned data is unavailable, additional time will be needed to source and prepare alternative public datasets.\n",
    "- Sourcing alternative datasets may affect the project timeline and the quality of the final outcomes.\n",
    "- The stakeholders will provide timely feedback to guide any changes or adaptations needed in the project.\n",
    "\n",
    "##### Constraints\n",
    "- The team has restricted experience with advanced object detection methods, which may impact the initial development and refinement of the model.\n",
    "- Most of the available data is not labeled, presenting a challenge for training supervised machine learning models. Some labeled data exists but belongs to another researcher, and access to it is uncertain.\n",
    "- The project must be completed within a short, 9-week period, which constrains the depth and breadth of potential research and model development.\n",
    "- The dataset may be skewed with an overrepresentation of specific UN numbers from certain wagons, which could limit the model's ability to generalize across different scenarios.\n",
    "- The size of the dataset makes it difficult to filter out specific wagons or relevant segments efficiently, posing a challenge for data processing and targeted training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### Risks and Contingencies\n",
    "\n",
    "**1. Lack of Data Access:**  \n",
    "*Risk:* Currently, we do not have access to the necessary video or linescan data, and there is a risk that we may never obtain it.  \n",
    "*Contingency Action:* Search for publicly available open-source datasets containing UN codes to proceed with model training and development.\n",
    "\n",
    "**2. Loss of Access to the Computational Cluster:**  \n",
    "*Risk:* While we currently have access to a high-performance cluster for training, loading, and fine-tuning models, there is a chance of losing this access due to technical failures or maintenance issues.  \n",
    "*Contingency Action:* Prepare to train, load, and fine-tune a smaller version of the model locally on personal computers.\n",
    "\n",
    "**3. Unavailability of Labeling Software:**  \n",
    "*Risk:* We plan to label the data with the help of our supervisor, Melissa, which is essential for fine-tuning and evaluating the model. If this step is delayed or cannot occur, it will impede progress.  \n",
    "*Contingency Action:* Learn how to use CVAT (Computer Vision Annotation Tool) and set it up on personal laptops to carry out data labeling independently.\n",
    "\n",
    "**4. Inaccessibility of Personal Laptops:**  \n",
    "*Risk:* Access to our laptops is crucial for development, data handling, and connecting to the cluster. If our laptops become unusable due to malfunction, our work will be disrupted.  \n",
    "*Contingency Action:* Use backup laptops that are ready for project work to ensure continuity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "#### Terminology\n",
    "\n",
    "**Business Terminology:**\n",
    "- **UN Number Hazard Plates**: Identification plates with UN numbers that indicate the nature of hazardous materials, improving safety during transport.\n",
    "- **Freight Trains**: Trains used for transporting goods, especially relevant when carrying hazardous materials.\n",
    "- **Flagship Project 5**: A project within the European \"Europe’s Rail\" initiative, focused on applying technologies to enhance rail transport safety.\n",
    "- **ADR (European Agreement concerning the International Carriage of Dangerous Goods by Road)**: International regulations governing the transport of hazardous goods.\n",
    "- **ProRail**: The Dutch railway network manager responsible for maintaining the railways.\n",
    "- **Line-Scan Camera**: A camera that captures images one line at a time for capturing objects like fast-moving trains.\n",
    "\n",
    "**Data Mining Terminology:**\n",
    "- **CRISP-DM (Cross-Industry Standard Process for Data Mining)**: A widely used methodology for managing data mining projects, consisting of six phases:\n",
    "  - **Business Understanding**: Defining objectives from a business perspective.\n",
    "  - **Data Understanding**: Collecting and analyzing data to gain insights.\n",
    "  - **Data Preparation**: Preparing data, such as annotating and normalizing, for model training.\n",
    "  - **Modeling**: Selecting and training models for the desired task.\n",
    "  - **Evaluation**: Assessing model performance using specific metrics.\n",
    "  - **Deployment**: Implementing the model in real-world applications.\n",
    "\n",
    "- **Object Detection**: Identifying specific objects (e.g., hazard plates) within images or videos.\n",
    "- **Optical Character Recognition (OCR)**: Extracting text from images, used here to read numbers on hazard plates.\n",
    "- **YOLO (You Only Look Once)**: A fast object detection model ideal for real-time applications.\n",
    "- **Faster R-CNN**: A more accurate but slightly slower object detection model, suitable for complex environments.\n",
    "- **Annotation**: Marking data (e.g., video frames) with labels like bounding boxes to create ground truth for model training.\n",
    "- **Bounding Boxes**: Rectangular boxes used in image processing to define regions of interest around an object.\n",
    "- **Normalization**: Adjusting data to a standard scale to ensure consistency in model input.\n",
    "- **Augmentation**: Enhancing training data through techniques like contrast adjustment to improve model robustness.\n",
    "- **Average Precision (AP)**: A metric for evaluating the accuracy of object detection models.\n",
    "- **Tesseract**: A commonly used OCR tool for extracting alphanumeric text from images.\n",
    "- **HOG (Histogram of Oriented Gradients)**: A feature descriptor used in object detection, especially for detecting shapes or text.\n",
    "- **Saliency Detection**: An algorithmic technique to identify key areas within images for focused analysis.\n",
    "- **Support Vector Regression (SVR)**: A machine learning algorithm for regression tasks, sometimes used to create likelihood maps for image processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Determine Data Mining Goals\n",
    "\n",
    "#### Data Mining Goals\n",
    "**Primary Data Mining Goal:** Create and train an object detection model capable of identifying and interpreting UN number hazard plates on freight wagons in real-time.\n",
    "\n",
    "**Specific Data Mining Goals:**\n",
    "1. **Object Detection and Localization**: Develop a model that achieves a high AP score for accurately detecting and localizing hazard plates on freight wagons within each video frame.\n",
    "\n",
    "2. **OCR for UN Number Extraction:** Use Tesseract to apply Optical Character Recognition (OCR) for accurately reading UN numbers on detected plates, aiming to optimize precision and minimize errors in text recognition.\n",
    "\n",
    "3. **Robustness Across Variable Conditions**: Enhance the model’s robustness by training it on datasets representing diverse lighting and weather conditions, with a goal to maintain high AP scores across these environments.\n",
    "\n",
    "4. **Optimization for Real-Time Processing**: Implement real-time object detection and OCR capabilities to ensure the model operates at a frame rate suitable for analyzing images from moving trains.\n",
    "\n",
    "#### Data Mining Success Criteria\n",
    "\n",
    "- **Object Detection AP**: Achieve an Mean Average Precision (mAP) of at least 0.70 for detecting and localizing hazard plates across varied conditions.\n",
    "- **OCR Precision for UN Numbers**: Ensure the Tesseract OCR module achieves high accuracy in reading UN numbers, even under challenging conditions, with a target precision score above 0.95.\n",
    "- **Processing Speed**: Ensure the model achieves a processing time per frame under 100 milliseconds to maintain real-time functionality.\n",
    "- **Environmental Robustness**: Maintain consistent mAP scores across different lighting and weather conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import HTML\n",
    "import kagglehub\n",
    "import torch\n",
    "import pytesseract\n",
    "import regex as re\n",
    "import math as Math\n",
    "import easyocr\n",
    "import numpy as np\n",
    "from transformers import Idefics2Processor, Idefics2ForConditionalGeneration, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "import json\n",
    "import contextlib\n",
    "import io\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "## Collect Initial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing video files\n",
    "video_directory = os.environ[\"PATH_TO_DATA\"]\n",
    "print (video_directory)\n",
    "\n",
    "# Specify output directory for detected frames\n",
    "output_path = os.environ[\"OUTPUT_PATH\"]\n",
    "output_directory = None\n",
    "if not os.path.exists(output_path):\n",
    "    output_directory = './data/output_frames'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "else:\n",
    "    output_directory = output_path\n",
    "print (output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all filenames in the directory\n",
    "video_files = [f for f in os.listdir(video_directory) if f.endswith(('.mp4'))]\n",
    "video_files[0]\n",
    "df = pd.read_csv('data/data_understanding_2024-11-28.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Describe Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "**Column Description:**\n",
    "\n",
    "- **Unnamed: 0**: The ID of the video.\n",
    "- **filename**: The name of the file, including the `.mp4` extension.\n",
    "- **fps**: Frames per second.\n",
    "- **frame_count**: The total number of frames in the video.\n",
    "- **width**: The width of the video in pixels.\n",
    "- **height**: The height of the video in pixels.\n",
    "- **resolution**: The video resolution, expressed as `width x height`.\n",
    "- **duration_seconds**: The video's duration in seconds.\n",
    "- **hash**: Hash of the video to check if it is original\n",
    "- **file_size_mb**: The file size in megabytes.\n",
    "- **train_detected**: Indicates whether a train was detected using a YOLO model. If the model's confidence score exceeded 10%, a train is considered detected, though this may not always be accurate.\n",
    "- **confidence**: The confidence score indicating how likely it is that the video contains a train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_trains = sum(df['train_detected'] == 0)\n",
    "print(f\"On {no_trains} videos there haven't been detected any trains. Of a total of {df.shape[0]} videos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_trains_percentage = no_trains / df.shape[0] * 100\n",
    "print(f\"This is {no_trains_percentage:.1f}% of all videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hash'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "They seem to be all orginal in terms of hashing, we probably cannot determine duplicates by hash alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MP4 video\n",
    "video = \"1690279852.mp4\"\n",
    "video2 = \"1690281303.mp4\"\n",
    "video_path = video_directory+'/'+video\n",
    "video2_path = video_directory+'/'+video2\n",
    "# Embed video in the notebook\n",
    "\n",
    "def get_video_html(video_path):\n",
    "    return HTML(f\"\"\"\n",
    "      <h1>Video {video_path}</h1>\n",
    "      <video width=\"480\" height=\"320\" controls>\n",
    "        <source src=\"{video_path}\" type=\"video/mp4\">\n",
    "        Your browser does not support the video tag.\n",
    "      </video>\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_video_html(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_video_html(video2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "**These are the same videos, the second video is only one second longer than the first video. There are duplicate video's in this dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['train_detected'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['confidence'].isnull() == False]['confidence'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(f\"<b>Minimal confidence: {df['confidence'].min():.2f}, maximal confidence: {df['confidence'].max():.2f}, average confidence: {df['confidence'].mean():.2f}</b>\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_max_confidence = df[df['confidence'] == df['confidence'].max()].iloc[0]['filename']\n",
    "video_min_confidence = df[df['confidence'] == df['confidence'].min()].iloc[0]['filename']\n",
    "\n",
    "print(f\"Video with maximal confidence: {video_max_confidence} ({df[\"confidence\"].max():.2f}%)\")\n",
    "get_video_html(video_directory+\"/\"+video_max_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Video with minimal confidence: {video_min_confidence} ({df[\"confidence\"].min():.2f}%)\")\n",
    "get_video_html(video_directory+\"/\"+video_min_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "**In the second video, trains are detected, but there are no actual trains present. This indicates that the 'train_detected' column lacks certainty. It may be more effective for a human to review and filter videos to identify those with trains and those without. Alternatively, training a model specifically to recognize freight trains could be considered, although this falls outside the scope of this project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "The frames per second (FPS) in your video data can influence the performance of training your model. FPS affects the temporal resolution and the amount of data fed into the model. When training on video data, the FPS determines how many frames are available to capture motion or other temporal patterns, which can influence model performance.\n",
    "\n",
    "For example, if you use a higher FPS, your model will have more frames to analyze within a given time frame, potentially improving its ability to capture finer details in motion (e.g., in object detection or action recognition tasks). However, processing more frames per second can also lead to higher computational costs and may require more memory and processing power, which might reduce training efficiency unless properly optimized.\n",
    "[source 1](https://library.fiveable.me/key-terms/deep-learning-systems/frames-per-second-fps)\n",
    "\n",
    "\n",
    "Conversely, lower FPS can reduce computational demands but may also decrease the temporal resolution of your data, making it harder for your model to accurately capture fast movements or dynamic changes. Depending on your specific use case, you'll need to balance FPS with your model's ability to process the data effectively while managing computational resources.\n",
    "[source 2](https://paulbridger.com/posts/video-analytics-pipeline-tuning/)\n",
    "\n",
    "It's also important to consider other factors like video resolution and preprocessing techniques, which could further affect how FPS influences your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(df, column, xlabel, ylabel=\"Frequency\", bins=10):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(df[column], bins=bins, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    plt.title(f\"Distribution of {xlabel}\", fontsize=14)\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_distribution(df, 'fps', xlabel='Frames Per Second (FPS)')\n",
    "plot_distribution(df, 'frame_count', xlabel='Total Frames', bins=20)\n",
    "plot_distribution(df, 'duration_seconds', xlabel='Duration (seconds)', bins=20)\n",
    "plot_distribution(df, 'file_size_mb', xlabel='File Size (MB)', bins=15)\n",
    "plot_distribution(df, 'resolution', xlabel='Resolution', bins=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_resolutions(df):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(df['width'], df['height'], c='orange', alpha=0.7, edgecolors='black')\n",
    "    plt.title(\"Resolution Scatter Plot (Width vs Height)\", fontsize=14)\n",
    "    plt.xlabel(\"Width (pixels)\", fontsize=12)\n",
    "    plt.ylabel(\"Height (pixels)\", fontsize=12)\n",
    "    plt.grid(linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "plot_resolutions(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "UN number codes with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_un = pd.read_csv(\"./data/un-number-labels.csv\")\n",
    "def get_hin_description(hin):\n",
    "    if(isinstance(hin, int) == False):\n",
    "        hin = int(hin)\n",
    "    hin_row = df_un[df_un['number'] == hin]\n",
    "    return hin_row['description'].values[0] if hin_row.shape[0] > 0 else None\n",
    "df_un.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "\n",
    "See `data-prep-coco.ipynb` and `data-preperation.ipynb`. \n",
    "See `data_augmentation_faster_rcnn.ipynb` for an attempt at data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# Modeling YOLOV11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "YOLO (You Only Look Once) is a popular object detection algorithm known for its speed and accuracy. YOLO models are designed to detect objects in images or video frames by dividing the image into a grid and predicting bounding boxes and class probabilities for each grid cell. YOLOv3 is one of the most widely used versions of the YOLO algorithm, offering a good balance between speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"stanislavlevendeev/hazmat-detection\")\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(path)\n",
    "print(device)\n",
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Training Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.train(\n",
    "    data=path+'\\\\yolo\\\\dataset.yaml', \n",
    "    epochs=10,\n",
    "    scale=0.5,\n",
    "    shear=1.1,\n",
    "    device=device,\n",
    "    degrees=10.5,\n",
    "    perspective=0.5,\n",
    "    mosaic=0.5,\n",
    "    hsv_h=0.015,\n",
    "    hsv_s=0.7,\n",
    "    hsv_v=0.4,\n",
    "    multiscale=True,\n",
    "    )\n",
    "model.save(\"data\\\\yolo\\\\yolo11n_trained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "# Evaluation YOLO v11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def draw_rectangles(image_path, results):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    img_height, img_width, _ = image.shape\n",
    "    boxes = results.boxes   \n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = map(int, box.xyxy[0])  # Convert to integers\n",
    "        confidence = box.conf[0]  # Confidence score\n",
    "        class_id = int(box.cls[0])  # Class ID\n",
    "        label = results.names[class_id]  # Class label\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "        # Put the label and confidence score\n",
    "        cv2.putText(image, f\"{label} {confidence:.2f}\", (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    \n",
    "    # Convert BGR image to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = YOLO(r\".\\data\\yolo\\best_scaled.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get predictions for this image\n",
    "results = model(\"images/two_signs_different_distance.jpg\")\n",
    "for result in results:\n",
    "    print(result.boxes)\n",
    "    draw_rectangles(result.path, result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "#### Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "<p> 10 epochs </p>\n",
    "<img src=\"images/yolo/results/results_10_epochs.png\" alt=\"YOLOv11 Results\" width=\"99%\"/>\n",
    "<p> 20 epochs </p>\n",
    "<img src=\"images/yolo/results/results_20_epochs.png\" alt=\"YOLOv11 Results\" width=\"99%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "The YOLO11x model with the greatest amount of parameters is chosen. The model is trained for 10 and 20 epochs on the data and the results are shown above. The fintuned model Faster R-CNN has been trained for 18 epochs and reached a these maximal metrics:\n",
    "- **bounding box regression loss(train/box_loss)** → Epoch 20, value: 0.98676\n",
    "- **classification loss(train/cls_loss)** → Epoch 20, value: 0.40708\n",
    "- **distribution focal loss(train/dfl_loss)** → Epoch 20, value: 1.02294\n",
    "- **precision(metrics/precision(B))** → Epoch 19, value: 0.99512\n",
    "- **recall(metrics/recall(B))** → Epoch 20, value: 0.99319\n",
    "- **mAP50(metrics/mAP50(B))** →  Epoch 15, value: 0.99477\n",
    "- **mAP50-95(metrics/mAP50-95(B))** → Epoch 10, value: 0.56793\n",
    "\n",
    "The model which was chosen as the best model was the model with the highest mAP@IoU=0.50:0.95 (overall mAP), so the checkpoint at epoch 10\n",
    "\n",
    "When evaluating the best model on the test set we get these metrics:\n",
    "- train/box_loss: 0.98615\n",
    "- train/cls_loss: 0.40763\n",
    "- train/dfl_loss: 0.96396\n",
    "- metrics/precision(B): 0.98832\n",
    "- metrics/recall(B): 0.98773\n",
    "- metrics/mAP50(B): 0.99365\n",
    "- metrics/mAP50-95(B): 0.57693"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "#### Training analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "\n",
    "The training metrics show that the model has performed well in terms of precision, recall, and mAP. The precision and recall values are close to 1, indicating that the model can accurately detect and classify the UN number hazard plates. The mAP50 and mAP50-95 values are also high, suggesting that the model performs well across different levels of IoU.\n",
    "\n",
    "During the testing phase, the model trained for 20 epochs achieved worse results than the model trained for 10 epochs. This could be due to overfitting or other factors affecting the model's generalization ability. The model trained for 10 epochs showed better performance on the test set, with high precision, recall, and mAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \".\\\\data\\\\yolo\\\\best_augmented_scaled.pt\"\n",
    "model = YOLO(model_path)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "<img src=\"images/yolo/results/confusion_matrix.png\" alt=\"Confusion matrix\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "From 1047 images of validation est only 19 images were considered as false positives and only 9 false negatives. The model has a high precision and recall, which is reflected in the confusion matrix. The model has a high true positive rate and a low false positive rate, indicating that it can accurately detect and classify UN number hazard plates.\n",
    "\n",
    "However, due to the almost the same conditions of the training dataset, the model may not generalize well to other scenarios or environments. Further testing on diverse datasets is recommended to assess the model's robustness and generalization capabilities. Also the continued data augmentation and training on more diverse data can improve the model's performance in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "##### False negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "\n",
    "Because the dataset consisted mostly from the real-case scenarios where the un-number placard is even far from the camera, the model has a hard time detecting the placard, placed close to the camera. Even though data augmentation is used, it can still be helpful to apply some scale augmentation to improve models accuracy on the close-up images.\n",
    "\n",
    "For example, the un number placard in the following image is not detected by the model, because it is too close to the camera. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "<img src=\"images/big_sign.jpg\" alt=\"False negative\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "#### Wheather conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "\n",
    "For this project, it was essential to develop a robust model capable of performing well under various weather conditions. However, our dataset primarily consisted of images captured under different lighting conditions, such as day and night, without significant weather variations.  \n",
    "\n",
    "To address this limitation, we evaluated the model using augmented images generated with the **Albumentations** library, applying weather-related transformations such as **rain, sunflare, shadow, and fog.** \n",
    "\n",
    "The model performed reasonably well on almost of all the conditions. However, the sinflare condition was the most challenging for the model, as it significantly impacted the visibility of the UN number hazard plates. Sot the model's performance was lower under this condition compared to others.\n",
    "\n",
    "For example in this image the model failed to detect the un-number placard, because of the sunflare.\n",
    "<img src=\"images/yolo/results/sunflare.png\" alt=\"Sunflare\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "Here are some examples of the model's performance under different weather conditions:\n",
    "- **Rain**: The model was able to detect the UN number hazard plates accurately under rainy conditions, showing robustness to weather-related challenges.\n",
    "\n",
    "<img src=\"images/yolo/rain_success.png\" alt=\"Rain\" width=\"60%\"/>\n",
    "\n",
    "\n",
    "- **Fog**: The model performed well under foggy conditions, indicating its ability to handle reduced visibility scenarios.\n",
    "\n",
    "\n",
    "<img src=\"images/yolo/fog_success.png\" alt=\"Rain\" width=\"60%\"/>\n",
    "\n",
    "\n",
    "- **Shadow**: The model performed well under shadow conditions, indicating its ability to handle variations in lighting and contrast.\n",
    "\n",
    "\n",
    "<img src=\"images/yolo/shadow_success.png\" alt=\"Rain\" width=\"60%\"/>\n",
    "\n",
    "\n",
    "- **Sunflare**: The model struggled to detect the UN number hazard plates under sunflare conditions, likely due to the glare and reduced visibility caused by direct sunlight.\n",
    "\n",
    "<img src=\"images/yolo/sunflare_success.png\" alt=\"Rain\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_augmented_images = \".\\\\data\\\\augmented_images\"\n",
    "def show_image_with_boxes(image_path, boxes):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for matplotlib\n",
    "\n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = map(int, box.xyxy[0])  # Convert to integers\n",
    "        confidence = box.conf[0]  # Confidence score\n",
    "\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, edgecolor='green', facecolor='none', linewidth=2))\n",
    "\n",
    "        # Put the label and confidence score\n",
    "        plt.text(x_min, y_min - 10, f\"Code: {confidence:.2f}\", color='green', fontsize=12)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def predict_augmented_images(augment):\n",
    "    results = model(path_to_augmented_images + f\"\\\\\" + augment, show_boxes=True, task=\"detect\", verbose=False)\n",
    "    random_results = random.sample(results, min(5, len(results)))\n",
    "    for result in random_results:\n",
    "        show_image_with_boxes(result.path, result.boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "##### Fog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_augmented_images(\"fog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "##### Rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_augmented_images(\"rain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "##### Shadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_augmented_images(\"shadow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "##### Sunflare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "#### **Specific Data Mining Goals:**\n",
    "\n",
    "##### **1. Object Detection and Localization:**\n",
    "Develop a model that achieves a high AP score for accurately detecting and localizing hazard plates on freight wagons within each video frame.\n",
    "\n",
    "**Approach and Outcome:**  \n",
    "To accomplish this goal, we finetuned a YOLO11x model, which demonstrated promising results in localizing hazard placards. However, the model is not yet fully robust and sometimes struggles to detect placards in challenging scenarios, such as when they are placed close to the camera or under direct sunlight (sunflare). The model's performance is generally strong, with high precision and recall rates, as indicated by the confusion matrix. The false positive and false negative rates are low, suggesting that the model can accurately detect and classify UN number hazard plates.\n",
    "\n",
    "**Improvement Strategies:**\n",
    "- **Data Augmentation:** Further augmenting the dataset with scale transformations could help the model better handle placards placed close to the camera.\n",
    "- **Model Optimization:** Fine-tuning the model architecture or hyperparameters could enhance its performance in challenging scenarios.\n",
    "- **Diverse Dataset:** Training the model on a more diverse dataset, including images with varying distances and angles, could improve its generalization capabilities.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##### **2. Robustness Across Variable Conditions:**\n",
    "Enhance the model’s robustness by training it on datasets representing diverse lighting and weather conditions, with a goal to maintain high AP scores across these environments.\n",
    "\n",
    "**Approach and Outcome:**  \n",
    "\n",
    "**Lighting Conditions:**\n",
    "The model demonstrated robustness to different lighting conditions, including day and night scenarios, because the dataset contained images captured under these conditions. The model's performance was consistent across various lighting settings, indicating its ability to generalize well to different times of day.\n",
    "\n",
    "To evaluate its performance under different weather conditions, we used data augmentation techniques from the **Albumentations** library. Augmentations like rain, sunflare, shadow, and fog were introduced to simulate adverse weather conditions. The model performed well under most conditions, except for sunflare, where it struggled due to glare and reduced visibility.\n",
    "\n",
    "**Further Improvements:**\n",
    "- **Sunflare Challenge:** Addressing the sunflare challenge could involve developing specialized augmentation techniques or training the model on additional sunflare images to improve its performance in such conditions.\n",
    "- **Weather-Specific Augmentations:** Creating weather-specific augmentation strategies tailored to each condition could enhance the model's robustness and prepare it for real-world deployment.\n",
    "\n",
    "---\n",
    "\n",
    "##### **3. Optimization for Real-Time Processing:**\n",
    "Implement real-time object detection and OCR capabilities to ensure the model operates at a frame rate suitable for analyzing images from moving trains.\n",
    "\n",
    "**Approach and Outcome:**  \n",
    "The model is optimized for real-time processing, with a frame rate suitable for analyzing images from moving trains. The YOLO11x architecture is known for its speed and efficiency, making it well-suited for real-time applications. \n",
    "\n",
    "The avarage processing time per frame is under **100 milliseconds**, which is sufficient for real-time processing. But due to use of idefics2 to read the un number from the placard, the overall processing time is higher than 100 milliseconds, escpecially because the processing time of idefics2 is bigger than **2 seconds**.\n",
    "\n",
    "**Further Improvements:**\n",
    "- **OCR Optimization:** Enhancing the OCR module's efficiency or exploring alternative OCR tools could reduce the overall processing time and improve real-time performance.\n",
    "- **Hardware Acceleration:** Leveraging hardware accelerators like GPUs or TPUs could further optimize the model's processing speed and enhance its real-time capabilities.\n",
    "\n",
    "### Data Mining Success Criteria evaluation\n",
    "\n",
    "- **Object Detection AP**: Achieve a Mean Average Precision (mAP) of at least 0.70 for detecting and localizing hazard plates across varied conditions.  \n",
    "  **Outcome:** Not achieved, with an mAP of 0.56793.\n",
    "\n",
    "- **OCR Precision for UN Numbers**: Ensure the Tesseract OCR module achieves high accuracy in reading UN numbers, even under challenging conditions, with a target precision score above 0.95.  \n",
    "  **Outcome:** Tesseract was unable to consistently recognize codes in difficult conditions, so we switched to using the **idefics2 VLM**, which performed significantly better, even with low-quality images. However, accuracy metrics for idefics2 have not been formally evaluated.\n",
    "\n",
    "- **Processing Speed**: Ensure the model achieves a processing time per frame under 100 milliseconds to maintain real-time functionality.  \n",
    "  **Outcome:** Not achieved. The model itself meets the processing speed requirement, but the OCR module's processing time exceeds the threshold, impacting the overall real-time performance.\n",
    "\n",
    "- **Environmental Robustness**: Maintain consistent mAP scores across different lighting and weather conditions.  \n",
    "  **Outcome:** Partly achieved. Lighting conditions were well represented in the dataset, and the model performed consistently across different lighting variations. Weather conditions, however, were not included in the dataset. The model's performance under simulated weather conditions was generally good, except for sunflare, where it failed to detect placards effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_augmented_images(\"sunflare\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "# Modeling OCR/idefics2-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Tesseract\n",
    "Tesseract is an open-source OCR engine that can be used to extract text from images. It supports multiple languages and can be integrated into various programming languages, including Python. Tesseract is known for its accuracy and flexibility, making it a popular choice for text recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n",
    "    plt.show()\n",
    "    \n",
    "def extract_un_number(text):\n",
    "    un = re.findall(r'\\d{2,}', text)\n",
    "    return un\n",
    "\n",
    "def extract_hin_number(text):\n",
    "    hin = re.findall(r'\\d{4,}', text)\n",
    "    return hin\n",
    "\n",
    "def get_text_from_image(image):\n",
    "    #split image horizontally in two pieces\n",
    "    h, w = image.shape\n",
    "    image_upper = image[0:int(h/2), 0:w]\n",
    "    image_lower = image[int(h/2):h, 0:w]\n",
    "    psm = 6\n",
    "    option = f\"--psm {psm}\"\n",
    "    text_un = pytesseract.image_to_string(image_upper, config=option)\n",
    "    text_hin = pytesseract.image_to_string(image_lower, config=option)\n",
    "    return extract_un_number(text_un), extract_hin_number(text_hin)\n",
    "\n",
    "def extract_bounding_box(image_path, xtl, ytl, xbr, ybr):\n",
    "    image = cv2.imread(image_path) \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.equalizeHist(image)\n",
    "    scale_percent = 200  # Scale by 200% (2x the size)\n",
    "    width = int(image.shape[1] * scale_percent / 100)\n",
    "    height = int(image.shape[0] * scale_percent / 100)\n",
    "    resized = cv2.resize(image, (width, height), interpolation=cv2.INTER_CUBIC)\n",
    "    crop_img = resized[\n",
    "        int(ytl*scale_percent/100):int(ybr*scale_percent/100), \n",
    "        int(xtl*scale_percent/100):int(xbr*scale_percent/100)]\n",
    "    return crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\".\\\\data\\\\yolo\\\\best_augmented_scaled.pt\")\n",
    "image_path = \".\\\\images\\\\hazard_plate.jpg\"\n",
    "results = model(image_path)\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        x_min, y_min, x_max, y_max = map(int, box.xyxy[0])  # Convert to integers\n",
    "        confidence = box.conf[0]  # Confidence score\n",
    "        crop_img = extract_bounding_box(image_path, x_min, y_min, x_max, y_max)\n",
    "        un_number, hin_number = get_text_from_image(crop_img)\n",
    "        show_image(crop_img)\n",
    "        print(f\"UN number: {un_number}, HIN number: {hin_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "### EasyOCR\n",
    "EasyOCR is a Python library that provides a simple interface for performing OCR tasks on images. It supports multiple languages and can detect text in various fonts and sizes. EasyOCR is designed to be user-friendly and efficient, making it a suitable choice for extracting text from images in real-time applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_image(image):\n",
    "    # Initialize the reader for digits\n",
    "    reader = easyocr.Reader([\"en\"])\n",
    "    result = reader.readtext(image, allowlist=\"0123456789\",detail=0)\n",
    "    h,w = None, None\n",
    "    try:\n",
    "        h, w = image.shape\n",
    "    except:\n",
    "        h,w,_ = image.shape\n",
    "    image_un = image[0:int(h/2), 0:w]\n",
    "    image_hin = image[int(h/2):h, 0:w]\n",
    "    result_un = reader.readtext(image_un, allowlist=\"0123456789\",detail=0)\n",
    "    result_hin = reader.readtext(image_hin, allowlist=\"0123456789\",detail=0)\n",
    "    return result_un,result_hin,result\n",
    "\n",
    "\n",
    "def extract_bounding_box(image_path, xtl, ytl, xbr, ybr):\n",
    "    image = image_path\n",
    "    if(isinstance(image_path, str)):\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = cv2.equalizeHist(image)\n",
    "    scale_percent = 200  # Scale by 200% (2x the size)\n",
    "    width = int(image.shape[1] * scale_percent / 100)\n",
    "    height = int(image.shape[0] * scale_percent / 100)\n",
    "    resized = cv2.resize(image, (width, height), interpolation=cv2.INTER_CUBIC)\n",
    "    crop_img = resized[\n",
    "        int(ytl*scale_percent/100):int(ybr*scale_percent/100), \n",
    "        int(xtl*scale_percent/100):int(xbr*scale_percent/100)]\n",
    "    return crop_img\n",
    "\n",
    "\n",
    "def show_image(image):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\".\\\\data\\\\yolo\\\\best.pt\")\n",
    "image_path = \".\\\\images\\\\hazard_plate.jpg\"\n",
    "results = model(image_path)\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        x_min, y_min, x_max, y_max = map(int, box.xyxy[0])  # Convert to integers\n",
    "        confidence = box.conf[0]  # Confidence score\n",
    "        crop_img = extract_bounding_box(image_path, x_min, y_min, x_max, y_max)\n",
    "        un_number = get_text_from_image(crop_img)\n",
    "        show_image(crop_img)\n",
    "        print(f\"UN number: {un_number}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "### idefics2 \n",
    "idefics2 is a Python library that offers OCR capabilities for extracting text from images. It provides an easy-to-use interface for processing images and recognizing text using optical character recognition techniques. idefics2 is designed to be fast and accurate, making it suitable for real-time text extraction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what version of PyTorch is installed\n",
    "print(torch.__version__)\n",
    "\n",
    "# Check the current CUDA version being used\n",
    "print(\"CUDA Version: \", torch.version.cuda)\n",
    "\n",
    "# Check if CUDA is available and if so, print the device name\n",
    "print(\"Device name:\", torch.cuda.get_device_properties(\"cuda\").name)\n",
    "\n",
    "# Check if FlashAttention is available\n",
    "print(\"FlashAttention available:\", torch.backends.cuda.flash_sdp_enabled())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processor and model\n",
    "processor = Idefics2Processor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "print('Processor loaded')\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "    \"HuggingFaceM4/idefics2-8b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device,\n",
    "    quantization_config=quantization_config,   \n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model = model.to(device)\n",
    "print('Model loaded')\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the image and extract two key values:\n",
    "\n",
    "    The UN number visible on the upper part of the placard.\n",
    "    The code visible on the lower part of the placard, located below the horizontal line separating the two sections.\n",
    "\n",
    "Both codes are printed in black. If either the upper or lower part cannot be detected, replace the missing value with \"0.\" Output the extracted values as plain text, separated by a comma if multiple codes are present. No additional context or formatting is needed.\n",
    "\n",
    "Input Examples:\n",
    "\n",
    "    {98 {line} 4567}\n",
    "    (not found, {line}, 8901)\n",
    "    {101 {line} 3345}\n",
    "    (not found, {line}, {not found})\n",
    "    {45 {line} 2789}\n",
    "    {22 {line} 5678}\n",
    "\n",
    "Desired Output:\n",
    "\n",
    "    98, 4567\n",
    "    0, 8901\n",
    "    101, 3345\n",
    "    0, 0\n",
    "    45, 2789\n",
    "    22, 5678\n",
    "\n",
    "Expected Transformation:\n",
    "\n",
    "    For each input example, extract the UN number and the code below the horizontal line.\n",
    "    If either part is missing (i.e., \"not found\"), replace it with 0.\n",
    "    Output the extracted values as plain text, separated by a comma, without any additional context or formatting.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "text = processor.apply_chat_template(messages, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelYolo = YOLO(\".\\\\data\\\\yolo\\\\best_augmented_scaled.pt\", task=\"detect\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    # Read the image using PIL\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def extract_bounding_box(image_path, xtl, ytl, xbr, ybr):\n",
    "    # check type of image path to check whetther its a  string\n",
    "    if isinstance(image_path, str):\n",
    "        image_path = Image.open(image_path).convert(\"RGB\")\n",
    "    crop_img = image_path.crop((xtl, ytl, xbr, ybr))\n",
    "    return crop_img\n",
    "\n",
    "def perform_ocr(image): \n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "    generated_text = model.generate(**inputs, max_new_tokens=500)\n",
    "    generated_text = processor.batch_decode(generated_text, skip_special_tokens=True)[0]\n",
    "    assistant_output = generated_text.split(\"Assistant:\")[1].strip()\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    # Split the output by comma to get the individual numbers\n",
    "    numbers = assistant_output.split(\",\")\n",
    "\n",
    "    # Strip any leading or trailing whitespace from the numbers\n",
    "    numbers = [number.strip().replace('.','') for number in numbers]\n",
    "    un_number, hin_number = numbers\n",
    "    return un_number, hin_number\n",
    "\n",
    "def show_image(image):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_path = \".\\\\images\\\\good_conditions.jpg\"\n",
    "results = modelYolo(image_path,stream=True)\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        x_min, y_min, x_max, y_max = map(int, box.xyxy[0])  # Convert to integers\n",
    "        crop_img = extract_bounding_box(image_path, x_min, y_min, x_max, y_max)\n",
    "        show_image(crop_img)\n",
    "        un_number = perform_ocr(crop_img)\n",
    "        print(f\"UN number: {un_number[0]}, HIN number: {un_number[1]}\")\n",
    "        desc = get_hin_description(un_number[1])\n",
    "        print(f\"Description: {desc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "# Evaluation OCR/idefics2-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "We evaluated three different OCR techniques—Tesseract, EasyOCR, and idefics2—on low-quality images to simulate real-world conditions. The performance of these models was as follows:\n",
    "\n",
    "- **Tesseract** preprocessed the test image in **2.0 ms** but failed to accurately extract the desired values.\n",
    "- **EasyOCR** preprocessed the image in **8.3 ms**, but it also failed to extract the correct code.\n",
    "\n",
    "The only model that successfully extracted the UN number and hazard placard code was **idenfics2-8b**. We employed a specific prompt for this model:\n",
    "\n",
    "#### Prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Analyze the image and extract two key values:\n",
    "\n",
    "    The UN number visible on the upper part of the placard.\n",
    "    The code visible on the lower part of the placard, located below the horizontal line separating the two sections.\n",
    "\n",
    "Both codes are printed in black. If either the upper or lower part cannot be detected, replace the missing value with \"0.\" Output the extracted values as plain text, separated by a comma if multiple codes are present. No additional context or formatting is needed.\n",
    "\n",
    "Input Examples:\n",
    "\n",
    "    {98 {line} 4567}\n",
    "    (not found, {line}, 8901)\n",
    "    {101 {line} 3345}\n",
    "    (not found, {line}, {not found})\n",
    "    {45 {line} 2789}\n",
    "    {22 {line} 5678}\n",
    "\n",
    "Desired Output:\n",
    "\n",
    "    98, 4567\n",
    "    0, 8901\n",
    "    101, 3345\n",
    "    0, 0\n",
    "    45, 2789\n",
    "    22, 5678\n",
    "\n",
    "Expected Transformation:\n",
    "\n",
    "    For each input example, extract the UN number and the code below the horizontal line.\n",
    "    If either part is missing (i.e., \"not found\"), replace it with 0.\n",
    "    Output the extracted values as plain text, separated by a comma, without any additional context or formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "\n",
    "This model produced the correct results and extracted the UN number and hazard code accurately. However, the processing time was much longer compared to the other models, taking up to **17.49 seconds per image**. To optimize this, we used quantization techniques, but the processing time still did not meet real-time requirements.\n",
    "\n",
    "We recommend exploring further literature to identify additional VLMs or alternative OCR techniques that may improve both accuracy and processing time. Techniques such as **FlashAttention** could be explored to enhance speed.\n",
    "\n",
    "Given its superior accuracy in handling low-quality images—which are common when processing frames from videos—**idenfics2-8b** remains our recommended OCR model for the time being, despite its slower processing speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "# Modeling Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "# Define the dataset class\n",
    "class HazmatDataset(Dataset):\n",
    "    def __init__(self, data_dir, annotations_file, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Load annotations\n",
    "        with open(annotations_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.images = {img['id']: img for img in data['images']}\n",
    "        self.annotations = data['annotations']\n",
    "        \n",
    "        # Create image_id to annotations mapping\n",
    "        self.img_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_to_anns:\n",
    "                self.img_to_anns[img_id] = []\n",
    "            self.img_to_anns[img_id].append(ann)\n",
    "        \n",
    "        self.ids = list(self.images.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.data_dir, 'images', img_info['file_name'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Get annotations\n",
    "        anns = self.img_to_anns.get(img_id, [])\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']\n",
    "            # Convert [x, y, w, h] to [x1, y1, x2, y2]\n",
    "            boxes.append([\n",
    "                bbox[0],\n",
    "                bbox[1],\n",
    "                bbox[0] + bbox[2],\n",
    "                bbox[1] + bbox[3]\n",
    "            ])\n",
    "            labels.append(ann['category_id'])\n",
    "            areas.append(ann['area'])\n",
    "            iscrowd.append(ann['iscrowd'])\n",
    "        \n",
    "        # Convert to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': areas,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            for transform in self.transforms:\n",
    "                img, target = transform(img, target)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        # Convert PIL image to tensor\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = F.hflip(image)\n",
    "            # Flip bounding boxes\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]  # Flip x-coordinates\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # Convert PIL image to tensor\n",
    "    transforms.append(ToTensor())\n",
    "    if train:\n",
    "        # Add training augmentations here if needed\n",
    "        transforms.append(RandomHorizontalFlip(0.5))\n",
    "    return transforms\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_classifier_loss = 0\n",
    "    total_box_reg_loss = 0\n",
    "    total_objectness_loss = 0\n",
    "    total_rpn_box_reg_loss = 0\n",
    "\n",
    "    # Voeg tqdm toe om de voortgang te tonen\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=True)\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Wrap the forward pass in autocast\n",
    "        with autocast():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Scale the loss and call backward\n",
    "        scaler.scale(losses).backward()\n",
    "        # Unscales the gradients and calls or skips optimizer.step()\n",
    "        scaler.step(optimizer)\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        # Bereken de totalen\n",
    "        total_loss += losses.item()\n",
    "        total_classifier_loss += loss_dict['loss_classifier'].item()\n",
    "        total_box_reg_loss += loss_dict['loss_box_reg'].item()\n",
    "        total_objectness_loss += loss_dict['loss_objectness'].item()\n",
    "        total_rpn_box_reg_loss += loss_dict['loss_rpn_box_reg'].item()\n",
    "\n",
    "        # Update tqdm-balk\n",
    "        progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{losses.item():.4f}\",\n",
    "            \"Classifier\": f\"{loss_dict['loss_classifier'].item():.4f}\",\n",
    "            \"BoxReg\": f\"{loss_dict['loss_box_reg'].item():.4f}\",\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_classifier_loss = total_classifier_loss / len(data_loader)\n",
    "    avg_box_reg_loss = total_box_reg_loss / len(data_loader)\n",
    "    avg_objectness_loss = total_objectness_loss / len(data_loader)\n",
    "    avg_rpn_box_reg_loss = total_rpn_box_reg_loss / len(data_loader)\n",
    "\n",
    "    return avg_loss, avg_classifier_loss, avg_box_reg_loss, avg_objectness_loss, avg_rpn_box_reg_loss\n",
    "\n",
    "\n",
    "\n",
    "# Load ground truth annotations\n",
    "coco_val = COCO('data/data_faster_rcnn/val/annotations/instances_val.json')\n",
    "\n",
    "# Prepare predictions in COCO format\n",
    "# Assuming you have a function to convert model outputs to COCO format\n",
    "# Conversion to COCO Format\n",
    "def convert_to_coco_format(outputs, image_ids):\n",
    "    coco_results = []\n",
    "    for output, image_id in zip(outputs, image_ids):\n",
    "        boxes = output['boxes'].cpu().numpy()\n",
    "        scores = output['scores'].cpu().numpy()\n",
    "        labels = output['labels'].cpu().numpy()\n",
    "        \n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            coco_results.append({\n",
    "                'image_id': image_id,\n",
    "                'category_id': int(label),\n",
    "                'bbox': [box[0], box[1], box[2] - box[0], box[3] - box[1]],\n",
    "                'score': float(score)\n",
    "            })\n",
    "    return coco_results\n",
    "\n",
    "# Validation Function\n",
    "def validate(model, data_loader, coco_gt, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    # Add tqdm\n",
    "    progress_bar = tqdm(data_loader, desc=\"Validation\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in progress_bar:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            image_ids = [target['image_id'].item() for target in targets]\n",
    "            coco_results = convert_to_coco_format(outputs, image_ids)\n",
    "            results.extend(coco_results)\n",
    "\n",
    "            # Update tqdm-bar\n",
    "            progress_bar.set_postfix({\"Processed\": len(results)})\n",
    "\n",
    "    if not results:\n",
    "        print(\"No predictions generated. Skipping evaluation.\")\n",
    "        return [0.0] * 6  # Return dummy metrics for empty results\n",
    "\n",
    "    # Suppress COCOeval output\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        coco_dt = coco_gt.loadRes(results)\n",
    "        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "    return coco_eval.stats\n",
    "\n",
    "\n",
    "# Custom backbone to return a dictionary of feature maps\n",
    "class BackboneWithChannels(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return {'0': x}\n",
    "    \n",
    "# Function to create a subset of the dataset\n",
    "def create_subset(dataset, percentage):\n",
    "    \"\"\"\n",
    "    Create a subset of the dataset based on the given percentage.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: The full dataset.\n",
    "    - percentage: The fraction of the dataset to use (value between 0.0 and 1.0).\n",
    "    \n",
    "    Returns:\n",
    "    - subset: A subset of the dataset containing the specified percentage of data.\n",
    "    \"\"\"\n",
    "    if not (0.0 < percentage <= 1.0):\n",
    "        raise ValueError(\"Percentage must be between 0.0 and 1.0.\")\n",
    "    \n",
    "    # Determine the subset size\n",
    "    total_samples = len(dataset)\n",
    "    subset_size = int(total_samples * percentage)\n",
    "    \n",
    "    # Shuffle and select a random subset of indices\n",
    "    indices = list(range(total_samples))\n",
    "    random.shuffle(indices)\n",
    "    subset_indices = indices[:subset_size]\n",
    "    \n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "def create_directory(base_path=\"data/models\"):\n",
    "    \"\"\"\n",
    "    Create a directory inside the base path named 'faster-rcnn-finetuned-{date}' \n",
    "    to store models and logs. The name includes the current date and time in the format 'DD-MM-YYYY HH:MM:SS'.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Base directory where the new directory will be created.\n",
    "\n",
    "    Returns:\n",
    "    - directory_path (str): Full path to the created directory.\n",
    "    \"\"\"\n",
    "    # Get the current date and time\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "    \n",
    "    # Define the full directory path\n",
    "    directory_name = f\"faster-rcnn-finetuned-{current_time}\"\n",
    "    directory_path = os.path.join(base_path, directory_name)\n",
    "    \n",
    "    # Create the directory\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Directory created: {directory_path}\")\n",
    "    return directory_path\n",
    "\n",
    "def train_model(directory, model, optimizer, train_loader, device, train_metrics_list, best_val_map, lr_scheduler, val_loader, coco_val, scaler, epoch):\n",
    "    \n",
    "    epoch+=1\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_classifier_loss, train_box_reg_loss, train_objectness_loss, train_rpn_box_reg_loss = train_one_epoch(\n",
    "        model, optimizer, train_loader, device, scaler)\n",
    "    \n",
    "    # Validate and get all COCO-metrics\n",
    "    val_metrics = validate(model, val_loader, coco_val, device)\n",
    "    val_map = val_metrics[0]  # mAP@IoU=0.50:0.95\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    \n",
    "    # Obtain the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Prepare data for logging\n",
    "    data = {\n",
    "        \"epoch\": epoch,\n",
    "        \"time_elapsed\": (int(minutes), int(seconds)),\n",
    "        \"learning_rate\": current_lr,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"classifier_loss\": train_classifier_loss,\n",
    "        \"box_reg_loss\": train_box_reg_loss,\n",
    "        \"objectness_loss\": train_objectness_loss,\n",
    "        \"rpn_box_reg_loss\": train_rpn_box_reg_loss,\n",
    "        \"val_metrics\": val_metrics\n",
    "    }\n",
    "    \n",
    "    # Append current epoch data to metrics list\n",
    "    train_metrics_list.append(data)\n",
    "    \n",
    "    # Print summary for this epoch\n",
    "    print(f\"📊 Epoch {epoch} | ⏳ Time: {int(minutes)}m {int(seconds)}s | 🔄 LR: {current_lr:.6f}\")\n",
    "    print(f\"📉 Train Loss: {train_loss:.4f} | 🎯 Classifier: {train_classifier_loss:.4f} | 📦 Box Reg: {train_box_reg_loss:.4f}\")\n",
    "    print(f\"🔍 Objectness: {train_objectness_loss:.4f} | 🗂️ RPN Box Reg: {train_rpn_box_reg_loss:.4f}\")\n",
    "    print(f\"🧪 mAP | 🟢 mAP@IoU=0.50:0.95: {val_metrics[0]:.4f} | 🔵 mAP@IoU=0.50: {val_metrics[1]:.4f} | 🟣 mAP@IoU=0.75: {val_metrics[2]:.4f}\")\n",
    "    print(f\"📏 Small mAP: {val_metrics[3]:.4f} | 📐 Medium mAP: {val_metrics[4]:.4f} | 📏 Large mAP: {val_metrics[5]:.4f}\")\n",
    "    \n",
    "    # Save epoch data to a log file\n",
    "    save_epoch_data(directory, data)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save the latest checkpoint with all metrics\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_map': val_map,\n",
    "        'train_metrics_list': train_metrics_list  # Save all metrics\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(directory, \"latest_model.pth\"))\n",
    "    \n",
    "    # Save the best model if the val_map is the highest so far\n",
    "    if val_map > best_val_map:\n",
    "        best_val_map = val_map\n",
    "        torch.save(checkpoint, os.path.join(directory, \"best_model.pth\"))\n",
    "    \n",
    "    return best_val_map\n",
    "        \n",
    "\n",
    "\n",
    "def save_epoch_data(directory, data):\n",
    "    \"\"\"\n",
    "    Save training statistics for each epoch in a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): Path to the directory.\n",
    "    - data (dict): Contains data on metrics such as epoch, losses, and validation metrics.\n",
    "    \"\"\"\n",
    "    log_file_path = os.path.join(directory, \"training_log.txt\")\n",
    "    \n",
    "    with open(log_file_path, \"a\") as log_file:\n",
    "        log_file.write(f\"📊 Epoch {data['epoch']} | ⏳ Time: {data['time_elapsed'][0]}m {data['time_elapsed'][1]}s | 🔄 LR: {data['learning_rate']:.6f}\\n\")\n",
    "        log_file.write(f\"📉 Train Loss: {data['train_loss']:.4f} | 🎯 Classifier: {data['classifier_loss']:.4f} | 📦 Box Reg: {data['box_reg_loss']:.4f}\\n\")\n",
    "        log_file.write(f\"🔍 Objectness: {data['objectness_loss']:.4f} | 🗂️ RPN Box Reg: {data['rpn_box_reg_loss']:.4f}\\n\")\n",
    "        log_file.write(f\"🧪 Validation Metrics | 🟢 mAP@IoU=0.50:0.95: {data['val_metrics'][0]:.4f} | 🔵 mAP@IoU=0.50: {data['val_metrics'][1]:.4f} | 🟣 mAP@IoU=0.75: {data['val_metrics'][2]:.4f}\\n\")\n",
    "        log_file.write(f\"📏 Small mAP: {data['val_metrics'][3]:.4f} | 📐 Medium mAP: {data['val_metrics'][4]:.4f} | 📏 Large mAP: {data['val_metrics'][5]:.4f}\\n\")\n",
    "        log_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Training model on {device}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/train',\n",
    "    annotations_file='data/data_faster_rcnn/train/annotations/instances_train.json',\n",
    "    transforms=get_transform(train=True)\n",
    ")\n",
    "\n",
    "val_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/val',\n",
    "    annotations_file='data/data_faster_rcnn/val/annotations/instances_val.json',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "# Set the percentage of the training dataset to use (e.g. 0.x to 1)\n",
    "train_percentage = 1\n",
    "\n",
    "# Create a subset of the training dataset\n",
    "train_dataset_subset = create_subset(train_dataset, train_percentage)\n",
    "\n",
    "# Set the percentage of the val dataset to use (e.g. 0.x to 1)\n",
    "val_percentage = 1\n",
    "\n",
    "# Create a subset of the training dataset\n",
    "val_dataset_subset = create_subset(val_dataset, val_percentage)\n",
    "\n",
    "# amount of cpu cores\n",
    "workers = 2\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_subset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_subset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 2  # hazmat code and background\n",
    "\n",
    "# Create ResNet-101 backbone with FPN\n",
    "backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
    "\n",
    "# Define anchor generator for FPN\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32,), (64,), (128,), (256,), (512,)),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    ")\n",
    "\n",
    "# Multi-scale RoI pooling for FPN\n",
    "roi_pooler = MultiScaleRoIAlign(\n",
    "    featmap_names=['0', '1', '2', '3', '4'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2\n",
    ")\n",
    "\n",
    "print(\"initializing model...\")\n",
    "# Initialize Faster R-CNN with ResNet-101-FPN\n",
    "model = FasterRCNN(\n",
    "    backbone=backbone,\n",
    "    num_classes=num_classes,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 7710"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 23\n",
    "train_metrics_map = []\n",
    "best_val_map = float('-inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Create directory to store models and logs\n",
    "directory_finetuned_model = create_directory()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    best_val_map = train_model(\n",
    "        directory=directory_finetuned_model, \n",
    "        model=model, optimizer=optimizer, train_loader=train_loader, device=device, \n",
    "        train_metrics_list=train_metrics_map, best_val_map=best_val_map, lr_scheduler=lr_scheduler, \n",
    "        val_loader=val_loader, coco_val=coco_val, scaler=scaler, epoch=epoch\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "# Evaluation Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "directory_finetuned_model = \"data/models\"\n",
    "device = torch.device('gpu:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = os.path.join(directory_finetuned_model, 'best_model.pth')\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "val_map = checkpoint['val_map']\n",
    "epoch = checkpoint['epoch']\n",
    "#latest\n",
    "latest_model_path = os.path.join(directory_finetuned_model, 'latest_checkpoint.pth')\n",
    "checkpoint_latest = torch.load(latest_model_path, map_location=device)\n",
    "val_map_latest = checkpoint_latest['val_map']\n",
    "epoch_latest = checkpoint_latest['epoch']\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(f\"Validation mAP best model: {val_map:.4f}\")\n",
    "print(f\"Epoch best model: {epoch}\")\n",
    "\n",
    "print(f\"Validation mAP latest model: {val_map_latest:.4f}\")\n",
    "print(f\"Epoch latest model: {epoch_latest}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(checkpoint_path, title=\"Training and Validation Metrics over Epochs\"):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics from a given model checkpoint.\n",
    "    \n",
    "    Parameters:\n",
    "    - checkpoint_path (str): Path to the model checkpoint file (e.g., 'latest_model.pth').\n",
    "    - title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    train_metrics_list = checkpoint['train_metrics_list']\n",
    "    \n",
    "    # Extract metrics per epoch\n",
    "    epochs = [data['epoch'] for data in train_metrics_list]\n",
    "    train_loss_list = [data['train_loss'] for data in train_metrics_list]\n",
    "    classifier_loss_list = [data['classifier_loss'] for data in train_metrics_list]\n",
    "    box_reg_loss_list = [data['box_reg_loss'] for data in train_metrics_list]\n",
    "    objectness_loss_list = [data['objectness_loss'] for data in train_metrics_list]\n",
    "    rpn_box_reg_loss_list = [data['rpn_box_reg_loss'] for data in train_metrics_list]\n",
    "\n",
    "    # Extract validation mAP metrics\n",
    "    val_map_list = [data['val_metrics'][0] for data in train_metrics_list]  # mAP@IoU=0.50:0.95\n",
    "    val_map_50_list = [data['val_metrics'][1] for data in train_metrics_list]  # mAP@IoU=0.50\n",
    "    val_map_75_list = [data['val_metrics'][2] for data in train_metrics_list]  # mAP@IoU=0.75\n",
    "    val_map_small_list = [data['val_metrics'][3] for data in train_metrics_list]  # Small mAP\n",
    "    val_map_medium_list = [data['val_metrics'][4] for data in train_metrics_list]  # Medium mAP\n",
    "    val_map_large_list = [data['val_metrics'][5] for data in train_metrics_list]  # Large mAP\n",
    "\n",
    "    # Initialize the plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    # Plot training losses\n",
    "    plt.plot(epochs, train_loss_list, label='Training Loss', marker='o')\n",
    "    #     plt.plot(epochs, classifier_loss_list, label='Classifier Loss', marker='o')\n",
    "    #     plt.plot(epochs, box_reg_loss_list, label='Box Regression Loss', marker='o')\n",
    "    #     plt.plot(epochs, objectness_loss_list, label='Objectness Loss', marker='o')\n",
    "    #     plt.plot(epochs, rpn_box_reg_loss_list, label='RPN Box Regression Loss', marker='o')\n",
    "\n",
    "    # Plot validation mAP metrics\n",
    "    plt.plot(epochs, val_map_list, label='Validation mAP (IoU=0.50:0.95)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_50_list, label='Validation mAP (IoU=0.50)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_75_list, label='Validation mAP (IoU=0.75)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_small_list, label='Validation mAP (Small)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_medium_list, label='Validation mAP (Medium)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_large_list, label='Validation mAP (Large)', linestyle='--', marker='x')\n",
    "\n",
    "    # Set x-axis ticks to start from 1\n",
    "    plt.xticks(range(1, len(epochs) + 1))\n",
    "\n",
    "    # Set plot details\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest model checkpoint\n",
    "latest_model_path = os.path.join(directory_finetuned_model, 'latest_model.pth')\n",
    "plot_metrics(latest_model_path, \"Training and validation over epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transforms=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transforms:\n",
    "        for transform in transforms:\n",
    "            image, _ = transform(image, target=None)  # No target during inference\n",
    "    return image\n",
    "\n",
    "# Define preprocessing transforms\n",
    "test_transforms = get_transform(train=False)\n",
    "\n",
    "# Load the image\n",
    "image_path = 'images/hazard_plate.jpg'  # Replace with your image path\n",
    "image = load_image(image_path, transforms=test_transforms)\n",
    "image = image.to(device)\n",
    "# Wrap the image in a list as the model expects a batch\n",
    "with torch.no_grad():\n",
    "    predictions = model([image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_with_opacity(score):\n",
    "    \"\"\"\n",
    "    Get a color with opacity based on the confidence score.\n",
    "    Higher confidence = more red and higher opacity.\n",
    "    Lower confidence = random color and lower opacity.\n",
    "    \"\"\"\n",
    "    if score > 0.75:\n",
    "        # High confidence: Red with high opacity\n",
    "        color = (1, 0, 0, min(1.0, 0.3 + score))  # Red with opacity based on score\n",
    "    else:\n",
    "        # Low confidence: Random color with lower opacity\n",
    "        color = (random.random(), random.random(), random.random(), max(0.3, score))\n",
    "    return color\n",
    "\n",
    "def draw_predictions(image, predictions, threshold=0.5, classes=['background', 'hazmat']):\n",
    "    # Convert image from tensor to numpy array\n",
    "    image = image.cpu().permute(1, 2, 0).numpy()\n",
    "    image = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Filter predictions based on confidence threshold\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if label == 1:  # Only plot hazmat codes\n",
    "            x1, y1, x2, y2 = box\n",
    "            color = get_color_with_opacity(score)\n",
    "            \n",
    "            # Draw rectangle with opacity\n",
    "            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, \n",
    "                                 edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add text label with confidence score\n",
    "            label_name = classes[label]\n",
    "            ax.text(x1, y1, f'{label_name}: {score:.2f}', \n",
    "                    color='white', \n",
    "                    bbox=dict(facecolor=color[:3], alpha=0.6), \n",
    "                    fontsize=12)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, threshold=0.5):\n",
    "    # List of class names\n",
    "    classes = ['background', 'hazmat']\n",
    "    \n",
    "    # Load the image\n",
    "    image = load_image(image_path, transforms=test_transforms)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Wrap the image in a list as the model expects a batch\n",
    "    with torch.no_grad():\n",
    "        predictions = model([image])\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    prediction_time = end_time - start_time\n",
    "    print(f\"Prediction time: {prediction_time:.4f} seconds\")\n",
    "    \n",
    "    # Filter predictions based on threshold\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Apply threshold filter\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    # Print the predictions\n",
    "    if len(boxes) == 0:\n",
    "        print(\"No predictions meet the threshold.\")\n",
    "    else:\n",
    "        print(\"Predictions:\")\n",
    "        for label, score in zip(labels, scores):\n",
    "            class_name = classes[label]\n",
    "            print(f\"  {class_name}: {score:.2f}\")\n",
    "        # Display the predictions\n",
    "        draw_predictions(image, predictions, threshold=threshold, classes=classes)\n",
    "\n",
    "\n",
    "def get_color_with_opacity(score):\n",
    "    \"\"\"\n",
    "    Get a color with opacity based on the confidence score.\n",
    "    Higher confidence = more red and higher opacity.\n",
    "    Lower confidence = random color and lower opacity.\n",
    "    \"\"\"\n",
    "    if score > 0.75:\n",
    "        # High confidence: Red with high opacity\n",
    "        color = (1, 0, 0, min(1.0, 0.3 + score))  # Red with opacity based on score\n",
    "    else:\n",
    "        # Low confidence: Random color with lower opacity\n",
    "        color = (random.random(), random.random(), random.random(), max(0.3, score))\n",
    "    return color\n",
    "\n",
    "def draw_predictions(image, predictions, threshold=0.5, classes=['background', 'hazmat']):\n",
    "    # Convert image from tensor to numpy array\n",
    "    image = image.cpu().permute(1, 2, 0).numpy()\n",
    "    image = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Filter predictions based on confidence threshold\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if label == 1:  # Only plot hazmat codes\n",
    "            x1, y1, x2, y2 = box\n",
    "            color = get_color_with_opacity(score)\n",
    "            \n",
    "            # Draw rectangle with opacity\n",
    "            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, \n",
    "                                 edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add text label with confidence score\n",
    "            label_name = classes[label]\n",
    "            ax.text(x1, y1, f'{label_name}: {score:.2f}', \n",
    "                    color='white', \n",
    "                    bbox=dict(facecolor=color[:3], alpha=0.6), \n",
    "                    fontsize=12)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_image('data/data_faster_rcnn/val/images/1690281365_00595.jpg', threshold=0.29)\n",
    "predict_image('images/hazard_plate.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/close_up_number.webp', threshold=0)\n",
    "predict_image('images/un_numbers_test/2.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/3.jpg', threshold=0)\n",
    "predict_image('images/two_signs_different_distance.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/6.webp', threshold=0)\n",
    "predict_image('images/no_signs.jpg', threshold=0)\n",
    "predict_image('images/africalane_closed_off.jpg', threshold=0)\n",
    "predict_image('images/bikes_get_off.jpg', threshold=0)\n",
    "predict_image('images/gevaarlijke_stoffen_route.jpg', threshold=0)\n",
    "predict_image('images/great_britain_nb.jpeg', threshold=0)\n",
    "predict_image('images/priority-road-sign.webp', threshold=0)\n",
    "predict_image('images/reflective_un_number_on_truck.jpg', threshold=0)\n",
    "predict_image('images/traffic signs.jpg', threshold=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "## Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "test_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/test',\n",
    "    annotations_file='data/data_faster_rcnn/test/annotations/instances_test.json',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "# Create test data loader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Load the best model checkpoint\n",
    "\n",
    "model_path = os.path.join(directory_finetuned_model, 'best_model.pth')\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "# Load ground truth annotations for test set\n",
    "coco_test = COCO('data/data_faster_rcnn/test/annotations/instances_test.json')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = validate(model, test_loader, coco_test, device)\n",
    "\n",
    "# Print test metrics\n",
    "print(f\"Test Metrics - mAP: {test_metrics[0]:.4f}\")\n",
    "print(f\"mAP@0.5: {test_metrics[1]:.4f}, mAP@0.75: {test_metrics[2]:.4f}\")\n",
    "print(f\"mAP medium: {test_metrics[4]:.4f}, mAP large: {test_metrics[5]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /tmp/ipykernel_2090903/2616491437.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
    "#   checkpoint = torch.load(best_checkpoint_path)\n",
    "# loading annotations into memory...\n",
    "# Done (t=0.00s)\n",
    "# creating index...\n",
    "# index created!\n",
    "# Validation: 100%|██████████| 62/62 [02:38<00:00,  2.56s/it, Processed=1095]\n",
    "# Test Metrics - mAP: 0.5634\n",
    "# mAP@0.5: 0.9892, mAP@0.75: 0.4738\n",
    "# mAP small: -1.0000, mAP medium: 0.4648, mAP large: 0.5724"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "## Data prep for augmented weather evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop trhough all images from test and predict\n",
    "test_images_path = \"data/data_faster_rcnn/test/images\"\n",
    "\n",
    "# frames available\n",
    "test_images_path_list = os.listdir(test_images_path)\n",
    "random.shuffle(test_images_path_list)\n",
    "\n",
    "# Predict on the first 30 images\n",
    "for count, image_name in enumerate(test_images_path_list[:20]):\n",
    "    image_path = os.path.join(test_images_path, image_name)\n",
    "    predict_image(image_path, threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize(image):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "\n",
    "test_images_path = \"data/data_faster_rcnn/test/images\"\n",
    "\n",
    "# frames available\n",
    "test_images_path_list = os.listdir(test_images_path)\n",
    "random.shuffle(test_images_path_list)\n",
    "path = os.path.join(test_images_path, test_images_path_list[0])\n",
    "image = cv2.imread(path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "transform = A.Compose(\n",
    "    [A.RandomRain(brightness_coefficient=0.9, drop_width=1, blur_value=5, p=1)],\n",
    ")\n",
    "random.seed(7)\n",
    "transformed = transform(image=image)\n",
    "visualize(transformed['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [A.RandomSunFlare(flare_roi=(0, 0, 1,0.5), angle_lower=1, p=1)],\n",
    ")\n",
    "random.seed(7)\n",
    "transformed = transform(image=image)\n",
    "visualize(transformed['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [A.RandomShadow(num_shadows_lower=1, num_shadows_upper=1, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=1)],\n",
    ")\n",
    "random.seed(7)\n",
    "transformed = transform(image=image)\n",
    "visualize(transformed['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [A.RandomFog(fog_coef_lower=0.7, fog_coef_upper=0.8, alpha_coef=0.1, p=1)],\n",
    ")\n",
    "random.seed(7)\n",
    "transformed = transform(image=image)\n",
    "visualize(transformed['image'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "# frames available\n",
    "train_images_path = \"data/data_faster_rcnn/train/images\"\n",
    "train_list = os.listdir(train_images_path)\n",
    "random.shuffle(train_list)\n",
    "\n",
    "def add_to_dataset(image, dataset_name):\n",
    "    \"\"\"\n",
    "    Add an image to the specified dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - image (numpy.ndarray): The image to add (as a NumPy array).\n",
    "    - dataset_name (str): The name of the dataset to add the image to.\n",
    "    \"\"\"\n",
    "    # Get the dataset directory\n",
    "    dataset_dir = os.path.join('data/augmented_images', dataset_name)\n",
    "    \n",
    "    # Create the dataset directory if it doesn't exist\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    \n",
    "    # Get the image filename\n",
    "    image_id = len(os.listdir(dataset_dir)) + 1\n",
    "    image_filename = f\"{image_id}.jpg\"\n",
    "    \n",
    "    # Convert NumPy array to PIL image\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Save the image\n",
    "    image_pil.save(os.path.join(dataset_dir, image_filename))\n",
    "\n",
    "\n",
    "# Loop over train list\n",
    "for count, image_name in enumerate(train_list):\n",
    "    image_path = os.path.join(train_images_path, image_name)\n",
    "    \n",
    "    # Load image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    #check if we have max images per set\n",
    "    random_string = random.choice([\"rain\", \"sunflare\", \"shadow\", \"fog\"])\n",
    "\n",
    "    \n",
    "\n",
    "    # Apply augmentation based on the random choice\n",
    "    if random_string == \"rain\":\n",
    "        transform = A.Compose(\n",
    "            [A.RandomRain(brightness_coefficient=0.9, drop_width=1, blur_value=5, p=1)],\n",
    "        )\n",
    "\n",
    "    elif random_string == \"sunflare\":\n",
    "        transform = A.Compose(\n",
    "            [A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=1, p=1)],\n",
    "        )\n",
    "\n",
    "    elif random_string == \"shadow\":\n",
    "        transform = A.Compose(\n",
    "            [A.RandomShadow(num_shadows_lower=1, num_shadows_upper=1, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=1)],\n",
    "        )\n",
    "\n",
    "    elif random_string == \"fog\":\n",
    "        transform = A.Compose(\n",
    "            [A.RandomFog(fog_coef_lower=0.7, fog_coef_upper=0.8, alpha_coef=0.1, p=1)],\n",
    "        )\n",
    "\n",
    "    # Apply transformation\n",
    "    transformed = transform(image=image)\n",
    "    \n",
    "    # Save transformed image\n",
    "    add_to_dataset(transformed['image'], random_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go over each set rain, sunflare, shadow, fog and predict the images\n",
    "\n",
    "# base path\n",
    "base_path = \"data/augmented_images\"\n",
    "\n",
    "# loop over each set\n",
    "\n",
    "for set_name in [\"rain\", \"sunflare\", \"shadow\", \"fog\"]:\n",
    "    # Get all images in the set\n",
    "    set_path = os.path.join(base_path, set_name)\n",
    "\n",
    "    # loop over each image and use the predict_image function\n",
    "    for count, image_name in enumerate(os.listdir(set_path)):\n",
    "        image_path = os.path.join(set_path, image_name)\n",
    "        predict_image(image_path, threshold=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "## Model metrics\n",
    "\n",
    "<img src=\"data/models/validation.png\" alt=\"validation over epochs\" width=1000>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open training log file\n",
    "\n",
    "log_file_path = os.path.join(\"data\",\"models\", \"training_log.txt\")\n",
    "\n",
    "with open(log_file_path, \"r\") as log_file:\n",
    "    print(log_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "The fintuned model Faster R-CNN has been trained for 18 epochs and reached a these maximal metrics:\n",
    "\n",
    "- **mAP@IoU=0.50:0.95 (overall mAP)** → Epoch 9, value: 0.5303\n",
    "- **mAP@IoU=0.50** → Epoch 7 and 12 to 18 value: 0.9792\n",
    "- **mAP@IoU=0.75** → Epoch 6, value: 0.4170\n",
    "- **Medium mAP** → Epoch 8, value: 0.4431\n",
    "- **Large mAP** → Epoch 6, value: 0.5395\n",
    "- **RPN Box Reg** → Epoch 5 to 18, value: 0.0008\n",
    "- **Objectness** → Epoch 5 to 18, value: 0.0007\n",
    "- **Box Reg** → Epoch 18, value: 0.0341\n",
    "- **Classifier** → Epoch 4 and 5, value: 0.0182\n",
    "- **Train loss** → Epoch 18, 12, 10, 7, value: 0.0540\n",
    "\n",
    "\n",
    "The model which was chosen as the best model was the model with the highest mAP@IoU=0.50:0.95 (overall mAP), so the checkpoint at epoch 9\n",
    "\n",
    "When evaluating the best model on the test set we get these metrics:\n",
    "- mAP@=0.50:0.95: 0.5634\n",
    "- mAP@0.5: 0.9892\n",
    "- mAP@0.75: 0.4738\n",
    "- mAP medium: 0.4648\n",
    "- mAP large: 0.5724\n",
    "\n",
    "Which are slightly better than the results on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "## Training analysis\n",
    "Based on the analysis of the training and validation metrics, it can be concluded that additional training with the current configuration (data and hyperparameters) yields diminishing returns. The training loss remains stable at approximately 0.0540 over the last 7 to 18 epochs, indicating little improvement with further training. Additionally, the validation mAP scores have plateaued between epochs 4 and 18, showing no significant change. \n",
    "\n",
    "Therefore, it can be inferred that training for around 7 epochs provides near-optimal results while minimizing the time spent on training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "## False positives\n",
    "\n",
    "Predictions:\n",
    "  - hazmat: 0.88\n",
    "  - hazmat: 0.15\n",
    "  - hazmat: 0.14\n",
    "  - hazmat: 0.06\n",
    "\n",
    "<br>\n",
    "<img src=\"images/predictions/bmw_prediction.png\" alt=\"Model prediction on a BMW licence plate\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "Predictions:\n",
    "- hazmat: 0.82\n",
    "- hazmat: 0.81\n",
    "- hazmat: 0.14\n",
    "- hazmat: 0.11\n",
    "- hazmat: 0.06\n",
    "- hazmat: 0.06\n",
    "- hazmat: 0.06\n",
    "- hazmat: 0.06\n",
    "- hazmat: 0.05\n",
    "- hazmat: 0.05\n",
    "\n",
    "<img src=\"images/predictions/trafficsignpred.png\" alt=\"Model prediction on a traffic sign\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "The model appears to produce some false positives, mistakenly identifying certain objects as hazmat placards when they are not. This issue is particularly prevalent with objects that are square and have colors such as yellow, red, or orange. Expanding the training dataset and applying data augmentation techniques should help mitigate this problem by improving the model's ability to differentiate between actual placards and visually similar objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "## False negatives\n",
    "\n",
    "<img src=\"images/predictions/hazmatclose.png\" width=600>\n",
    "\n",
    "In certain cases, the model failed to detect UN number placards in high-resolution images. This issue likely arises because the training dataset primarily consists of images where UN number placards were captured from a distance. As a result, the model may have overfitted to the assumption that UN numbers appear relatively small in images. To address this, applying targeted data augmentation techniques—such as zooming in on UN numbers, rotating them (e.g., upside down), and varying their orientation—can help the model generalize better to different scales and perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "## Weather Conditions\n",
    "\n",
    "For this project, it was essential to develop a robust model capable of performing well under various weather conditions. However, our dataset primarily consisted of images captured under different lighting conditions, such as day and night, without significant weather variations.  \n",
    "\n",
    "To address this limitation, we evaluated the model using augmented images generated with the **Albumentations** library, applying weather-related transformations such as **rain, sunflare, shadow, and fog.** The model performed reasonably well on these augmented images, successfully identifying objects in most cases. However, it is important to note that data augmentation does not always provide a fully realistic representation of real-world weather conditions.  \n",
    "\n",
    "While the results indicate that the model can likely handle different weather conditions to some extent, further improvements are needed to ensure robust performance in real-life scenarios. Incorporating actual weather-diverse data, along with advanced augmentation techniques that simulate real-world complexity more accurately, would enhance the model's generalization capabilities.\n",
    "\n",
    "### Predictions on Augmented Images:\n",
    "\n",
    "<h4> Rain </h4>\n",
    "<img src=\"images/predictions/rainpred.png\" alt=\"Augmented image with rain effect\" width=\"400\"/>\n",
    "<h4> Flare </h4>\n",
    "<img src=\"images/predictions/flare.png\" alt=\"Augmented image with sunflare effect\" width=\"400\"/>\n",
    "<h4> Shadows </h4>\n",
    "<img src=\"images/predictions/shadowspred.png\" alt=\"Augmented image with shadow effect\" width=\"400\"/>\n",
    "<h4> Fog </h4>\n",
    "<img src=\"images/predictions/fog_pred.png\" alt=\"Augmented image with fog effect\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "## Reflection on Data Mining Goals\n",
    "\n",
    "### **Primary Data Mining Goal:**\n",
    "Create and train an object detection model capable of identifying and interpreting UN number hazard plates on freight wagons in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "### **Specific Data Mining Goals:**\n",
    "\n",
    "#### **1. Object Detection and Localization:**\n",
    "Develop a model that achieves a high AP score for accurately detecting and localizing hazard plates on freight wagons within each video frame.\n",
    "\n",
    "**Approach and Outcome:**  \n",
    "To accomplish this goal, we finetuned a Faster R-CNN model, which demonstrated promising results in localizing hazard placards. However, the model is not yet fully robust and occasionally produces false positives. These false detections often occur when objects with similar visual characteristics—such as square shapes and colors resembling hazard placards (e.g., yellow, red, or orange)—are present in the scene.\n",
    "\n",
    "**Improvement Strategies:**\n",
    "- **1.1: Expanding the training dataset** with a greater variety of real-world scenarios to improve generalization.\n",
    "- **1.2: Advanced data augmentation**, such as applying transformations that simulate real-life conditions (e.g., partial occlusion, varying angles, and different lighting conditions).\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Robustness Across Variable Conditions:**\n",
    "Enhance the model’s robustness by training it on datasets representing diverse lighting and weather conditions, with a goal to maintain high AP scores across these environments.\n",
    "\n",
    "**Approach and Outcome:**  \n",
    "While we did not have access to datasets covering a wide range of weather conditions, we leveraged data captured at different times of the day, covering various lighting conditions such as daytime, nighttime, and low-light scenarios. The model demonstrated strong performance across these lighting variations, indicating a certain level of robustness in this aspect.\n",
    "\n",
    "To evaluate its performance under different weather conditions, we used data augmentation techniques from the **Albumentations** library. Augmentations like rain, sunflare, shadow, and fog were introduced to simulate adverse weather conditions. Although the model performed reasonably well on these augmented images, it is important to acknowledge that synthetic augmentations do not fully replicate real-world conditions.\n",
    "\n",
    "**Further Improvements:**\n",
    "To improve the model’s robustness, it would be beneficial to collect and incorporate real-world data by filming freight wagons across different seasons and weather conditions. This would ensure the model can generalize better to practical scenarios. Additional techniques to enhance robustness, such as advanced augmentation strategies, are discussed in sections **1.1** and **1.2**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Optimization for Real-Time Processing:**\n",
    "Implement real-time object detection and OCR capabilities to ensure the model operates at a frame rate suitable for analyzing images from moving trains.\n",
    "\n",
    "**Approach and Outcome:**  \n",
    "Currently, the model does not perform in real-time. Inference can take up to **2.8 seconds per frame**, and reading the hazard placard (OCR) requires an additional **2 seconds**, making the total processing time **at least 4.8 seconds per frame**. This delay is far from real-time performance requirements, which typically demand processing speeds of **30 frames per second (FPS)** or faster, depending on the train's speed and camera setup.\n",
    "\n",
    "**Optimization Strategies:**\n",
    "To enhance processing speed, several optimization strategies can be considered:\n",
    "- **4.1: Model Quantization:** Reducing the precision of model parameters (e.g., from 32-bit floating point to 8-bit integers) to speed up computations with minimal accuracy loss.\n",
    "- **4.2: Efficient Attention Mechanisms:** Using lightweight attention models to focus computational resources on relevant regions, improving both speed and accuracy.\n",
    "- **4.3: Model Pruning:** Removing redundant weights and layers to reduce computation overhead.\n",
    "- **4.4: Hardware Acceleration:** Leveraging GPUs, TPUs, or edge AI devices for faster inference.\n",
    "\n",
    "#### Data Mining Success Criteria evaluation\n",
    "\n",
    "- **Object Detection AP**: Achieve a Mean Average Precision (mAP) of at least 0.70 for detecting and localizing hazard plates across varied conditions.  \n",
    "  **Outcome:** Not achieved, with an mAP of 0.5303.\n",
    "\n",
    "- **OCR Precision for UN Numbers**: Ensure the Tesseract OCR module achieves high accuracy in reading UN numbers, even under challenging conditions, with a target precision score above 0.95.  \n",
    "  **Outcome:** Tesseract was unable to consistently recognize codes in difficult conditions, so we switched to using the **idefics2 VLM**, which performed significantly better, even with low-quality images. However, accuracy metrics for idefics2 have not been formally evaluated.\n",
    "\n",
    "- **Processing Speed**: Ensure the model achieves a processing time per frame under 100 milliseconds to maintain real-time functionality.  \n",
    "  **Outcome:** Not achieved. The model takes longer to process predictions, and the OCR stage, which involves **idefics2**, also contributes to longer processing times, resulting in a total time greater than 100 milliseconds.\n",
    "\n",
    "- **Environmental Robustness**: Maintain consistent mAP scores across different lighting and weather conditions.  \n",
    "  **Outcome:** Partly achieved. Lighting conditions were well represented in the dataset, and the model performed consistently across different lighting variations. Weather conditions, however, were not included in the dataset, but the model performed reasonably well when evaluated with augmented data. The mAP score was not measured for augmented weather conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "## Demo for YOLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained YOLO model\n",
    "model = YOLO(\"./data/yolo/best_augmented_scaled.pt\", task=\"detect\")\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to draw bounding boxes on the frame\n",
    "def draw_boxes(frame, results):\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            x_min, y_min, x_max, y_max = map(int, box.xyxy[0])  # Convert to integers\n",
    "            confidence = box.conf[0]  # Confidence score\n",
    "            class_id = int(box.cls[0])  # Class ID\n",
    "            label = result.names[class_id]  # Class label\n",
    "\n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            # Put the label and confidence score\n",
    "            cv2.putText(frame, f\" {confidence:.2f}\", (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    return frame\n",
    "\n",
    "# Open the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Predict the frame using the YOLO model\n",
    "    results = model(frame,stream=True)\n",
    "\n",
    "    # Draw bounding boxes on the frame\n",
    "    frame = draw_boxes(frame, results)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
