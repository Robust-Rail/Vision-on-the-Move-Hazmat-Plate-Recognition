{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92064ee0",
   "metadata": {},
   "source": [
    "# UN number detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddbc104",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "For this project, we have been tasked with developing a machine learning model capable of recognizing UN number hazard plates. These plates, commonly displayed on freight train wagons, indicate the types of hazardous materials being transported. The successful implementation of this model will contribute to a more efficient and secure railway system across the EU.\n",
    "\n",
    "<img src=\"https://interessante-bilder.startbilder.de/1200/zagns-gaskesselwagen-aus-deutschland-vtg-498869.jpg\" alt=\"Hazard Plate\" width=\"500\"/>\n",
    "\n",
    "The hazard plates play a crucial role in ensuring the safety of transportation by providing essential information about the nature of the substances on board, such as flammability, toxicity, or corrosiveness. By automating the recognition process with machine learning, the handling and tracking of these hazardous materials can be streamlined, reducing manual labor and minimizing potential human errors.\n",
    " Determine business objectives\n",
    "\n",
    "### Determine business objectives\n",
    "#### Background\n",
    "The specific expectations and objectives of the EU for this project are not yet fully defined, but the initiative's roots are clear. This project is spearheaded by the University of Twente, with researcher Mellisa Tijink serving as our supervisor. Our team, composed of pre-master's Computer Science students, has been tasked with developing the machine learning model. Mellisa Tijink plays a pivotal role as the intermediary between our team and major stakeholders, including ProRail, the EU, and other experts in the field.\n",
    "\n",
    "This project is part of a broader initiative aimed at enhancing rail freight operations within Europe, aligning with the EU’s goals for improved efficiency and safety. More information on the initiative can be found on the official project site: [EU Rail FP5](https://projects.rail-research.europa.eu/eurail-fp5/).\n",
    "\n",
    "Flagship Project 5: *TRANS4M-R aims to establish rail freight as the backbone of a low-emission, resilient European logistics chain that meets end-user needs. It focuses on two main technological clusters: 'Full Digital Freight Train Operation' and 'Seamless Freight Operation', which will develop and demonstrate solutions to increase rail capacity, efficiency, and cross-border coordination. By integrating Digital Automatic Coupler (DAC) solutions with software-defined systems, the project seeks to optimize network management and enhance cooperation among infrastructure managers. The ultimate goal is to create an EU-wide, interoperable rail freight framework with unified technologies and seamless operations across borders and various stakeholders, boosting the EU transport and logistics sector.*\n",
    "\n",
    "#### Business objectives\n",
    "\n",
    "**Primary Objective:** Develop an object detection model for UN number hazard plates on freight wagons.\n",
    "\n",
    "**Sub-objectives:**\n",
    "1. Detect and identify UN number hazard plates: Ensure the model can accurately locate hazard plates on freight wagons. \n",
    "2. Read and interpret the UN numbers: Implement recognition capabilities to accurately read the numbers on the detected plates.\n",
    "3. Ensure model robustness and accuracy: Train the model to achieve high accuracy and reliability under various conditions (e.g., different lighting, weather).\n",
    "4. Optimize model for speed: Make sure the model runs efficiently and in real-time to function on moving trains.\n",
    "5. Adapt the model for moving environments: Design and test the model to handle the unique challenges of detecting and reading plates on trains in motion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b5b7f5",
   "metadata": {},
   "source": [
    "### Assess Situation\n",
    "\n",
    "#### Inventory of resources\n",
    "\n",
    "**Business Experts:** Our team currently lacks extensive expertise in this area. We can consult Tijink for some questions\n",
    "\n",
    "**Data Mining Team:** \n",
    "- Melissa Tijink (Researcher in Data Management & Biometrics/Electrical Engineering, Mathematics, and Computer Science)\n",
    "- E. Talavera Martínez (Researcher in Data Management & Biometrics)\n",
    "- Ewaldo Nieuwenhuis (Pre-master student in Computer Science)\n",
    "- Stanislav Levendeev (Pre-master student in Computer Science)\n",
    "\n",
    "**Data:**\n",
    "1. **Video Data of Freight Trains:** This consists of video footage of moving freight trains, where the freight wagons should display the UN numbers.\n",
    "2. **Line Scan Camera Pictures:** These are high-resolution images of the train, but they are very spread out. It is still uncertain if these will be useful.\n",
    "3. **Photos of ADR Warning Signs:** These are images of ADR signs on freight trains. However, this is not exactly what we need since our objective is to build a model that recognizes UN numbers.\n",
    "4. **Public data sources of trucks:** These are public images from the internet from trucks containing the UN numbers, unlabeled images\n",
    "\n",
    "**Computing Resources:** We have access to a cluster from the University of Twente, which we can use to train or fine-tune our model.\n",
    "\n",
    "**Software:** We will use Python, Jupyter Notebook, Keras, PyTorch, and TensorFlow for analyzing, cleaning, preparing the data, and modeling. For data labeling, we will use [CVAT](https://www.cvat.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fa8efa",
   "metadata": {},
   "source": [
    "### Requirements, assumptions, and constraints\n",
    "\n",
    "##### Requirements\n",
    "- Object detection capability for UN number hazard plates.\n",
    "- Text recognition to read and extract UN numbers.\n",
    "- High accuracy and precision in detection and recognition.\n",
    "- Robust performance under varying conditions (weather, lighting, speed).\n",
    "- Speed optimization for fast processing with minimal lag\n",
    "- Real-time processing for operation on moving trains.\n",
    "\n",
    "##### Assumptions\n",
    "- Consistent access to a high-performance computational cluster for model training and testing.\n",
    "- The high-performance cluster is necessary due to the heavy processing demands of deep learning models.\n",
    "- Local machines are not sufficient for the required high computational tasks.\n",
    "- Project-specific data, including images and videos of freight trains with hazard plates, will be provided as planned.\n",
    "- Data will include varied conditions (different lighting and weather) to ensure robustness.\n",
    "- Access to diverse data is essential for creating a model that generalizes well to real-world scenarios.\n",
    "- The stakeholders will provide timely feedback to guide any changes or adaptations needed in the project.\n",
    "\n",
    "##### Constraints\n",
    "- The team has restricted experience with advanced object detection methods, which may impact the initial development and refinement of the model.\n",
    "- Most of the available data is not labeled, presenting a challenge for training supervised machine learning models. Some labeled data exists but belongs to another researcher, and access to it is uncertain.\n",
    "- The dataset may be skewed with an overrepresentation of specific UN numbers from certain wagons, which could limit the model's ability to generalize across different scenarios.\n",
    "- The size of the dataset makes it difficult to filter out specific wagons or relevant segments efficiently, posing a challenge for data processing and targeted training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8b668c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaee2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "error\n",
    "import cv2 \n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import Image\n",
    "from IPython.display import display as ipy_display\n",
    "\n",
    "sys.path.append('..')  # Adds the parent directory to the path\n",
    "\n",
    "from src.draw.utils import draw_box\n",
    "from src.annotation.annotation_validator import (\n",
    "    display_yolo_sample,\n",
    "    analyze_yolo_label_distribution,\n",
    "    display_sample_annotations_yolo,\n",
    "    find_missing_labels,\n",
    "    visualize_annotations_from_links\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8ec92",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "## Collect Initial Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef07547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing video files\n",
    "data_dir = os.environ[\"PATH_TO_DATA\"]\n",
    "video_directory = data_dir + \"/videos\"\n",
    "print (video_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732fe416",
   "metadata": {},
   "source": [
    "### ProRail Dataset of videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499185ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all filenames in the directory\n",
    "video_files = [f for f in os.listdir(video_directory) if f.endswith(('.mp4'))]\n",
    "video_files[0]\n",
    "# show example video file in jupyter notebook with display\n",
    "example_video_path = os.path.join(video_directory, video_files[0])\n",
    "display.Video(example_video_path, embed=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce92f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(video_files[0])\n",
    "df_video = pd.read_csv(f'{data_dir}/video_data_info.csv')\n",
    "df_video.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d86c9",
   "metadata": {},
   "source": [
    "### HIN text labels\n",
    "This dataset is the dataset that describes which hazardous materials are being transported\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hin = pd.read_csv(f\"{data_dir}/un-number-labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff871fcd",
   "metadata": {},
   "source": [
    "### COCO dataset ProRail\n",
    "The COCO dataset is a formatted dataset for training the model for Faster-RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dir = data_dir + \"/data_faster_rcnn\"\n",
    "train_dir = coco_dir + \"/train\"\n",
    "val_dir = coco_dir + \"/val\"\n",
    "test_dir = coco_dir + \"/test\"\n",
    "\n",
    "# image files are in the images folder in each of the train, val and test folders\n",
    "train_image_dir = train_dir + \"/images\"\n",
    "val_image_dir = val_dir + \"/images\"\n",
    "test_image_dir = test_dir + \"/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a701100b",
   "metadata": {},
   "source": [
    "### Ultralytic YOLO format ProRail\n",
    "This is the dataset used to finetune the YOLO model this is in Ultralytics format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory for your YOLO dataset\n",
    "YOLO_ROOT = Path(f\"{data_dir}/yolo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffdee20",
   "metadata": {},
   "source": [
    "### HazTruck dataset\n",
    "This is the public dataset including public images and annotations for trucks with hazmat plates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76955879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_haztruck = pd.read_csv(f'{data_dir}/haztruck_dataset.csv')\n",
    "df_haztruck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d31912",
   "metadata": {},
   "source": [
    "## Describe Data\n",
    "### Video dataset ProRail\n",
    "**Column Description:**\n",
    "\n",
    "- **Unnamed: 0**: The ID of the video.\n",
    "- **filename**: The name of the file, including the `.mp4` extension.\n",
    "- **fps**: Frames per second.\n",
    "- **frame_count**: The total number of frames in the video.\n",
    "- **width**: The width of the video in pixels.\n",
    "- **height**: The height of the video in pixels.\n",
    "- **resolution**: The video resolution, expressed as `width x height`.\n",
    "- **duration_seconds**: The video's duration in seconds.\n",
    "- **hash**: Hash of the video to check if it is original\n",
    "- **file_size_mb**: The file size in megabytes.\n",
    "- **train_detected**: Indicates whether a train was detected using a YOLO model. If the model's confidence score exceeded 10%, a train is considered detected, though this may not always be accurate.\n",
    "- **confidence**: The confidence score indicating how likely it is that the video contains a train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" The rows and column size are {df_video.shape} \\n\")\n",
    "print(f\" The describe of the dataframe is: \\n {df_video.describe(include='all')} \\n\")\n",
    "print(f\" The info of the dataframe is: \\n {df_video.info()} \\n\")\n",
    "print(f\" The columns of the dataframe are: \\n {df_video.columns} \\n\")\n",
    "print(f\" The number of null values in each column are: \\n {df_video.isnull().sum()} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f58b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_video['hash'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ac0c0",
   "metadata": {},
   "source": [
    "They seem to be all orginal in terms of hashing, we probably cannot determine duplicates by hash alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1df5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MP4 video\n",
    "video = \"1690279852.mp4\"\n",
    "video2 = \"1690281303.mp4\"\n",
    "video_path = video_directory+'/'+video\n",
    "video2_path = video_directory+'/'+video2\n",
    "# Embed video in the notebook\n",
    "\n",
    "display.Video(video_path, embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53068851",
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Video(video2_path, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a502e926",
   "metadata": {},
   "source": [
    "**These are the same videos, the second video is only one second longer than the first video. There are duplicate video's in this dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafd789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos sorted by highest to lowest FPS\n",
    "df_video['fps'] = pd.to_numeric(df_video['fps'], errors='coerce')\n",
    "df_x = df_video.sort_values(by='fps', ascending=False)\n",
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max and avg fps\n",
    "min_fps = df_video['fps'].min()\n",
    "max_fps = df_video['fps'].max()\n",
    "avg_fps = df_video['fps'].mean()\n",
    "print(f\" The minimum fps is {min_fps}, the maximum fps is {max_fps} and the average fps is {avg_fps} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9f89a",
   "metadata": {},
   "source": [
    "The frames per second (FPS) in your video data can influence the performance of training your model. FPS affects the temporal resolution and the amount of data fed into the model. When training on video data, the FPS determines how many frames are available to capture motion or other temporal patterns, which can influence model performance.\n",
    "\n",
    "For example, if you use a higher FPS, your model will have more frames to analyze within a given time frame, potentially improving its ability to capture finer details in motion (e.g., in object detection or action recognition tasks). However, processing more frames per second can also lead to higher computational costs and may require more memory and processing power, which might reduce training efficiency unless properly optimized.\n",
    "[source 1](https://library.fiveable.me/key-terms/deep-learning-systems/frames-per-second-fps)\n",
    "\n",
    "\n",
    "Conversely, lower FPS can reduce computational demands but may also decrease the temporal resolution of your data, making it harder for your model to accurately capture fast movements or dynamic changes. Depending on your specific use case, you'll need to balance FPS with your model's ability to process the data effectively while managing computational resources.\n",
    "[source 2](https://paulbridger.com/posts/video-analytics-pipeline-tuning/)\n",
    "\n",
    "It's also important to consider other factors like video resolution and preprocessing techniques, which could further affect how FPS influences your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827aae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(df, column, xlabel, ylabel=\"Frequency\", bins=10):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(df[column], bins=bins, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    plt.title(f\"Distribution of {xlabel}\", fontsize=14)\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0365be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(df_video, 'fps', xlabel='Frames Per Second (FPS)')\n",
    "plot_distribution(df_video, 'frame_count', xlabel='Total Frames', bins=20)\n",
    "plot_distribution(df_video, 'duration_seconds', xlabel='Duration (seconds)', bins=20)\n",
    "plot_distribution(df_video, 'file_size_mb', xlabel='File Size (MB)', bins=15)\n",
    "plot_distribution(df_video, 'resolution', xlabel='Resolution', bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34beb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resolutions(df):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(df['width'], df['height'], c='orange', alpha=0.7, edgecolors='black')\n",
    "    plt.title(\"Resolution Scatter Plot (Width vs Height)\", fontsize=14)\n",
    "    plt.xlabel(\"Width (pixels)\", fontsize=12)\n",
    "    plt.ylabel(\"Height (pixels)\", fontsize=12)\n",
    "    plt.grid(linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "plot_resolutions(df_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f8ef3",
   "metadata": {},
   "source": [
    "### HIN text labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c97da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get amount of unique UN numbers in the dataframe\n",
    "unique_un_numbers = df_hin['number'].nunique()\n",
    "print(f\" The amount of unique UN numbers in the dataframe is {unique_un_numbers} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec9a04",
   "metadata": {},
   "source": [
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.firesafe.org.uk%2Fwp-content%2Fuploads%2F2011%2F05%2FHazchem_sign3.gif&f=1&nofb=1&ipt=213e14aa1b853778982c7947745401044f12ea17de088b2a1556d3f89f5fd052\" alt=\"Image of UN number plate\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337bf30c",
   "metadata": {},
   "source": [
    "The Hazard Identification Number (HIN) plate is used to identify the type of hazardous material a train is carrying. You can find the HIN on the upper part of the plate (1). The class number is not relevant in this research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09744c7",
   "metadata": {},
   "source": [
    "### COCO dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d0174d",
   "metadata": {},
   "source": [
    "We used an 80/10/10 split for our dataset:\n",
    "\n",
    "- **80% Training:** Used to train the model and learn patterns from the data.\n",
    "- **10% Validation:** Used to tune model hyperparameters and prevent overfitting.\n",
    "- **10% Testing:** Used to evaluate the final model performance on unseen data.\n",
    "\n",
    "**Dataset Directory Structure (COCO Format)**\n",
    "\n",
    "The dataset is organized using a structure inspired by the popular **COCO (Common Objects in Context)** format. This is a standard and highly effective way to manage data for object detection, ensuring that the images and their corresponding labels are kept separate but clearly linked.\n",
    "\n",
    "The data is split into three distinct sets: **training**, **validation**, and **testing**.\n",
    "\n",
    "**Visual Layout**\n",
    "\n",
    "Here is a visual representation of the directory tree:\n",
    "\n",
    "```\n",
    "data_faster_rcnn/\n",
    "├── train/\n",
    "│   ├── images/\n",
    "│   │   ├── 1690801380_00384.jpg\n",
    "│   │   ├── 1690801380_00385.jpg\n",
    "│   │   └── ...\n",
    "│   └── annotations/\n",
    "│       └── instances_train.json\n",
    "│\n",
    "├── val/\n",
    "│   ├── images/\n",
    "│   │   ├── 1690802500_00100.jpg\n",
    "│   │   └── ...\n",
    "│   └── annotations/\n",
    "│       └── instances_val.json\n",
    "│\n",
    "└── test/\n",
    "    ├── images/\n",
    "    │   │   ├── 1690804000_00050.jpg\n",
    "    │   │   └── ...\n",
    "    └── annotations/\n",
    "        └── instances_test.json\n",
    "```\n",
    "\n",
    "**Explanation of Components**\n",
    "\n",
    "*   **data_faster_rcnn/**: This is the main root folder that contains the entire dataset.\n",
    "\n",
    "*   **train/, val/, test/**: These three folders represent the primary data splits.\n",
    "    *   **train**: This folder contains the majority of the data, used for **training** the machine learning model.\n",
    "    *   **val**: (Validation) This folder holds a smaller set of data used to fine-tune the model during training and prevent it from \"memorizing\" the training data.\n",
    "    *   **test**: This folder contains data that the model has never seen before, used for the final **evaluation** of the model's performance.\n",
    "\n",
    "*   **images/**: Inside each of the `train`, `val`, and `test` folders, this sub-folder contains all the actual image files (e.g., `.jpg`, `.png`).\n",
    "\n",
    "*   **annotations/**: This sub-folder contains the label data.\n",
    "    *   **instances_train.json**: This single JSON file contains all the annotations (like bounding boxes and object categories) for **every single bounding box** featured inside the `train/images/` folder. The same logic applies to `instances_val.json` and `instances_test.json`.\n",
    "\n",
    "**Image Filename Convention**\n",
    "\n",
    "The images in this dataset are individual frames extracted from the **Prorail video dataset**. Each filename follows a specific pattern that links it back to the original source video and its position within that video.\n",
    "\n",
    "The format is: **`video_id_frame_id.jpg`**\n",
    "\n",
    "**Example:** `1690801380_00384.jpg`\n",
    "\n",
    "*   **1690801380**: This is the unique **Video ID**, which identifies the source video the frame was taken from.\n",
    "*   **00384**: This is the **Frame ID**, which represents the specific frame number from that video sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ab975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(image_dir, n=3, title=\"\"):\n",
    "    print(title)\n",
    "    image_files = os.listdir(image_dir)[:n]\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        ipy_display(Image(filename=img_path, width=300))\n",
    "\n",
    "show_examples(train_image_dir, n=1, title=\"Example training images:\")\n",
    "show_examples(val_image_dir, n=1, title=\"Example validation images:\")\n",
    "show_examples(test_image_dir, n=1, title=\"Example test images:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb0f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define the path ---\n",
    "annotation_file = os.path.join(train_dir, \"annotations\", \"instances_train.json\")\n",
    "\n",
    "# --- 2. Load the JSON annotation file ---\n",
    "print(f\"Loading annotations from: {annotation_file}\\n\")\n",
    "with open(annotation_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- 3. Explore the main structure of the JSON file ---\n",
    "print(\"The main keys in the JSON file are:\")\n",
    "print(f\"-> {list(data.keys())}\\n\")\n",
    "\n",
    "# --- 4. Create helpful mappings for easy data lookup ---\n",
    "# Create a dictionary to map category IDs to category names\n",
    "categories = {cat['id']: cat['name'] for cat in data['categories']}\n",
    "print(\"Found Categories:\")\n",
    "print(f\"-> {categories}\\n\")\n",
    "\n",
    "# Create a dictionary to map image IDs to their file names\n",
    "images = {img['id']: img['file_name'] for img in data['images']}\n",
    "\n",
    "# --- 5. Display the first few annotations ---\n",
    "print(\"--- Displaying Details for the First 5 Annotations ---\\n\")\n",
    "\n",
    "# Get the list of all annotations\n",
    "annotations = data['annotations']\n",
    "\n",
    "for i, ann in enumerate(annotations[:5]): # Loop through the first 5 annotations\n",
    "    image_id = ann['image_id']\n",
    "    category_id = ann['category_id']\n",
    "    bbox = ann['bbox']\n",
    "\n",
    "    # Use the mappings to get human-readable names\n",
    "    image_filename = images[image_id]\n",
    "    category_name = categories[category_id]\n",
    "\n",
    "    print(f\"**Annotation #{i+1}**\")\n",
    "    print(f\"  - Image File:    {image_filename}\")\n",
    "    print(f\"  - Category:      {category_name}\")\n",
    "    print(f\"  - Bounding Box:  {bbox}  (Format: [x, y, width, height])\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65834b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show amount of annotations\n",
    "print(f\"Total number of annotations: {len(data['annotations'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show if any are null: x, y, width, height or Image File\n",
    "null_bboxes = [ann for ann in data['annotations'] if any(coord is None for coord in ann['bbox'])]\n",
    "null_image_ids = [ann['image_id'] for ann in data['annotations'] if ann['image_id'] not in images]\n",
    "print(f\"Number of annotations with null bounding box coordinates: {len(null_bboxes)}\")\n",
    "print(f\"Number of annotations with invalid image IDs: {len(null_image_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0638bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Select a Sample Annotation ---\n",
    "# Let's pick the 10th annotation in the list as an example\n",
    "annotation = data['annotations'][10]\n",
    "\n",
    "# --- 3. Find the Corresponding Image Path ---\n",
    "image_id = annotation['image_id']\n",
    "# Use a generator expression to find the image info dict matching the ID\n",
    "image_info = next(img for img in data['images'] if img['id'] == image_id)\n",
    "image_path = os.path.join(train_image_dir, image_info['file_name'])\n",
    "\n",
    "print(f\"Found Image Path: {image_path}\")\n",
    "\n",
    "# --- 4. Load the Image using OpenCV ---\n",
    "# cv2.imread loads the image as a NumPy array in BGR format\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for correct color display\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "\n",
    "if image is None:\n",
    "    raise FileNotFoundError(f\"Could not load image at path: {image_path}\")\n",
    "\n",
    "# --- 5. Extract and Format the Bounding Box ---\n",
    "# COCO format is [x_min, y_min, width, height]\n",
    "bbox_coco = annotation['bbox']\n",
    "\n",
    "# Convert COCO format to the format needed for drawing: (x_min, y_min, x_max, y_max)\n",
    "x, y, w, h = bbox_coco\n",
    "ground_truth = (x, y, x + w, y + h)\n",
    "\n",
    "# Get the category name to use as a label\n",
    "category_id = annotation['category_id']\n",
    "category_info = next(cat for cat in data['categories'] if cat['id'] == category_id)\n",
    "label = category_info['name']\n",
    "\n",
    "print(f\"Bounding Box (x, y, w, h): {bbox_coco}\")\n",
    "print(f\"Drawing Box (x1, y1, x2, y2): {ground_truth}\")\n",
    "print(f\"Label: {label}\")\n",
    "\n",
    "# --- 6. Draw the Bounding Box on the Image ---\n",
    "# Call your drawing function\n",
    "image_with_box = draw_box(image=image, ground_truth=ground_truth)\n",
    "\n",
    "# # --- 7. Display the Final Image ---\n",
    "# # Matplotlib displays images in RGB format, so we need to convert from OpenCV's BGR\n",
    "plt.title(f\"Ground Truth Box on Image: {image_info['file_name']}\")\n",
    "plt.axis('off') # Hide the axes for a cleaner look\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f65cba",
   "metadata": {},
   "source": [
    "### Ultralytics YOLO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e77100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first sample from the 'train' split\n",
    "display_yolo_sample(YOLO_ROOT, split=\"train\", image_index=0)\n",
    "\n",
    "# Display the fifth sample from the 'validation' split (example)\n",
    "display_yolo_sample(YOLO_ROOT, split=\"val\", image_index=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed99ec02",
   "metadata": {},
   "source": [
    "#### **Explanation of YOLO Label File Format**\n",
    "\n",
    "This document explains the structure and meaning of the YOLO `.txt` label files used for object detection tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Filename Convention**\n",
    "\n",
    "The name of each label file directly corresponds to an image file and often contains metadata about its source.\n",
    "\n",
    "**Example Filename:** `1690281365_00053.txt`\n",
    "\n",
    "This name is broken down as follows:\n",
    "*   **`1690281365`**: This is the **Video ID**, a unique identifier for the video from which the image frame was extracted.\n",
    "*   **`_`**: A separator character.\n",
    "*   **`00053`**: This is the **Frame Number**, indicating that this is the 53rd frame analyzed from that specific video.\n",
    "*   **`.txt`**: The file extension for the label file.\n",
    "\n",
    "The corresponding image for this label file would be named `1690281365_00053.jpg` (or `.png`, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "**2. File Content Structure**\n",
    "\n",
    "Each `.txt` label file contains one or more lines. **Each line represents a single bounding box** for one object detected in the image.\n",
    "\n",
    "The format for each line is:\n",
    "\n",
    "```\n",
    "[class_id] [x_center] [y_center] [width] [height]\n",
    "```\n",
    "\n",
    "All five values are space-separated. Let's break down each component:\n",
    "\n",
    "| Component | Description |\n",
    "| :--- | :--- |\n",
    "| **`class_id`** | An integer representing the object's class. This ID is **zero-indexed** (starts from 0). It maps to the class names defined in your `dataset.yaml` file (e.g., `0` could be 'person', `1` could be 'car'). |\n",
    "| **`x_center`** | The **horizontal center** of the bounding box. This value is **normalized** by the image's width, so it's a float between 0.0 and 1.0. (e.g., `0.5` means the center is exactly in the middle of the image horizontally). |\n",
    "| **`y_center`** | The **vertical center** of the bounding box. This value is **normalized** by the image's height, so it's a float between 0.0 and 1.0. (e.g., `0.5` means the center is exactly in the middle of the image vertically). |\n",
    "| **`width`** | The **width** of the bounding box. This value is also **normalized** by the image's width. |\n",
    "| **`height`** | The **height** of the bounding box. This value is also **normalized** by the image's height. |\n",
    "\n",
    "> **What does \"normalized\" mean?**\n",
    "> A normalized value is a fraction of the total dimension. To get the actual pixel value, you would multiply the normalized value by the image's dimension:\n",
    "> * `absolute_x_center_in_pixels = x_center * image_width`\n",
    "> * `absolute_width_in_pixels = width * image_width`\n",
    "> * `absolute_y_center_in_pixels = y_center * image_height`\n",
    "> * `absolute_height_in_pixels = height * image_height`\n",
    "\n",
    "---\n",
    "\n",
    "**3. Examples**\n",
    "\n",
    "**Example 1: Single Bounding Box**\n",
    "\n",
    "*   **File:** `1690281365_00053.txt`\n",
    "*   **Content:**\n",
    "    ```\n",
    "    0 0.0785078125 0.4359953704 0.072671875 0.0501203704\n",
    "    ```\n",
    "\n",
    "*   **Interpretation:**\n",
    "    *   `0`: This bounding box is for an object of class `0`.\n",
    "    *   `0.0785...`: The center of the box is located at ~7.85% of the image's width from the left edge.\n",
    "    *   `0.4359...`: The center of the box is located at ~43.6% of the image's height from the top edge.\n",
    "    *   `0.0726...`: The width of the box is ~7.27% of the total image width.\n",
    "    *   `0.0501...`: The height of the box is ~5.01% of the total image height.\n",
    "\n",
    "**Example 2: Multiple Bounding Boxes**\n",
    "\n",
    "If an image contains two objects, the label file will have two lines.\n",
    "\n",
    "*   **Content:**\n",
    "    ```\n",
    "    0 0.0785078125 0.4359953704 0.072671875 0.0501203704\n",
    "    0 0.0784407812 0.5359953704 0.1671875000 0.0501203704\n",
    "    ```\n",
    "\n",
    "*   **Interpretation:**\n",
    "    *   This file describes **two separate objects** found in the corresponding image.\n",
    "    *   Both objects belong to **class `0`**.\n",
    "    *   They are located at slightly different positions (note the different `y_center` values) and have different widths (`width`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfddf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "analyze_yolo_label_distribution(YOLO_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f84c2",
   "metadata": {},
   "source": [
    "We use a 80/10/10 split on the YOLO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323dc028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the validation check\n",
    "missing_files = find_missing_labels(YOLO_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce5781c",
   "metadata": {},
   "source": [
    "There are no images without a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sample_annotations_yolo(YOLO_ROOT, amount=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ead6f",
   "metadata": {},
   "source": [
    "\n",
    "#### Explanation of the YOLO `dataset.yaml` File\n",
    "\n",
    "The `dataset.yaml` file is the central configuration file for a YOLO dataset. It acts as a map, telling the training script where to find the data and what kind of objects it should learn to detect.\n",
    "\n",
    "**Example `dataset.yaml`**\n",
    "\n",
    "```yaml\n",
    "# ------------------------------------------------------------------\n",
    "# Root path to the dataset. All other paths are relative to this one.\n",
    "# ------------------------------------------------------------------\n",
    "path: yourpath\\UN-number-detection\\data\\annotations\\prorail\\yolo\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Paths to the image sets for each split (train, validation, test)\n",
    "# ------------------------------------------------------------------\n",
    "train: images/train  # Training images\n",
    "val: images/val      # Validation images\n",
    "test: images/test    # (Optional) Test images\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Dataset Class Definitions\n",
    "# ------------------------------------------------------------------\n",
    "nc: 1  # Number of classes\n",
    "names: ['hazmat_plate']  # List of class names\n",
    "```\n",
    "\n",
    "**Detailed Breakdown of Each Key**\n",
    "\n",
    "Here is a detailed explanation of what each line means.\n",
    "\n",
    "| Key | Example Value | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **`path`** | `yourpath\\yolo` | The **absolute path** to the root directory of your YOLO dataset. All other paths in this file (`train`, `val`, `test`) are relative to this one. |\n",
    "| **`train`** | `images/train` | The **relative path** from the `path` directory to the folder containing your **training images**. |\n",
    "| **`val`** | `images/val` | The **relative path** from the `path` directory to the folder containing your **validation images**. |\n",
    "| **`test`** | `images/test` | The **relative path** from the `path` directory to the folder containing your **test images**. This is optional and used for final model evaluation. |\n",
    "| **`nc`** | `1` | **Number of Classes**. This is a crucial integer that tells the model how many different object categories it needs to learn. |\n",
    "| **`names`** | `['hazmat_plate']` | A **list of class names**. The order of this list directly maps to the class IDs used in your `.txt` label files. |\n",
    "\n",
    "**How It Works Together**\n",
    "\n",
    "**1. Path Resolution:**\n",
    "\n",
    "The training script combines the `path` with the `train`, `val`, and `test` paths to find the images.\n",
    "\n",
    "> For example, based on the file above, the full path to the training images would be:\n",
    "> **`C:\\...\\prorail\\yolo`** + **`images/train`** = **`C:\\...\\prorail\\yolo\\images\\train`**\n",
    "\n",
    "**2. Class Mapping:**\n",
    "\n",
    "The `nc` and `names` fields are critical for the model to understand the labels. The index of the item in the `names` list corresponds to the `class_id` in your label files.\n",
    "\n",
    "> In this example:\n",
    "> *   The list `names` has **1** item.\n",
    "> *   The value of `nc` is **1**.\n",
    "> *   This means a `class_id` of **`0`** in a `.txt` label file will correspond to the class **`'hazmat_plate'`**.\n",
    "\n",
    "If you had more classes, the mapping would look like this:\n",
    "\n",
    "```yaml\n",
    "nc: 3\n",
    "names: ['person', 'car', 'dog']\n",
    "```\n",
    "*   `class_id` **`0`** = `'person'`\n",
    "*   `class_id` **`1`** = `'car'`\n",
    "*   `class_id` **`2`** = `'dog'`\n",
    "\n",
    "It is essential that `nc` is equal to the number of items in the `names` list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a0e21b",
   "metadata": {},
   "source": [
    "### Haztruck dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c364b",
   "metadata": {},
   "source": [
    "**Description of the `haztruck_dataset.csv` Master Annotation File**\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This file serves as the master annotation log for a dataset created to detect and identify UN Number hazardous material (hazmat) plates on trucks. It is structured as a comprehensive CSV (Comma-Separated Values) file, where each row represents a **single bounding box** for one hazmat plate. This rich format contains not only the location of the plate but also extensive metadata about the image source, quality, and the specific codes on the plate.\n",
    "\n",
    "**File Format**\n",
    "\n",
    "The data is stored in a standard CSV format. It is important to note that a single image can be represented by multiple rows if it contains more than one hazmat plate. For instance, if `image_id` 73 contains two plates, there will be two rows with `image_id` set to 73, each describing a different bounding box.\n",
    "\n",
    "**Column-by-Column Breakdown**\n",
    "\n",
    "The table below describes each of the 15 columns in the `haztruck_dataset.csv` file.\n",
    "\n",
    "| Column Name | Example Value | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **`image_id`** | `2` | A unique integer that identifies the image file. |\n",
    "| **`link`** | `https://stock.adobe.com/...` | The URL to the **source page** where the original image is hosted. For Adobe Stock, this is the product/landing page, **not a direct link to the image file itself.** |\n",
    "| **`website`** | `Adobe Stock` | The name of the website or platform where the image was sourced. |\n",
    "| **`author`** | `M. Perfectti` | The creator or uploader of the source image. |\n",
    "| **`resolution`** | `1000x667` | The dimensions of the image in pixels (Width x Height). |\n",
    "| **`date_accessed`** | `06/02/2025 18:06` | The timestamp when the image was downloaded and logged. |\n",
    "| **`added_by`** | `Ewaldo Nieuwenhuis` | The name of the annotator who added the data to the dataset. |\n",
    "| **`image_name`** | `2.png` | The local filename of the image, corresponding to its `image_id`. |\n",
    "| **`box_label`** | `hazmat_sign` | The class label for the annotated object (in this case, always a hazmat sign). |\n",
    "| **`box_xtl`** | `158.67` | The **X**-coordinate of the **T**op-**L**eft corner of the bounding box (in pixels). |\n",
    "| **`box_ytl`** | `186.83` | The **Y**-coordinate of the **T**op-**L**eft corner of the bounding box (in pixels). |\n",
    "| **`box_xbr`** | `368.87` | The **X**-coordinate of the **B**ottom-**R**ight corner of the bounding box (in pixels). |\n",
    "| **`box_ybr`** | `337.69` | The **Y**-coordinate of the **B**ottom-**R**ight corner of the bounding box (in pixels). |\n",
    "| **`issue`** | `low quality`, `rotated` | An optional flag noting any potential quality issues or specific attributes of the annotation or image, such as poor lighting, obstruction, or weather conditions. |\n",
    "| **`code`** | `33/1993` | The specific identification codes visible on the hazmat plate. |\n",
    "\n",
    "**Key Concepts and Usage**\n",
    "\n",
    "**Bounding Box Coordinate System**\n",
    "\n",
    "The bounding box coordinates are provided in an **absolute pixel format**, defining the box by its top-left `(box_xtl, box_ytl)` and bottom-right `(box_xbr, box_ybr)` corners. This format is common but must be converted to a normalized, center-based format (like `x_center`, `y_center`, `width`, `height`) for use with object detection frameworks like YOLO.\n",
    "\n",
    "**The 'code' Field Explained**\n",
    "\n",
    "This field is a critical part of the dataset, capturing the text on the hazmat plate. The format `XX/YYYY` corresponds to the European ADR agreement for transporting dangerous goods:\n",
    "*   **Top Number (Hazard Identification Number):** This two or three-digit code indicates the primary and subsidiary dangers of the substance (e.g., `33` signifies a highly flammable liquid).\n",
    "*   **Bottom Number (UN Number):** This four-digit code is a globally recognized number identifying the specific hazardous substance being transported (e.g., `1203` is gasoline/petrol).\n",
    "\n",
    "**Purpose and Application**\n",
    "\n",
    "This master CSV file is the **single source of truth** for the `haztruck_dataset`. It is designed to be processed by scripts to:\n",
    "1.  **Generate Labels:** Automatically create label files in formats required by machine learning frameworks (e.g., YOLO `.txt` files or Pascal VOC `.xml` files).\n",
    "2.  **Data Analysis:** Perform detailed analysis on the dataset, such as calculating class distribution, analyzing image sources, or filtering by quality issues noted in the `issue` column.\n",
    "3.  **Auditing and Verification:** Provide full traceability for each annotation, linking back to the original source page and the person who added it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" The rows and column size are {df_haztruck.shape} \\n\")\n",
    "print(f\" The describe of the dataframe is: \\n {df_haztruck.describe(include='all')} \\n\")\n",
    "print(f\" The info of the dataframe is: \\n {df_haztruck.info()} \\n\")\n",
    "print(f\" The columns of the dataframe are: \\n {df_haztruck.columns} \\n\")\n",
    "print(f\" The number of null values in each column are: \\n {df_haztruck.isnull().sum()} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d672f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting style for better aesthetics\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "total_annotations = len(df_haztruck)\n",
    "unique_images = df_haztruck['image_id'].nunique()\n",
    "\n",
    "print(f\"Total number of annotations (bounding boxes): {total_annotations:,}\")\n",
    "print(f\"Total number of unique images: {unique_images:,}\")\n",
    "if unique_images > 0:\n",
    "    print(f\"Average annotations per image: {total_annotations / unique_images:.2f}\")\n",
    "\n",
    "print(\"\\n--- Annotations by Website ---\")\n",
    "website_counts = df_haztruck['website'].value_counts()\n",
    "print(website_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=website_counts.index, y=website_counts.values, palette='plasma')\n",
    "plt.title('Number of Annotations Sourced from Each Website')\n",
    "plt.xlabel('Website')\n",
    "plt.ylabel('Number of Annotations')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Annotations by Author (Top 10) ---\")\n",
    "author_counts = df_haztruck['author'].value_counts()\n",
    "top_10_authors = author_counts.head(10)\n",
    "print(top_10_authors)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=top_10_authors.values, y=top_10_authors.index, palette='magma', orient='h')\n",
    "plt.title('Top 10 Authors by Number of Annotations Contributed')\n",
    "plt.xlabel('Number of Annotations')\n",
    "plt.ylabel('Author')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fill missing values in 'issue' column to mean 'No Issue' for analysis\n",
    "issue_counts = df_haztruck['issue'].fillna('No Issue').value_counts()\n",
    "print(\"Breakdown of annotations by issue:\")\n",
    "print(issue_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=issue_counts.index, y=issue_counts.values, palette='coolwarm')\n",
    "plt.title('Distribution of Annotation Quality Issues')\n",
    "plt.xlabel('Issue Type')\n",
    "plt.ylabel('Number of Annotations')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_annotations_from_links(df_haztruck, num_samples=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
