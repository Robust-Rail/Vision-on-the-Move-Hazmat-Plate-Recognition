{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s3549852/.conda/envs/dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing libraries...\")\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from PIL import Image\n",
    "import contextlib\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1583115/3664114777.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "# Define the dataset class\n",
    "class HazmatDataset(Dataset):\n",
    "    def __init__(self, data_dir, annotations_file, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Load annotations\n",
    "        with open(annotations_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.images = {img['id']: img for img in data['images']}\n",
    "        self.annotations = data['annotations']\n",
    "        \n",
    "        # Create image_id to annotations mapping\n",
    "        self.img_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_to_anns:\n",
    "                self.img_to_anns[img_id] = []\n",
    "            self.img_to_anns[img_id].append(ann)\n",
    "        \n",
    "        self.ids = list(self.images.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.data_dir, 'images', img_info['file_name'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Get annotations\n",
    "        anns = self.img_to_anns.get(img_id, [])\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']\n",
    "            # Convert [x, y, w, h] to [x1, y1, x2, y2]\n",
    "            boxes.append([\n",
    "                bbox[0],\n",
    "                bbox[1],\n",
    "                bbox[0] + bbox[2],\n",
    "                bbox[1] + bbox[3]\n",
    "            ])\n",
    "            labels.append(ann['category_id'])\n",
    "            areas.append(ann['area'])\n",
    "            iscrowd.append(ann['iscrowd'])\n",
    "        \n",
    "        # Convert to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': areas,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            for transform in self.transforms:\n",
    "                img, target = transform(img, target)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        # Convert PIL image to tensor\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = F.hflip(image)\n",
    "            # Flip bounding boxes\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]  # Flip x-coordinates\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # Convert PIL image to tensor\n",
    "    transforms.append(ToTensor())\n",
    "    if train:\n",
    "        # Add training augmentations here if needed\n",
    "        transforms.append(RandomHorizontalFlip(0.5))\n",
    "    return transforms\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_classifier_loss = 0\n",
    "    total_box_reg_loss = 0\n",
    "    total_objectness_loss = 0\n",
    "    total_rpn_box_reg_loss = 0\n",
    "\n",
    "    # Voeg tqdm toe om de voortgang te tonen\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=True)\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Wrap the forward pass in autocast\n",
    "        with autocast():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Scale the loss and call backward\n",
    "        scaler.scale(losses).backward()\n",
    "        # Unscales the gradients and calls or skips optimizer.step()\n",
    "        scaler.step(optimizer)\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        # Bereken de totalen\n",
    "        total_loss += losses.item()\n",
    "        total_classifier_loss += loss_dict['loss_classifier'].item()\n",
    "        total_box_reg_loss += loss_dict['loss_box_reg'].item()\n",
    "        total_objectness_loss += loss_dict['loss_objectness'].item()\n",
    "        total_rpn_box_reg_loss += loss_dict['loss_rpn_box_reg'].item()\n",
    "\n",
    "        # Update tqdm-balk\n",
    "        progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{losses.item():.4f}\",\n",
    "            \"Classifier\": f\"{loss_dict['loss_classifier'].item():.4f}\",\n",
    "            \"BoxReg\": f\"{loss_dict['loss_box_reg'].item():.4f}\",\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_classifier_loss = total_classifier_loss / len(data_loader)\n",
    "    avg_box_reg_loss = total_box_reg_loss / len(data_loader)\n",
    "    avg_objectness_loss = total_objectness_loss / len(data_loader)\n",
    "    avg_rpn_box_reg_loss = total_rpn_box_reg_loss / len(data_loader)\n",
    "\n",
    "    return avg_loss, avg_classifier_loss, avg_box_reg_loss, avg_objectness_loss, avg_rpn_box_reg_loss\n",
    "\n",
    "\n",
    "\n",
    "# Load ground truth annotations\n",
    "coco_val = COCO('data/data_faster_rcnn/val/annotations/instances_val.json')\n",
    "\n",
    "# Prepare predictions in COCO format\n",
    "# Assuming you have a function to convert model outputs to COCO format\n",
    "# Conversion to COCO Format\n",
    "def convert_to_coco_format(outputs, image_ids):\n",
    "    coco_results = []\n",
    "    for output, image_id in zip(outputs, image_ids):\n",
    "        boxes = output['boxes'].cpu().numpy()\n",
    "        scores = output['scores'].cpu().numpy()\n",
    "        labels = output['labels'].cpu().numpy()\n",
    "        \n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            coco_results.append({\n",
    "                'image_id': image_id,\n",
    "                'category_id': int(label),\n",
    "                'bbox': [box[0], box[1], box[2] - box[0], box[3] - box[1]],\n",
    "                'score': float(score)\n",
    "            })\n",
    "    return coco_results\n",
    "\n",
    "# Validation Function\n",
    "def validate(model, data_loader, coco_gt, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    # Add tqdm\n",
    "    progress_bar = tqdm(data_loader, desc=\"Validation\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in progress_bar:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            image_ids = [target['image_id'].item() for target in targets]\n",
    "            coco_results = convert_to_coco_format(outputs, image_ids)\n",
    "            results.extend(coco_results)\n",
    "\n",
    "            # Update tqdm-bar\n",
    "            progress_bar.set_postfix({\"Processed\": len(results)})\n",
    "\n",
    "    if not results:\n",
    "        print(\"No predictions generated. Skipping evaluation.\")\n",
    "        return [0.0] * 6  # Return dummy metrics for empty results\n",
    "\n",
    "    # Suppress COCOeval output\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        coco_dt = coco_gt.loadRes(results)\n",
    "        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "    return coco_eval.stats\n",
    "\n",
    "\n",
    "# Custom backbone to return a dictionary of feature maps\n",
    "class BackboneWithChannels(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return {'0': x}\n",
    "    \n",
    "# Function to create a subset of the dataset\n",
    "def create_subset(dataset, percentage):\n",
    "    \"\"\"\n",
    "    Create a subset of the dataset based on the given percentage.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: The full dataset.\n",
    "    - percentage: The fraction of the dataset to use (value between 0.0 and 1.0).\n",
    "    \n",
    "    Returns:\n",
    "    - subset: A subset of the dataset containing the specified percentage of data.\n",
    "    \"\"\"\n",
    "    if not (0.0 < percentage <= 1.0):\n",
    "        raise ValueError(\"Percentage must be between 0.0 and 1.0.\")\n",
    "    \n",
    "    # Determine the subset size\n",
    "    total_samples = len(dataset)\n",
    "    subset_size = int(total_samples * percentage)\n",
    "    \n",
    "    # Shuffle and select a random subset of indices\n",
    "    indices = list(range(total_samples))\n",
    "    random.shuffle(indices)\n",
    "    subset_indices = indices[:subset_size]\n",
    "    \n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "def create_directory(base_path=\"data/models\"):\n",
    "    \"\"\"\n",
    "    Create a directory inside the base path named 'faster-rcnn-finetuned-{date}' \n",
    "    to store models and logs. The name includes the current date and time in the format 'DD-MM-YYYY HH:MM:SS'.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Base directory where the new directory will be created.\n",
    "\n",
    "    Returns:\n",
    "    - directory_path (str): Full path to the created directory.\n",
    "    \"\"\"\n",
    "    # Get the current date and time\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "    \n",
    "    # Define the full directory path\n",
    "    directory_name = f\"faster-rcnn-finetuned-{current_time}\"\n",
    "    directory_path = os.path.join(base_path, directory_name)\n",
    "    \n",
    "    # Create the directory\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Directory created: {directory_path}\")\n",
    "    return directory_path\n",
    "\n",
    "def train_model(directory, model, optimizer, train_loader, device, train_metrics_list, best_val_map, lr_scheduler, val_loader, coco_val, scaler, epoch):\n",
    "    \n",
    "    epoch+=1\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_classifier_loss, train_box_reg_loss, train_objectness_loss, train_rpn_box_reg_loss = train_one_epoch(\n",
    "        model, optimizer, train_loader, device, scaler)\n",
    "    \n",
    "    # Validate and get all COCO-metrics\n",
    "    val_metrics = validate(model, val_loader, coco_val, device)\n",
    "    val_map = val_metrics[0]  # mAP@IoU=0.50:0.95\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    \n",
    "    # Obtain the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Prepare data for logging\n",
    "    data = {\n",
    "        \"epoch\": epoch,\n",
    "        \"time_elapsed\": (int(minutes), int(seconds)),\n",
    "        \"learning_rate\": current_lr,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"classifier_loss\": train_classifier_loss,\n",
    "        \"box_reg_loss\": train_box_reg_loss,\n",
    "        \"objectness_loss\": train_objectness_loss,\n",
    "        \"rpn_box_reg_loss\": train_rpn_box_reg_loss,\n",
    "        \"val_metrics\": val_metrics\n",
    "    }\n",
    "    \n",
    "    # Append current epoch data to metrics list\n",
    "    train_metrics_list.append(data)\n",
    "    \n",
    "    # Print summary for this epoch\n",
    "    print(f\"📊 Epoch {epoch} | ⏳ Time: {int(minutes)}m {int(seconds)}s | 🔄 LR: {current_lr:.6f}\")\n",
    "    print(f\"📉 Train Loss: {train_loss:.4f} | 🎯 Classifier: {train_classifier_loss:.4f} | 📦 Box Reg: {train_box_reg_loss:.4f}\")\n",
    "    print(f\"🔍 Objectness: {train_objectness_loss:.4f} | 🗂️ RPN Box Reg: {train_rpn_box_reg_loss:.4f}\")\n",
    "    print(f\"🧪 mAP | 🟢 mAP@IoU=0.50:0.95: {val_metrics[0]:.4f} | 🔵 mAP@IoU=0.50: {val_metrics[1]:.4f} | 🟣 mAP@IoU=0.75: {val_metrics[2]:.4f}\")\n",
    "    print(f\"📏 Small mAP: {val_metrics[3]:.4f} | 📐 Medium mAP: {val_metrics[4]:.4f} | 📏 Large mAP: {val_metrics[5]:.4f}\")\n",
    "    \n",
    "    # Save epoch data to a log file\n",
    "    save_epoch_data(directory, data)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save the latest checkpoint with all metrics\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_map': val_map,\n",
    "        'train_metrics_list': train_metrics_list  # Save all metrics\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(directory, \"latest_model.pth\"))\n",
    "    \n",
    "    # Save the best model if the val_map is the highest so far\n",
    "    if val_map > best_val_map:\n",
    "        best_val_map = val_map\n",
    "        torch.save(checkpoint, os.path.join(directory, \"best_model.pth\"))\n",
    "    \n",
    "    return best_val_map\n",
    "        \n",
    "\n",
    "\n",
    "def save_epoch_data(directory, data):\n",
    "    \"\"\"\n",
    "    Save training statistics for each epoch in a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): Path to the directory.\n",
    "    - data (dict): Contains data on metrics such as epoch, losses, and validation metrics.\n",
    "    \"\"\"\n",
    "    log_file_path = os.path.join(directory, \"training_log.txt\")\n",
    "    \n",
    "    with open(log_file_path, \"a\") as log_file:\n",
    "        log_file.write(f\"📊 Epoch {data['epoch']} | ⏳ Time: {data['time_elapsed'][0]}m {data['time_elapsed'][1]}s | 🔄 LR: {data['learning_rate']:.6f}\\n\")\n",
    "        log_file.write(f\"📉 Train Loss: {data['train_loss']:.4f} | 🎯 Classifier: {data['classifier_loss']:.4f} | 📦 Box Reg: {data['box_reg_loss']:.4f}\\n\")\n",
    "        log_file.write(f\"🔍 Objectness: {data['objectness_loss']:.4f} | 🗂️ RPN Box Reg: {data['rpn_box_reg_loss']:.4f}\\n\")\n",
    "        log_file.write(f\"🧪 Validation Metrics | 🟢 mAP@IoU=0.50:0.95: {data['val_metrics'][0]:.4f} | 🔵 mAP@IoU=0.50: {data['val_metrics'][1]:.4f} | 🟣 mAP@IoU=0.75: {data['val_metrics'][2]:.4f}\\n\")\n",
    "        log_file.write(f\"📏 Small mAP: {data['val_metrics'][3]:.4f} | 📐 Medium mAP: {data['val_metrics'][4]:.4f} | 📏 Large mAP: {data['val_metrics'][5]:.4f}\\n\")\n",
    "        log_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s3549852/.conda/envs/dev/lib/python3.12/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/s3549852/.conda/envs/dev/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/s3549852/.conda/envs/dev/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (6): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (7): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (8): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (9): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (10): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (11): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (12): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (13): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (14): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (15): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (16): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (17): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (18): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (19): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (20): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (21): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (22): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3', '4'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "device = torch.device('cuda:0')\n",
    "print(f\"Training model on {device}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/train',\n",
    "    annotations_file='data/data_faster_rcnn/train/annotations/instances_train.json',\n",
    "    transforms=get_transform(train=True)\n",
    ")\n",
    "\n",
    "val_dataset = HazmatDataset(\n",
    "    data_dir='data/data_faster_rcnn/val',\n",
    "    annotations_file='data/data_faster_rcnn/val/annotations/instances_val.json',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "# Set the percentage of the training dataset to use (e.g. 0.x to 1)\n",
    "train_percentage = 1\n",
    "\n",
    "# Create a subset of the training dataset\n",
    "train_dataset_subset = create_subset(train_dataset, train_percentage)\n",
    "\n",
    "# Set the percentage of the val dataset to use (e.g. 0.x to 1)\n",
    "val_percentage = 1\n",
    "\n",
    "# Create a subset of the training dataset\n",
    "val_dataset_subset = create_subset(val_dataset, val_percentage)\n",
    "\n",
    "# amount of cpu cores\n",
    "workers = 2\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_subset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_subset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 2  # hazmat code and background\n",
    "\n",
    "# Create ResNet-101 backbone with FPN\n",
    "backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
    "\n",
    "# Define anchor generator for FPN\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32,), (64,), (128,), (256,), (512,)),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    ")\n",
    "\n",
    "# Multi-scale RoI pooling for FPN\n",
    "roi_pooler = MultiScaleRoIAlign(\n",
    "    featmap_names=['0', '1', '2', '3', '4'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2\n",
    ")\n",
    "\n",
    "print(\"initializing model...\")\n",
    "# Initialize Faster R-CNN with ResNet-101-FPN\n",
    "model = FasterRCNN(\n",
    "    backbone=backbone,\n",
    "    num_classes=num_classes,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 14 11:38:58 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.76                 Driver Version: 550.76         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:17:00.0 Off |                    0 |\n",
      "|  0%   69C    P0            229W /  300W |    9121MiB /  46068MiB |     43%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A40                     On  |   00000000:65:00.0 Off |                    0 |\n",
      "|  0%   34C    P8             28W /  300W |       4MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A40                     On  |   00000000:CA:00.0 Off |                    0 |\n",
      "|  0%   29C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A40                     On  |   00000000:E3:00.0 Off |                    0 |\n",
      "|  0%   29C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1582103      C   ...unners/cuda_v11/ollama_llama_server       8602MiB |\n",
      "|    0   N/A  N/A   1583115      C   ...s3549852/.conda/envs/dev/bin/python        504MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "Number of GPUs available: 2\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def show_gpu_usage():\n",
    "    result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
    "    print(result.stdout.decode())\n",
    "\n",
    "show_gpu_usage()\n",
    "# Check how many GPUs are available\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 7710"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Directory created: data/models/faster-rcnn-finetuned-14-01-2025 11:39:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/478 [00:00<?, ?it/s]/tmp/ipykernel_1583115/3664114777.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████| 478/478 [22:29<00:00,  2.82s/it, Loss=0.1613, Classifier=0.0725, BoxReg=0.0752]\n",
      "Validation: 100%|██████████| 90/90 [03:36<00:00,  2.41s/it, Processed=9739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 | ⏳ Time: 26m 7s | 🔄 LR: 0.005000\n",
      "📉 Train Loss: 0.1701 | 🎯 Classifier: 0.0712 | 📦 Box Reg: 0.0666\n",
      "🔍 Objectness: 0.0289 | 🗂️ RPN Box Reg: 0.0033\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.3037 | 🔵 mAP@IoU=0.50: 0.5762 | 🟣 mAP@IoU=0.75: 0.2738\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.2000 | 📏 Large mAP: 0.3067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [20:51<00:00,  2.62s/it, Loss=0.1493, Classifier=0.0640, BoxReg=0.0756]\n",
      "Validation: 100%|██████████| 90/90 [03:22<00:00,  2.25s/it, Processed=15272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 2 | ⏳ Time: 24m 15s | 🔄 LR: 0.005000\n",
      "📉 Train Loss: 0.1415 | 🎯 Classifier: 0.0610 | 📦 Box Reg: 0.0684\n",
      "🔍 Objectness: 0.0097 | 🗂️ RPN Box Reg: 0.0023\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.3513 | 🔵 mAP@IoU=0.50: 0.6884 | 🟣 mAP@IoU=0.75: 0.3182\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.2005 | 📏 Large mAP: 0.3552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [21:12<00:00,  2.66s/it, Loss=0.1442, Classifier=0.0575, BoxReg=0.0809]\n",
      "Validation: 100%|██████████| 90/90 [03:37<00:00,  2.41s/it, Processed=17019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 3 | ⏳ Time: 24m 50s | 🔄 LR: 0.005000\n",
      "📉 Train Loss: 0.1292 | 🎯 Classifier: 0.0539 | 📦 Box Reg: 0.0670\n",
      "🔍 Objectness: 0.0064 | 🗂️ RPN Box Reg: 0.0019\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.4021 | 🔵 mAP@IoU=0.50: 0.7462 | 🟣 mAP@IoU=0.75: 0.3658\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.3755 | 📏 Large mAP: 0.4059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [21:21<00:00,  2.68s/it, Loss=0.0915, Classifier=0.0352, BoxReg=0.0528]\n",
      "Validation: 100%|██████████| 90/90 [03:36<00:00,  2.40s/it, Processed=8915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 4 | ⏳ Time: 24m 58s | 🔄 LR: 0.000500\n",
      "📉 Train Loss: 0.1083 | 🎯 Classifier: 0.0439 | 📦 Box Reg: 0.0594\n",
      "🔍 Objectness: 0.0036 | 🗂️ RPN Box Reg: 0.0014\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.4780 | 🔵 mAP@IoU=0.50: 0.8247 | 🟣 mAP@IoU=0.75: 0.4855\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.3900 | 📏 Large mAP: 0.4811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [21:07<00:00,  2.65s/it, Loss=0.1066, Classifier=0.0419, BoxReg=0.0607]\n",
      "Validation: 100%|██████████| 90/90 [03:25<00:00,  2.29s/it, Processed=8067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 5 | ⏳ Time: 24m 34s | 🔄 LR: 0.000500\n",
      "📉 Train Loss: 0.1030 | 🎯 Classifier: 0.0415 | 📦 Box Reg: 0.0573\n",
      "🔍 Objectness: 0.0029 | 🗂️ RPN Box Reg: 0.0014\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.4820 | 🔵 mAP@IoU=0.50: 0.8344 | 🟣 mAP@IoU=0.75: 0.4657\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.4014 | 📏 Large mAP: 0.4846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [21:16<00:00,  2.67s/it, Loss=0.1051, Classifier=0.0476, BoxReg=0.0525]\n",
      "Validation: 100%|██████████| 90/90 [03:44<00:00,  2.50s/it, Processed=9145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 6 | ⏳ Time: 25m 2s | 🔄 LR: 0.000500\n",
      "📉 Train Loss: 0.0998 | 🎯 Classifier: 0.0397 | 📦 Box Reg: 0.0563\n",
      "🔍 Objectness: 0.0025 | 🗂️ RPN Box Reg: 0.0013\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.4998 | 🔵 mAP@IoU=0.50: 0.8485 | 🟣 mAP@IoU=0.75: 0.4968\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.4315 | 📏 Large mAP: 0.5021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [21:56<00:00,  2.75s/it, Loss=0.1143, Classifier=0.0546, BoxReg=0.0548]\n",
      "Validation: 100%|██████████| 90/90 [03:40<00:00,  2.45s/it, Processed=6801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 7 | ⏳ Time: 25m 37s | 🔄 LR: 0.000050\n",
      "📉 Train Loss: 0.0960 | 🎯 Classifier: 0.0379 | 📦 Box Reg: 0.0547\n",
      "🔍 Objectness: 0.0021 | 🗂️ RPN Box Reg: 0.0013\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.4996 | 🔵 mAP@IoU=0.50: 0.8495 | 🟣 mAP@IoU=0.75: 0.4859\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.4175 | 📏 Large mAP: 0.5024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [21:10<00:00,  2.66s/it, Loss=0.1144, Classifier=0.0450, BoxReg=0.0655]\n",
      "Validation: 100%|██████████| 90/90 [03:28<00:00,  2.32s/it, Processed=6657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 8 | ⏳ Time: 24m 39s | 🔄 LR: 0.000050\n",
      "📉 Train Loss: 0.0952 | 🎯 Classifier: 0.0375 | 📦 Box Reg: 0.0543\n",
      "🔍 Objectness: 0.0021 | 🗂️ RPN Box Reg: 0.0013\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.5073 | 🔵 mAP@IoU=0.50: 0.8516 | 🟣 mAP@IoU=0.75: 0.5120\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.4324 | 📏 Large mAP: 0.5098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [20:56<00:00,  2.63s/it, Loss=0.0735, Classifier=0.0291, BoxReg=0.0428]\n",
      "Training: 100%|██████████| 478/478 [20:43<00:00,  2.60s/it, Loss=0.0807, Classifier=0.0325, BoxReg=0.0465]\n",
      "Validation: 100%|██████████| 90/90 [03:24<00:00,  2.27s/it, Processed=6524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 10 | ⏳ Time: 24m 8s | 🔄 LR: 0.000005\n",
      "📉 Train Loss: 0.0946 | 🎯 Classifier: 0.0370 | 📦 Box Reg: 0.0543\n",
      "🔍 Objectness: 0.0020 | 🗂️ RPN Box Reg: 0.0012\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.5076 | 🔵 mAP@IoU=0.50: 0.8534 | 🟣 mAP@IoU=0.75: 0.5125\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.4306 | 📏 Large mAP: 0.5111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [21:08<00:00,  2.65s/it, Loss=0.0829, Classifier=0.0314, BoxReg=0.0492]\n",
      "Validation: 100%|██████████| 90/90 [03:24<00:00,  2.28s/it, Processed=6490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 11 | ⏳ Time: 24m 33s | 🔄 LR: 0.000005\n",
      "📉 Train Loss: 0.0945 | 🎯 Classifier: 0.0371 | 📦 Box Reg: 0.0542\n",
      "🔍 Objectness: 0.0020 | 🗂️ RPN Box Reg: 0.0012\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.5081 | 🔵 mAP@IoU=0.50: 0.8528 | 🟣 mAP@IoU=0.75: 0.5133\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.4304 | 📏 Large mAP: 0.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [20:44<00:00,  2.60s/it, Loss=0.0754, Classifier=0.0268, BoxReg=0.0460]\n",
      "Validation: 100%|██████████| 90/90 [03:28<00:00,  2.32s/it, Processed=6461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 12 | ⏳ Time: 24m 13s | 🔄 LR: 0.000005\n",
      "📉 Train Loss: 0.0943 | 🎯 Classifier: 0.0369 | 📦 Box Reg: 0.0541\n",
      "🔍 Objectness: 0.0020 | 🗂️ RPN Box Reg: 0.0012\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.5079 | 🔵 mAP@IoU=0.50: 0.8529 | 🟣 mAP@IoU=0.75: 0.5146\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.4319 | 📏 Large mAP: 0.5111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [20:53<00:00,  2.62s/it, Loss=0.1239, Classifier=0.0577, BoxReg=0.0622]\n",
      "Validation: 100%|██████████| 90/90 [03:28<00:00,  2.31s/it, Processed=6454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 13 | ⏳ Time: 24m 22s | 🔄 LR: 0.000001\n",
      "📉 Train Loss: 0.0942 | 🎯 Classifier: 0.0368 | 📦 Box Reg: 0.0542\n",
      "🔍 Objectness: 0.0020 | 🗂️ RPN Box Reg: 0.0012\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.5081 | 🔵 mAP@IoU=0.50: 0.8533 | 🟣 mAP@IoU=0.75: 0.5138\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.4319 | 📏 Large mAP: 0.5113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [20:34<00:00,  2.58s/it, Loss=0.0970, Classifier=0.0369, BoxReg=0.0575]\n",
      "Validation: 100%|██████████| 90/90 [03:26<00:00,  2.30s/it, Processed=6448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 14 | ⏳ Time: 24m 1s | 🔄 LR: 0.000001\n",
      "📉 Train Loss: 0.0945 | 🎯 Classifier: 0.0369 | 📦 Box Reg: 0.0543\n",
      "🔍 Objectness: 0.0020 | 🗂️ RPN Box Reg: 0.0012\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.5082 | 🔵 mAP@IoU=0.50: 0.8533 | 🟣 mAP@IoU=0.75: 0.5128\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.4319 | 📏 Large mAP: 0.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 478/478 [20:39<00:00,  2.59s/it, Loss=0.0760, Classifier=0.0314, BoxReg=0.0425]\n",
      "Validation: 100%|██████████| 90/90 [03:24<00:00,  2.27s/it, Processed=6441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 15 | ⏳ Time: 24m 3s | 🔄 LR: 0.000001\n",
      "📉 Train Loss: 0.0938 | 🎯 Classifier: 0.0367 | 📦 Box Reg: 0.0538\n",
      "🔍 Objectness: 0.0020 | 🗂️ RPN Box Reg: 0.0012\n",
      "🧪 mAP | 🟢 mAP@IoU=0.50:0.95: 0.5083 | 🔵 mAP@IoU=0.50: 0.8533 | 🟣 mAP@IoU=0.75: 0.5143\n",
      "📏 Small mAP: -1.0000 | 📐 Medium mAP: 0.4319 | 📏 Large mAP: 0.5120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 395/478 [17:14<03:35,  2.59s/it, Loss=0.0981, Classifier=0.0400, BoxReg=0.0551]"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 23\n",
    "train_metrics_map = []\n",
    "best_val_map = float('-inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Create directory to store models and logs\n",
    "directory_finetuned_model = create_directory()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    best_val_map = train_model(\n",
    "        directory=directory_finetuned_model, \n",
    "        model=model, optimizer=optimizer, train_loader=train_loader, device=device, \n",
    "        train_metrics_list=train_metrics_map, best_val_map=best_val_map, lr_scheduler=lr_scheduler, \n",
    "        val_loader=val_loader, coco_val=coco_val, scaler=scaler, epoch=epoch\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "device = torch.device('cuda')\n",
    "model_path = os.path.join(directory_finetuned_model, 'best_model.pth')\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "val_map = checkpoint['val_map']\n",
    "epoch = checkpoint['epoch']\n",
    "#latest\n",
    "latest_model_path = os.path.join(directory_finetuned_model, 'latest_model.pth')\n",
    "checkpoint_latest = torch.load(latest_model_path, map_location=device)\n",
    "val_map_latest = checkpoint_latest['val_map']\n",
    "epoch_latest = checkpoint_latest['epoch']\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(f\"Validation mAP best model: {val_map:.4f}\")\n",
    "print(f\"Epoch best model: {epoch}\")\n",
    "\n",
    "print(f\"Validation mAP latest model: {val_map_latest:.4f}\")\n",
    "print(f\"Epoch latest model: {epoch_latest}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(checkpoint_path, title=\"Training and Validation Metrics over Epochs\"):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics from a given model checkpoint.\n",
    "    \n",
    "    Parameters:\n",
    "    - checkpoint_path (str): Path to the model checkpoint file (e.g., 'latest_model.pth').\n",
    "    - title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    train_metrics_list = checkpoint['train_metrics_list']\n",
    "    \n",
    "    # Extract metrics per epoch\n",
    "    epochs = [data['epoch'] for data in train_metrics_list]\n",
    "    train_loss_list = [data['train_loss'] for data in train_metrics_list]\n",
    "    classifier_loss_list = [data['classifier_loss'] for data in train_metrics_list]\n",
    "    box_reg_loss_list = [data['box_reg_loss'] for data in train_metrics_list]\n",
    "    objectness_loss_list = [data['objectness_loss'] for data in train_metrics_list]\n",
    "    rpn_box_reg_loss_list = [data['rpn_box_reg_loss'] for data in train_metrics_list]\n",
    "\n",
    "    # Extract validation mAP metrics\n",
    "    val_map_list = [data['val_metrics'][0] for data in train_metrics_list]  # mAP@IoU=0.50:0.95\n",
    "    val_map_50_list = [data['val_metrics'][1] for data in train_metrics_list]  # mAP@IoU=0.50\n",
    "    val_map_75_list = [data['val_metrics'][2] for data in train_metrics_list]  # mAP@IoU=0.75\n",
    "    val_map_small_list = [data['val_metrics'][3] for data in train_metrics_list]  # Small mAP\n",
    "    val_map_medium_list = [data['val_metrics'][4] for data in train_metrics_list]  # Medium mAP\n",
    "    val_map_large_list = [data['val_metrics'][5] for data in train_metrics_list]  # Large mAP\n",
    "\n",
    "    # Initialize the plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    # Plot training losses\n",
    "    plt.plot(epochs, train_loss_list, label='Training Loss', marker='o')\n",
    "    #     plt.plot(epochs, classifier_loss_list, label='Classifier Loss', marker='o')\n",
    "    #     plt.plot(epochs, box_reg_loss_list, label='Box Regression Loss', marker='o')\n",
    "    #     plt.plot(epochs, objectness_loss_list, label='Objectness Loss', marker='o')\n",
    "    #     plt.plot(epochs, rpn_box_reg_loss_list, label='RPN Box Regression Loss', marker='o')\n",
    "\n",
    "    # Plot validation mAP metrics\n",
    "    plt.plot(epochs, val_map_list, label='Validation mAP (IoU=0.50:0.95)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_50_list, label='Validation mAP (IoU=0.50)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_75_list, label='Validation mAP (IoU=0.75)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_small_list, label='Validation mAP (Small)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_medium_list, label='Validation mAP (Medium)', linestyle='--', marker='x')\n",
    "    plt.plot(epochs, val_map_large_list, label='Validation mAP (Large)', linestyle='--', marker='x')\n",
    "\n",
    "    # Set x-axis ticks to start from 1\n",
    "    plt.xticks(range(1, len(epochs) + 1))\n",
    "\n",
    "    # Set plot details\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest model checkpoint\n",
    "latest_model_path = os.path.join(directory_finetuned_model, 'latest_model.pth')\n",
    "plot_metrics(latest_model_path, \"Training and validation over epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transforms=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transforms:\n",
    "        for transform in transforms:\n",
    "            image, _ = transform(image, target=None)  # No target during inference\n",
    "    return image\n",
    "\n",
    "# Define preprocessing transforms\n",
    "test_transforms = get_transform(train=False)\n",
    "\n",
    "# Load the image\n",
    "image_path = 'images/hazard_plate.jpg'  # Replace with your image path\n",
    "image = load_image(image_path, transforms=test_transforms)\n",
    "image = image.to(device)\n",
    "# Wrap the image in a list as the model expects a batch\n",
    "with torch.no_grad():\n",
    "    predictions = model([image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_predictions(image, predictions, threshold=0.5, classes=['background', 'hazmat']):\n",
    "    # Convert image from tensor to numpy array\n",
    "    image = image.cpu().permute(1, 2, 0).numpy()\n",
    "    image = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Filter predictions based on confidence threshold\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if label == 1:  # Only plot hazmat codes\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color='blue', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            label_name = classes[label]\n",
    "            ax.text(x1, y1, f'{label_name}: {score:.2f}', color='white', backgroundcolor='blue', fontsize=12)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, threshold=0.5):\n",
    "    # List of class names\n",
    "    classes = ['background', 'hazmat']\n",
    "    \n",
    "    # Load the image\n",
    "    image = load_image(image_path, transforms=test_transforms)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # Wrap the image in a list as the model expects a batch\n",
    "    with torch.no_grad():\n",
    "        predictions = model([image])\n",
    "    \n",
    "    # Filter predictions based on threshold\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Apply threshold filter\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    # Print the predictions\n",
    "    if len(boxes) == 0:\n",
    "        print(\"No predictions meet the threshold.\")\n",
    "    else:\n",
    "        print(\"Predictions:\")\n",
    "        for label, score in zip(labels, scores):\n",
    "            class_name = classes[label]\n",
    "            print(f\"  {class_name}: {score:.2f}\")\n",
    "        # Display the predictions\n",
    "        draw_predictions(image, predictions, threshold=threshold, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image('data/data_faster_rcnn/val/images/1690281365_00595.jpg', threshold=0.29)\n",
    "predict_image('images/hazard_plate.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/1.webp', threshold=0)\n",
    "predict_image('images/un_numbers_test/2.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/3.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/4.jpg', threshold=0)\n",
    "predict_image('images/un_numbers_test/6.webp', threshold=0)\n",
    "predict_image('images/un_numbers_test/7.jpg', threshold=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop trhough all images from test and predict\n",
    "\n",
    "test_images_path = \"data/data_faster_rcnn/test/images\"\n",
    "\n",
    "# frames available\n",
    "test_images_path_list = os.listdir(test_images_path)\n",
    "import random\n",
    "random.shuffle(test_images_path_list)\n",
    "\n",
    "# Predict on the first 30 images\n",
    "for count, image_name in enumerate(test_images_path_list[:20]):\n",
    "    image_path = os.path.join(test_images_path, image_name)\n",
    "    predict_image(image_path, threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dev)",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
