{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to get the predictions on a photo with the ground truth public dataset faster rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_id(filename):\n",
    "    \"\"\"Extracts the numeric ID from filenames like 'shadow-0-aug_124.jpg'.\"\"\"\n",
    "    match = re.search(r'aug_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Example usage:\n",
    "filenames = [\n",
    "    \"shadow-0-aug_124.jpg\",\n",
    "    \"sun_flare-0-aug_10.jpg\",\n",
    "    \"shadow-0-aug_125.jpg\",\n",
    "    \"sun_flare-0-aug_101.jpg\",\n",
    "    \"fog-0-aug_99.jpg\"\n",
    "]\n",
    "\n",
    "ids = [extract_id(f) for f in filenames]\n",
    "print(ids)  # Output: [124, 10, 125, 101, 99]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importing libraries...\")\n",
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import json\n",
    "from PIL import Image\n",
    "import contextlib\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        # Check image type\n",
    "        if not isinstance(image, (torch.Tensor, Image.Image)):\n",
    "            raise TypeError(f\"Unsupported image type: {type(image)}. Expected torch.Tensor or PIL.Image.\")\n",
    "        \n",
    "        if torch.rand(1) < self.prob:\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                width = image.shape[-1]\n",
    "                image = F.hflip(image)\n",
    "            else:\n",
    "                width, _ = image.size\n",
    "                image = F.hflip(image)\n",
    "            \n",
    "            # Flip bounding boxes\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]  # Flip x-coordinates\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n",
    "\n",
    "class RandomBrightnessCont(object):\n",
    "    def __init__(self, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5):\n",
    "        self.color_jitter = ColorJitter(brightness, contrast, saturation, hue)\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            image = self.color_jitter(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomBlur(object):\n",
    "    def __init__(self, kernel_size=3, p=0.5):\n",
    "        self.blur = GaussianBlur(kernel_size)\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            image = self.blur(image)\n",
    "        return image, target\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "class RandomRotate(object):\n",
    "    def __init__(self, angle_range=10, p=0.5):\n",
    "        self.angle_range = angle_range\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() >= self.p:\n",
    "            return image, target\n",
    "\n",
    "        angle = random.uniform(-self.angle_range, self.angle_range)\n",
    "        original_width, original_height = self._get_image_size(image)\n",
    "\n",
    "        # Rotate image with expansion to get new dimensions\n",
    "        image_pil = image if isinstance(image, Image.Image) else F.to_pil_image(image)\n",
    "        image_pil_rotated = F.rotate(image_pil, angle, expand=True)\n",
    "        new_width, new_height = image_pil_rotated.size\n",
    "\n",
    "        # Convert back to tensor if needed\n",
    "        image = F.to_tensor(image_pil_rotated) if isinstance(image, torch.Tensor) else image_pil_rotated\n",
    "\n",
    "        # Rotate bounding boxes\n",
    "        boxes = target['boxes']\n",
    "        if len(boxes) == 0:\n",
    "            return image, target\n",
    "\n",
    "        # Compute rotation matrix with expansion offset\n",
    "        cx_orig = original_width / 2\n",
    "        cy_orig = original_height / 2\n",
    "\n",
    "        # Calculate expansion offset (min_x, min_y)\n",
    "        corners_original = torch.tensor([\n",
    "            [0, 0],\n",
    "            [original_width, 0],\n",
    "            [original_width, original_height],\n",
    "            [0, original_height]\n",
    "        ])\n",
    "        corners_rotated = self._rotate_points(corners_original, -angle, (cx_orig, cy_orig))\n",
    "        min_x = corners_rotated[:, 0].min()\n",
    "        min_y = corners_rotated[:, 1].min()\n",
    "\n",
    "        # Rotate and translate box corners\n",
    "        boxes_rotated = []\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            corners = torch.tensor([\n",
    "                [x1, y1], [x2, y1], [x2, y2], [x1, y2]\n",
    "            ])\n",
    "            corners_rot = self._rotate_points(corners, -angle, (cx_orig, cy_orig))\n",
    "            corners_rot -= torch.tensor([[min_x, min_y]])  # Adjust for expansion\n",
    "\n",
    "            # Clamp to new image bounds\n",
    "            x_min = max(0.0, corners_rot[:, 0].min().item())\n",
    "            y_min = max(0.0, corners_rot[:, 1].min().item())\n",
    "            x_max = min(new_width, corners_rot[:, 0].max().item())\n",
    "            y_max = min(new_height, corners_rot[:, 1].max().item())\n",
    "\n",
    "            if x_max > x_min and y_max > y_min:\n",
    "                boxes_rotated.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        target['boxes'] = torch.tensor(boxes_rotated, dtype=torch.float32) if boxes_rotated else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        return image, target\n",
    "\n",
    "    def _rotate_points(self, points, angle, center):\n",
    "        angle_rad = math.radians(angle)\n",
    "        cos_theta = math.cos(angle_rad)\n",
    "        sin_theta = math.sin(angle_rad)\n",
    "        cx, cy = center\n",
    "\n",
    "        # Translate points to origin\n",
    "        translated = points - torch.tensor([[cx, cy]])\n",
    "\n",
    "        # Apply rotation\n",
    "        x_rot = translated[:, 0] * cos_theta - translated[:, 1] * sin_theta\n",
    "        y_rot = translated[:, 0] * sin_theta + translated[:, 1] * cos_theta\n",
    "\n",
    "        # Translate back\n",
    "        rotated_points = torch.stack([x_rot + cx, y_rot + cy], dim=1)\n",
    "        return rotated_points\n",
    "\n",
    "    def _get_image_size(self, image):\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            return image.shape[-1], image.shape[-2]\n",
    "        elif isinstance(image, Image.Image):\n",
    "            return image.size\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported image type.\")\n",
    "    \n",
    "class RandomZoom(object):\n",
    "    def __init__(self, zoom_range=(1.0, 2.0), p=0.5):\n",
    "        self.zoom_range = zoom_range\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            boxes = target['boxes']\n",
    "            if len(boxes) == 0:\n",
    "                return image, target\n",
    "\n",
    "            box_idx = random.randint(0, len(boxes) - 1)\n",
    "            x1, y1, x2, y2 = boxes[box_idx].numpy()\n",
    "\n",
    "            width, height = image.size\n",
    "            center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "            box_width, box_height = x2 - x1, y2 - y1\n",
    "\n",
    "            zoom_factor = random.uniform(*self.zoom_range)\n",
    "            crop_width = box_width / zoom_factor\n",
    "            crop_height = box_height / zoom_factor\n",
    "\n",
    "            crop_x1 = max(0, center_x - crop_width / 2)\n",
    "            crop_y1 = max(0, center_y - crop_height / 2)\n",
    "            crop_x2 = min(width, center_x + crop_width / 2)\n",
    "            crop_y2 = min(height, center_y + crop_height / 2)\n",
    "\n",
    "            image = image.crop((int(crop_x1), int(crop_y1), int(crop_x2), int(crop_y2)))\n",
    "            target['boxes'][:, [0, 2]] -= crop_x1\n",
    "            target['boxes'][:, [1, 3]] -= crop_y1\n",
    "            target['boxes'][:, [0, 2]] = target['boxes'][:, [0, 2]].clamp(0, crop_x2 - crop_x1)\n",
    "            target['boxes'][:, [1, 3]] = target['boxes'][:, [1, 3]].clamp(0, crop_y2 - crop_y1)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    \n",
    "def get_augmented_transform(train):\n",
    "    \"\"\"\n",
    "    Get transform pipeline with augmentations for training or validation\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    \n",
    "    if train:\n",
    "        # Applies a series of data augmentations specifically for the training set\n",
    "        transforms.extend([\n",
    "            RandomHorizontalFlip(0.5),  # Horizontally flips the image with a 50% probability\n",
    "            RandomBrightnessCont(  # Adjusts brightness, contrast, saturation, and hue with specified ranges\n",
    "                brightness=0.3, \n",
    "                contrast=0.4, \n",
    "                saturation=0.5, \n",
    "                hue=0.5, \n",
    "                p=0.5  # Applies these adjustments with a 50% probability\n",
    "            ),\n",
    "            RandomBlur(kernel_size=3, p=0.5),  # Applies Gaussian blur with a kernel size of 3, 30% chance\n",
    "            RandomRotate(angle_range=50, p=0.5),  # Rotates the image by -10 to +10 degrees, 30% chance\n",
    "\n",
    "            RandomZoom(zoom_range=(0.05, 0.99), p=1)  # Zooms the image by a factor between 0.05 and 0.9, 60% chance\n",
    "        ])\n",
    "\n",
    "    # Converts the image to a tensor for model input\n",
    "    transforms.append(ToTensor())\n",
    "    \n",
    "    return Compose(transforms)\n",
    "\n",
    "def visualize_augmentations(dataset, num_samples=5):\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 4 * num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "        orig_img, orig_target = dataset[idx]\n",
    "        \n",
    "        if isinstance(orig_img, torch.Tensor):\n",
    "            orig_img_np = orig_img.permute(1, 2, 0).numpy()\n",
    "        else:\n",
    "            orig_img_np = np.array(orig_img)\n",
    "        \n",
    "        axes[i, 0].imshow(orig_img_np)\n",
    "        axes[i, 0].set_title('Original')\n",
    "        \n",
    "        aug_img, aug_target = dataset[idx]\n",
    "        aug_img_np = aug_img.permute(1, 2, 0).numpy() if isinstance(aug_img, torch.Tensor) else np.array(aug_img)\n",
    "        \n",
    "        axes[i, 1].imshow(aug_img_np)\n",
    "        axes[i, 1].set_title('Augmented')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "scaler = GradScaler()\n",
    "class HazmatDataset(Dataset):\n",
    "    def __init__(self, data_dir, annotations_file, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Load annotations\n",
    "        with open(annotations_file) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.images = {img['id']: img for img in data['images']}\n",
    "        self.annotations = data['annotations']\n",
    "\n",
    "        # Create image_id to annotations mapping\n",
    "        self.img_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_to_anns:\n",
    "                self.img_to_anns[img_id] = []\n",
    "            self.img_to_anns[img_id].append(ann)\n",
    "\n",
    "        self.ids = list(self.images.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.data_dir, 'images', img_info['file_name'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Get annotations\n",
    "        anns = self.img_to_anns.get(img_id, [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "\n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']\n",
    "            # Convert [x, y, w, h] to [x1, y1, x2, y2]\n",
    "            x_min = bbox[0]\n",
    "            y_min = bbox[1]\n",
    "            x_max = bbox[0] + bbox[2]\n",
    "            y_max = bbox[1] + bbox[3]\n",
    "\n",
    "            # Filter out degenerate boxes\n",
    "            if x_max > x_min and y_max > y_min:\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(ann['category_id'])\n",
    "                areas.append(ann['area'])\n",
    "                iscrowd.append(ann['iscrowd'])\n",
    "\n",
    "        # Convert to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': areas,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    \n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        # Convert PIL image to tensor\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = F.hflip(image)\n",
    "            # Flip bounding boxes\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]  # Flip x-coordinates\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # Convert PIL image to tensor\n",
    "    transforms.append(ToTensor())\n",
    "    if train:\n",
    "        # Add training augmentations here if needed\n",
    "        transforms.append(RandomHorizontalFlip(0.5))\n",
    "    return transforms\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_classifier_loss = 0\n",
    "    total_box_reg_loss = 0\n",
    "    total_objectness_loss = 0\n",
    "    total_rpn_box_reg_loss = 0\n",
    "\n",
    "    # Voeg tqdm toe om de voortgang te tonen\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=True)\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Wrap the forward pass in autocast\n",
    "        with autocast(device_type='cuda'):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Scale the loss and call backward\n",
    "        scaler.scale(losses).backward()\n",
    "        # Unscales the gradients and calls or skips optimizer.step()\n",
    "        scaler.step(optimizer)\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        # Bereken de totalen\n",
    "        total_loss += losses.item()\n",
    "        total_classifier_loss += loss_dict['loss_classifier'].item()\n",
    "        total_box_reg_loss += loss_dict['loss_box_reg'].item()\n",
    "        total_objectness_loss += loss_dict['loss_objectness'].item()\n",
    "        total_rpn_box_reg_loss += loss_dict['loss_rpn_box_reg'].item()\n",
    "\n",
    "        # Update tqdm-balk\n",
    "        progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{losses.item():.4f}\",\n",
    "            \"Classifier\": f\"{loss_dict['loss_classifier'].item():.4f}\",\n",
    "            \"BoxReg\": f\"{loss_dict['loss_box_reg'].item():.4f}\",\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_classifier_loss = total_classifier_loss / len(data_loader)\n",
    "    avg_box_reg_loss = total_box_reg_loss / len(data_loader)\n",
    "    avg_objectness_loss = total_objectness_loss / len(data_loader)\n",
    "    avg_rpn_box_reg_loss = total_rpn_box_reg_loss / len(data_loader)\n",
    "\n",
    "    return avg_loss, avg_classifier_loss, avg_box_reg_loss, avg_objectness_loss, avg_rpn_box_reg_loss\n",
    "\n",
    "\n",
    "\n",
    "# Load ground truth annotations\n",
    "coco_val = COCO('data/data_faster_rcnn/val/annotations/instances_val.json')\n",
    "\n",
    "# Prepare predictions in COCO format\n",
    "# Assuming you have a function to convert model outputs to COCO format\n",
    "# Conversion to COCO Format\n",
    "def convert_to_coco_format(outputs, image_ids):\n",
    "    coco_results = []\n",
    "    for output, image_id in zip(outputs, image_ids):\n",
    "        boxes = output['boxes'].cpu().numpy()\n",
    "        scores = output['scores'].cpu().numpy()\n",
    "        labels = output['labels'].cpu().numpy()\n",
    "        \n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            coco_results.append({\n",
    "                'image_id': image_id,\n",
    "                'category_id': int(label),\n",
    "                'bbox': [box[0], box[1], box[2] - box[0], box[3] - box[1]],\n",
    "                'score': float(score)\n",
    "            })\n",
    "    return coco_results\n",
    "\n",
    "# Validation Function\n",
    "def validate_old(model, data_loader, coco_gt, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    # Add tqdm\n",
    "    progress_bar = tqdm(data_loader, desc=\"Validation\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in progress_bar:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            image_ids = [target['image_id'].item() for target in targets]\n",
    "            coco_results = convert_to_coco_format(outputs, image_ids)\n",
    "            results.extend(coco_results)\n",
    "\n",
    "            # Update tqdm-bar\n",
    "            progress_bar.set_postfix({\"Processed\": len(results)})\n",
    "\n",
    "    if not results:\n",
    "        print(\"No predictions generated. Skipping evaluation.\")\n",
    "        return [0.0] * 6  # Return dummy metrics for empty results\n",
    "\n",
    "    # Suppress COCOeval output\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        coco_dt = coco_gt.loadRes(results)\n",
    "        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "    return coco_eval.stats\n",
    "def validate(model, data_loader, coco_gt, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    total_loss = 0\n",
    "    total_classifier_loss = 0\n",
    "    total_box_reg_loss = 0\n",
    "    total_objectness_loss = 0\n",
    "    total_rpn_box_reg_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"Validation\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in progress_bar:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Compute detections for mAP (in eval mode)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Convert outputs to COCO format\n",
    "            image_ids = [t['image_id'].item() for t in targets]\n",
    "            coco_results = convert_to_coco_format(outputs, image_ids)\n",
    "            results.extend(coco_results)\n",
    "\n",
    "            # Compute validation loss by temporarily switching to train mode\n",
    "            model.train()  # Switch to train mode to compute loss\n",
    "            loss_dict = model(images, targets)\n",
    "            model.eval()   # Switch back to eval mode\n",
    "\n",
    "            # Sum the losses\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += losses.item()\n",
    "            total_classifier_loss += loss_dict['loss_classifier'].item()\n",
    "            total_box_reg_loss += loss_dict['loss_box_reg'].item()\n",
    "            total_objectness_loss += loss_dict['loss_objectness'].item()\n",
    "            total_rpn_box_reg_loss += loss_dict['loss_rpn_box_reg'].item()\n",
    "\n",
    "            # Update progress bar with validation loss\n",
    "            progress_bar.set_postfix({\n",
    "                \"Processed\": len(results),\n",
    "                \"Val Loss\": f\"{losses.item():.4f}\"\n",
    "            })\n",
    "\n",
    "    # Calculate average validation losses\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_classifier_loss = total_classifier_loss / len(data_loader)\n",
    "    avg_box_reg_loss = total_box_reg_loss / len(data_loader)\n",
    "    avg_objectness_loss = total_objectness_loss / len(data_loader)\n",
    "    avg_rpn_box_reg_loss = total_rpn_box_reg_loss / len(data_loader)\n",
    "\n",
    "    # Compute COCO metrics if there are results\n",
    "    if not results:\n",
    "        print(\"No predictions generated. Skipping evaluation.\")\n",
    "        return [0.0] * 6, (avg_loss, avg_classifier_loss, avg_box_reg_loss, avg_objectness_loss, avg_rpn_box_reg_loss)\n",
    "\n",
    "    # Evaluate using COCO API\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        coco_dt = coco_gt.loadRes(results)\n",
    "        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "    return coco_eval.stats, (avg_loss, avg_classifier_loss, avg_box_reg_loss, avg_objectness_loss, avg_rpn_box_reg_loss)\n",
    "\n",
    "# Custom backbone to return a dictionary of feature maps\n",
    "class BackboneWithChannels(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return {'0': x}\n",
    "    \n",
    "# Function to create a subset of the dataset\n",
    "def create_subset(dataset, percentage):\n",
    "    \"\"\"\n",
    "    Create a subset of the dataset based on the given percentage.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: The full dataset.\n",
    "    - percentage: The fraction of the dataset to use (value between 0.0 and 1.0).\n",
    "    \n",
    "    Returns:\n",
    "    - subset: A subset of the dataset containing the specified percentage of data.\n",
    "    \"\"\"\n",
    "    if not (0.0 < percentage <= 1.0):\n",
    "        raise ValueError(\"Percentage must be between 0.0 and 1.0.\")\n",
    "    \n",
    "    # Determine the subset size\n",
    "    total_samples = len(dataset)\n",
    "    subset_size = int(total_samples * percentage)\n",
    "    \n",
    "    # Shuffle and select a random subset of indices\n",
    "    indices = list(range(total_samples))\n",
    "    random.shuffle(indices)\n",
    "    subset_indices = indices[:subset_size]\n",
    "    \n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "def create_directory(base_path=\"data/models\"):\n",
    "    \"\"\"\n",
    "    Create a directory inside the base path named 'faster-rcnn-finetuned-{date}' \n",
    "    to store models and logs. The name includes the current date and time in the format 'DD-MM-YYYY HH:MM:SS'.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Base directory where the new directory will be created.\n",
    "\n",
    "    Returns:\n",
    "    - directory_path (str): Full path to the created directory.\n",
    "    \"\"\"\n",
    "    # Get the current date and time\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "    \n",
    "    # Define the full directory path\n",
    "    directory_name = f\"faster-rcnn-finetuned-{current_time}\"\n",
    "    directory_path = os.path.join(base_path, directory_name)\n",
    "    \n",
    "    # Create the directory\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Directory created: {directory_path}\")\n",
    "    return directory_path\n",
    "\n",
    "def train_model_old(directory, model, optimizer, train_loader, device, train_metrics_list, best_val_map, lr_scheduler, val_loader, coco_val, scaler, epoch):\n",
    "    \n",
    "    epoch+=1\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_classifier_loss, train_box_reg_loss, train_objectness_loss, train_rpn_box_reg_loss = train_one_epoch(\n",
    "        model, optimizer, train_loader, device, scaler)\n",
    "    \n",
    "    # Validate and get all COCO-metrics\n",
    "    val_metrics = validate(model, val_loader, coco_val, device)\n",
    "    val_map = val_metrics[0]  # mAP@IoU=0.50:0.95\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    \n",
    "    # Obtain the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Prepare data for logging\n",
    "    data = {\n",
    "        \"epoch\": epoch,\n",
    "        \"time_elapsed\": (int(minutes), int(seconds)),\n",
    "        \"learning_rate\": current_lr,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"classifier_loss\": train_classifier_loss,\n",
    "        \"box_reg_loss\": train_box_reg_loss,\n",
    "        \"objectness_loss\": train_objectness_loss,\n",
    "        \"rpn_box_reg_loss\": train_rpn_box_reg_loss,\n",
    "        \"val_metrics\": val_metrics\n",
    "    }\n",
    "    \n",
    "    # Append current epoch data to metrics list\n",
    "    train_metrics_list.append(data)\n",
    "    \n",
    "    # Print summary for this epoch\n",
    "    print(f\"📊 Epoch {epoch} | ⏳ Time: {int(minutes)}m {int(seconds)}s | 🔄 LR: {current_lr:.6f}\")\n",
    "    print(f\"📉 Train Loss: {train_loss:.4f} | 🎯 Classifier: {train_classifier_loss:.4f} | 📦 Box Reg: {train_box_reg_loss:.4f}\")\n",
    "    print(f\"🔍 Objectness: {train_objectness_loss:.4f} | 🗂️ RPN Box Reg: {train_rpn_box_reg_loss:.4f}\")\n",
    "    print(f\"🧪 mAP | 🟢 mAP@IoU=0.50:0.95: {val_metrics[0]:.4f} | 🔵 mAP@IoU=0.50: {val_metrics[1]:.4f} | 🟣 mAP@IoU=0.75: {val_metrics[2]:.4f}\")\n",
    "    print(f\"📏 Small mAP: {val_metrics[3]:.4f} | 📐 Medium mAP: {val_metrics[4]:.4f} | 📏 Large mAP: {val_metrics[5]:.4f}\")\n",
    "    \n",
    "    # Save epoch data to a log file\n",
    "    save_epoch_data(directory, data)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save the latest checkpoint with all metrics\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_map': val_map,\n",
    "        'train_metrics_list': train_metrics_list  # Save all metrics\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(directory, \"latest_model.pth\"))\n",
    "    \n",
    "    # Save the best model if the val_map is the highest so far\n",
    "    if val_map > best_val_map:\n",
    "        best_val_map = val_map\n",
    "        torch.save(checkpoint, os.path.join(directory, \"best_model.pth\"))\n",
    "    \n",
    "    return best_val_map\n",
    "\n",
    "def train_model(\n",
    "    directory, \n",
    "    model, \n",
    "    optimizer, \n",
    "    train_loader, \n",
    "    device, \n",
    "    train_metrics_list, \n",
    "    best_val_map, \n",
    "    lr_scheduler, \n",
    "    val_loader, \n",
    "    coco_val, \n",
    "    scaler, \n",
    "    epoch, \n",
    "    # Early stopping parameters\n",
    "    patience=5, \n",
    "    delta=0.001, \n",
    "    monitor='val_map', \n",
    "    maximize=True,\n",
    "    epochs_no_improve=0,\n",
    "    early_stop=False\n",
    "):\n",
    "    epoch += 1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train one epoch\n",
    "    train_loss, train_classifier_loss, train_box_reg_loss, train_objectness_loss, train_rpn_box_reg_loss = train_one_epoch(\n",
    "        model, optimizer, train_loader, device, scaler)\n",
    "    \n",
    "    # Validate and get metrics and losses\n",
    "    val_metrics, val_losses = validate(model, val_loader, coco_val, device)\n",
    "    val_loss, val_classifier_loss, val_box_reg_loss, val_objectness_loss, val_rpn_box_reg_loss = val_losses\n",
    "    val_map = val_metrics[0]  # mAP@IoU=0.50:0.95\n",
    "    \n",
    "    # Determine current metric based on monitor\n",
    "    current_metric = val_map if monitor == 'val_map' else val_loss\n",
    "    \n",
    "    # Check if current metric is the best\n",
    "    if maximize:\n",
    "        improved = (current_metric - best_val_map) > delta\n",
    "    else:\n",
    "        improved = (best_val_map - current_metric) > delta\n",
    "    \n",
    "    if improved:\n",
    "        best_val_map = current_metric\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    # Check if early stopping is triggered\n",
    "    if epochs_no_improve >= patience:\n",
    "        early_stop = True\n",
    "        print(f\"🚨 Early stopping triggered at epoch {epoch}!\")\n",
    "    \n",
    "    # --- Rest of the function remains unchanged until return ---\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Prepare data for logging (add early stopping info)\n",
    "    data = {\n",
    "        \"epoch\": epoch,\n",
    "        \"time_elapsed\": (int(minutes), int(seconds)),\n",
    "        \"learning_rate\": current_lr,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"classifier_loss\": train_classifier_loss,\n",
    "        \"box_reg_loss\": train_box_reg_loss,\n",
    "        \"objectness_loss\": train_objectness_loss,\n",
    "        \"rpn_box_reg_loss\": train_rpn_box_reg_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_classifier_loss\": val_classifier_loss,\n",
    "        \"val_box_reg_loss\": val_box_reg_loss,\n",
    "        \"val_objectness_loss\": val_objectness_loss,\n",
    "        \"val_rpn_box_reg_loss\": val_rpn_box_reg_loss,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"early_stop\": early_stop,\n",
    "        \"epochs_no_improve\": epochs_no_improve\n",
    "    }\n",
    "    \n",
    "    train_metrics_list.append(data)\n",
    "    \n",
    "    # Print summary with validation losses\n",
    "    print(f\"📊 Epoch {epoch} | ⏳ Time: {int(minutes)}m {int(seconds)}s | 🔄 LR: {current_lr:.6f}\")\n",
    "    print(f\"📉 Train Loss: {train_loss:.4f} | 🎯 Classifier: {train_classifier_loss:.4f} | 📦 Box Reg: {train_box_reg_loss:.4f}\")\n",
    "    print(f\"🔍 Objectness: {train_objectness_loss:.4f} | 🗂️ RPN Box Reg: {train_rpn_box_reg_loss:.4f}\")\n",
    "    print(f\"📉 Val Loss: {val_loss:.4f} | 🎯 Val Classifier: {val_classifier_loss:.4f} | 📦 Val Box Reg: {val_box_reg_loss:.4f}\")\n",
    "    print(f\"🔍 Val Objectness: {val_objectness_loss:.4f} | 🗂️ Val RPN Box Reg: {val_rpn_box_reg_loss:.4f}\")\n",
    "    print(f\"🧪 mAP | 🟢 mAP@IoU=0.50:0.95: {val_metrics[0]:.4f} | 🔵 mAP@IoU=0.50: {val_metrics[1]:.4f} | 🟣 mAP@IoU=0.75: {val_metrics[2]:.4f}\")\n",
    "    print(f\"📏 Small mAP: {val_metrics[3]:.4f} | 📐 Medium mAP: {val_metrics[4]:.4f} | 📏 Large mAP: {val_metrics[5]:.4f}\")\n",
    "    print(f\"🛑 Early stopping counter: {epochs_no_improve}/{patience}\")\n",
    "    \n",
    "    save_epoch_data(directory, data)\n",
    "\n",
    "    # Save checkpoints and update learning rate\n",
    "    lr_scheduler.step()\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_map': val_map,\n",
    "        'train_metrics_list': train_metrics_list\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(directory, \"latest_model.pth\"))\n",
    "    \n",
    "    if improved:\n",
    "        torch.save(checkpoint, os.path.join(directory, \"best_model.pth\"))\n",
    "    \n",
    "    return best_val_map, epochs_no_improve, early_stop\n",
    "\n",
    "from datetime import datetime\n",
    "def save_epoch_data(directory, data):\n",
    "    \"\"\"\n",
    "    Save training statistics for each epoch in a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): Path to the directory.\n",
    "    - data (dict): Contains data on metrics such as epoch, losses, and validation metrics.\n",
    "    \"\"\"\n",
    "    log_file_path = os.path.join(directory, \"training_log.txt\")\n",
    "    \n",
    "    lines = [\n",
    "        f\"Datetime {datetime.now()}\\n\",\n",
    "        f\"Epoch {data['epoch']} | Time: {data['time_elapsed'][0]}m {data['time_elapsed'][1]}s | LR: {data['learning_rate']:.10f}\\n\",\n",
    "        f\"Train Loss: {data['train_loss']:.4f} | Classifier: {data['classifier_loss']:.4f} | Box Reg: {data['box_reg_loss']:.4f}\\n\",\n",
    "        f\"Objectness: {data['objectness_loss']:.4f} | RPN Box Reg: {data['rpn_box_reg_loss']:.4f}\\n\",\n",
    "        f\"Validation Metrics: | mAP@IoU=0.50:0.95: {data['val_metrics'][0]:.4f} | mAP@IoU=0.50: {data['val_metrics'][1]:.4f} | mAP@IoU=0.75: {data['val_metrics'][2]:.4f}\\n\",\n",
    "        f\"Small mAP: {data['val_metrics'][3]:.4f} | Medium mAP: {data['val_metrics'][4]:.4f} | Large mAP: {data['val_metrics'][5]:.4f}\\n\",\n",
    "        f\"Early Stop: {data['early_stop']} | Epochs no improvement: {data['epochs_no_improve']}\\n\\n\"\n",
    "    ]\n",
    "    with open(log_file_path, \"a\") as log_file:\n",
    "        log_file.writelines(lines)\n",
    "# Initialize model\n",
    "num_classes = 2  # hazmat code and background\n",
    "\n",
    "# Create ResNet-101 backbone with FPN\n",
    "backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
    "\n",
    "# Define anchor generator for FPN\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32,), (64,), (128,), (256,), (512,)),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    ")\n",
    "\n",
    "# Multi-scale RoI pooling for FPN\n",
    "roi_pooler = MultiScaleRoIAlign(\n",
    "    featmap_names=['0', '1', '2', '3', '4'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2\n",
    ")\n",
    "\n",
    "print(\"initializing model...\")\n",
    "# Initialize Faster R-CNN with ResNet-101-FPN\n",
    "model = FasterRCNN(\n",
    "    backbone=backbone,\n",
    "    num_classes=num_classes,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ")\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "# Load the model\n",
    "# directory_finetuned_model = os.path.join(\"data\", \"models\", \"faster-rcnn-finetuned-20-02-2025 11:53:54\")\n",
    "directory_finetuned_model = os.path.join(\"data\", \"models\")\n",
    "device = torch.device('cuda')\n",
    "model_path = os.path.join(directory_finetuned_model, 'latest_model.pth')\n",
    "checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "val_map = checkpoint['val_map']\n",
    "epoch = checkpoint['epoch']\n",
    "#latest\n",
    "latest_model_path = os.path.join(directory_finetuned_model, 'latest_model.pth')\n",
    "checkpoint_latest = torch.load(latest_model_path, map_location=device, weights_only=False)\n",
    "val_map_latest = checkpoint_latest['val_map']\n",
    "epoch_latest = checkpoint_latest['epoch']\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(f\"Validation mAP best model: {val_map:.4f}\")\n",
    "print(f\"Epoch best model: {epoch}\")\n",
    "\n",
    "print(f\"Validation mAP latest model: {val_map_latest:.4f}\")\n",
    "print(f\"Epoch latest model: {epoch_latest}\")\n",
    "\n",
    "def load_image(image_path, transforms=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transforms:\n",
    "        for transform in transforms:\n",
    "            image, _ = transform(image, target=None)  # No target during inference\n",
    "    return image\n",
    "\n",
    "# Define preprocessing transforms\n",
    "test_transforms = get_transform(train=False)\n",
    "\n",
    "# Load the image\n",
    "image_path = 'images/hazard_plate.jpg'  # Replace with your image path\n",
    "image = load_image(image_path, transforms=test_transforms)\n",
    "image = image.to(device)\n",
    "# Wrap the image in a list as the model expects a batch\n",
    "with torch.no_grad():\n",
    "    predictions = model([image])\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def get_color_with_opacity(score):\n",
    "    \"\"\"\n",
    "    Get a color with opacity based on the confidence score.\n",
    "    Higher confidence = more red and higher opacity.\n",
    "    Lower confidence = random color and lower opacity.\n",
    "    \"\"\"\n",
    "    if score > 0.75:\n",
    "        # High confidence: Red with high opacity\n",
    "        color = (1, 0, 0, min(1.0, 0.3 + score))  # Red with opacity based on score\n",
    "    else:\n",
    "        # Low confidence: Random color with lower opacity\n",
    "        color = (random.random(), random.random(), random.random(), max(0.3, score))\n",
    "    return color\n",
    "\n",
    "def draw_predictions(image, predictions, threshold=0.5, classes=['background', 'hazmat'], save_path=None):\n",
    "    \"\"\"\n",
    "    Draw predictions on the image. If save_path is provided, the image is saved to that path,\n",
    "    otherwise it is displayed.\n",
    "    \"\"\"\n",
    "    # Convert image from tensor to numpy array\n",
    "    image_np = image.cpu().permute(1, 2, 0).numpy()\n",
    "    image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Filter predictions based on confidence threshold\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image_np)\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if label == 1:  # Only plot hazmat codes\n",
    "            x1, y1, x2, y2 = box\n",
    "            color = get_color_with_opacity(score)\n",
    "            \n",
    "            # Draw rectangle with opacity\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, \n",
    "                                   edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add text label with confidence score\n",
    "            label_name = classes[label]\n",
    "            ax.text(x1, y1, f'{label_name}: {score:.2f}', \n",
    "                    color='white', \n",
    "                    bbox=dict(facecolor=color[:3], alpha=0.6), \n",
    "                    fontsize=12)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Save or display the image\n",
    "    if save_path:\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        # Save with tight layout and no padding\n",
    "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "        plt.close(fig)\n",
    "        print(f\"Image saved to: {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def predict_image(image_path, model, device, test_transforms, threshold=0.5, save_path=None):\n",
    "    \"\"\"\n",
    "    Load an image, predict using the model, and display or save results.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the input image\n",
    "        model: The detection model\n",
    "        device: The computation device (CPU/GPU)\n",
    "        test_transforms: Transforms to apply to the image\n",
    "        threshold (float): Confidence threshold for predictions\n",
    "        save_path (str, optional): Path to save the output image with predictions\n",
    "    \"\"\"\n",
    "    # List of class names\n",
    "    classes = ['background', 'hazmat']\n",
    "    \n",
    "    # Load the image\n",
    "    image = load_image(image_path, transforms=test_transforms)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Wrap the image in a list as the model expects a batch\n",
    "    with torch.no_grad():\n",
    "        predictions = model([image])\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    prediction_time = end_time - start_time\n",
    "    print(f\"Prediction time: {prediction_time:.4f} seconds\")\n",
    "    \n",
    "    # Filter predictions based on threshold\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Apply threshold filter\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    # Print the predictions\n",
    "    if len(boxes) == 0:\n",
    "        print(\"No predictions meet the threshold.\")\n",
    "    else:\n",
    "        print(\"Predictions:\")\n",
    "        for label, score in zip(labels, scores):\n",
    "            class_name = classes[label]\n",
    "            print(f\"  {class_name}: {score:.2f}\")\n",
    "    \n",
    "    # Display or save the predictions\n",
    "    draw_predictions(image, predictions, threshold=threshold, classes=classes, save_path=save_path)\n",
    "    \n",
    "    return predictions\n",
    "x = predict_image(image_path='images/hazard_plate.jpg', \n",
    "              model=model, \n",
    "              device=device, \n",
    "              test_transforms=test_transforms, \n",
    "              threshold=0.5)\n",
    "\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make code to draw the GT on the picture from public dataset\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "\n",
    "def check_images_with_annotations(annotation_file, images_dir, max_images=10):\n",
    "    with open(annotation_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # Create a dictionary mapping image_id to all its annotations\n",
    "        image_annotations = defaultdict(list)\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            image_annotations[annotation[\"image_id\"]].append(annotation)\n",
    "        \n",
    "        # Get list of images and shuffle them\n",
    "        images = data[\"images\"]\n",
    "        # random.shuffle(images)\n",
    "        \n",
    "        # Process up to max_images\n",
    "        for image_data in images[:max_images]:\n",
    "            image_id = image_data[\"id\"]\n",
    "            file_name = image_data[\"file_name\"]\n",
    "            \n",
    "            # Check if this image has annotations\n",
    "            if image_id not in image_annotations:\n",
    "                continue\n",
    "                \n",
    "            # Read the image\n",
    "            path_im = f\"{images_dir}/{file_name}\"\n",
    "            image = cv2.imread(path_im)\n",
    "            if image is None:\n",
    "                print(f\"Image not found: {path_im}\")\n",
    "                continue\n",
    "                \n",
    "            # Convert BGR (OpenCV format) to RGB (Matplotlib format)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Plot the image\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(image)\n",
    "            \n",
    "            # Draw all annotations for this image\n",
    "            annotations_for_image = image_annotations[image_id]\n",
    "            for annotation in annotations_for_image:\n",
    "                x, y, w, h = annotation[\"bbox\"]\n",
    "                x, y, w, h = round(x), round(y), round(w), round(h)\n",
    "                \n",
    "                # Draw the bounding box with thicker lines\n",
    "                plt.gca().add_patch(plt.Rectangle((x, y), w, h, edgecolor='green', facecolor='none', linewidth=4))\n",
    "                \n",
    "                # Add \"Ground Truth\" label above each bounding box\n",
    "                plt.text(x, y-5, \"Ground Truth\", color='green', fontsize=12, \n",
    "                         bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))\n",
    "            \n",
    "            plt.title(f\"Image file: {file_name}\", fontsize=12)\n",
    "            plt.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Usage\n",
    "annotations_file = \"data/public_dataset/test/annotations/instances_test.json\"\n",
    "images_dir = \"data/public_dataset/test/images\"\n",
    "check_images_with_annotations(annotations_file, images_dir, max_images=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_color_with_opacity(score):\n",
    "    \"\"\"\n",
    "    Get a color with opacity based on the confidence score.\n",
    "    Higher confidence = more red and higher opacity.\n",
    "    Lower confidence = random color and lower opacity.\n",
    "    \"\"\"\n",
    "    if score > 0.75:\n",
    "        # High confidence: Red with high opacity\n",
    "        color = (1, 0, 0, min(1.0, 0.3 + score))  # Red with opacity based on score\n",
    "    else:\n",
    "        # Low confidence: Random color with lower opacity\n",
    "        color = (random.random(), random.random(), random.random(), max(0.3, score))\n",
    "    return color\n",
    "\n",
    "def load_image(image_path, transforms=None):\n",
    "    \"\"\"Load an image and apply transforms if provided.\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transforms:\n",
    "        for transform in transforms:\n",
    "            image, _ = transform(image, target=None)  # No target during inference\n",
    "    return image\n",
    "\n",
    "def visualize_ground_truth_and_predictions(annotation_file, images_dir, model, device, test_transforms, \n",
    "                                          max_images=10, confidence_threshold=0.5, \n",
    "                                          classes=['background', 'hazmat'], save_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize both ground truth and model predictions on images.\n",
    "    \n",
    "    Args:\n",
    "        annotation_file (str): Path to the annotations JSON file\n",
    "        images_dir (str): Directory containing the images\n",
    "        model: The detection model\n",
    "        device: The computation device (CPU/GPU)\n",
    "        test_transforms: Transforms to apply to the image\n",
    "        max_images (int): Maximum number of images to process\n",
    "        confidence_threshold (float): Threshold for prediction confidence\n",
    "        classes (list): List of class names\n",
    "        save_dir (str, optional): Directory to save output images instead of displaying\n",
    "    \"\"\"\n",
    "    # Load annotations\n",
    "    with open(annotation_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # Create a dictionary mapping image_id to all its annotations\n",
    "        image_annotations = defaultdict(list)\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            image_annotations[annotation[\"image_id\"]].append(annotation)\n",
    "        \n",
    "        # Get list of images and shuffle them\n",
    "        images = data[\"images\"]\n",
    "        random.shuffle(images)\n",
    "        \n",
    "        # Process up to max_images\n",
    "        for image_data in images[:max_images]:\n",
    "            image_id = image_data[\"id\"]\n",
    "            file_name = image_data[\"file_name\"]\n",
    "            \n",
    "            # Check if this image has annotations\n",
    "            if image_id not in image_annotations:\n",
    "                continue\n",
    "                \n",
    "            # Read the image\n",
    "            image_path = os.path.join(images_dir, file_name)\n",
    "            \n",
    "            # Load image for CV2/matplotlib display\n",
    "            cv_image = cv2.imread(image_path)\n",
    "            if cv_image is None:\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Convert BGR (OpenCV format) to RGB (Matplotlib format)\n",
    "            cv_image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load image for model prediction\n",
    "            tensor_image = load_image(image_path, transforms=test_transforms)\n",
    "            tensor_image = tensor_image.to(device)\n",
    "            \n",
    "            # Make predictions\n",
    "            start_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                predictions = model([tensor_image])\n",
    "            prediction_time = time.time() - start_time\n",
    "            print(f\"Prediction time for {file_name}: {prediction_time:.4f} seconds\")\n",
    "            \n",
    "            # Create figure and axis\n",
    "            fig, ax = plt.subplots(1, figsize=(12, 10))\n",
    "            ax.imshow(cv_image_rgb)\n",
    "            \n",
    "            # Draw ground truth annotations\n",
    "            annotations_for_image = image_annotations[image_id]\n",
    "            for annotation in annotations_for_image:\n",
    "                x, y, w, h = annotation[\"bbox\"]\n",
    "                x, y, w, h = round(x), round(y), round(w), round(h)\n",
    "                \n",
    "                # Draw the bounding box with thick green lines\n",
    "                rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='white', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add \"Ground Truth\" label above each bounding box\n",
    "                ax.text(x + w+20, y+h+20, \"Ground Truth\", color='white', fontsize=12, ha='right',\n",
    "                            bbox=dict(facecolor='black', alpha=0.7, edgecolor='none', pad=1))\n",
    "            \n",
    "            # Draw model predictions\n",
    "            boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "            labels = predictions[0]['labels'].cpu().numpy()\n",
    "            scores = predictions[0]['scores'].cpu().numpy()\n",
    "            \n",
    "            # Filter predictions based on confidence threshold\n",
    "            keep = scores >= confidence_threshold\n",
    "            boxes = boxes[keep]\n",
    "            labels = labels[keep]\n",
    "            scores = scores[keep]\n",
    "            \n",
    "            for box, label, score in zip(boxes, labels, scores):\n",
    "                if label == 1:  # Only plot hazmat codes (adjust as needed)\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    color = get_color_with_opacity(score)\n",
    "                    \n",
    "                    # Draw rectangle with opacity\n",
    "                    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, \n",
    "                                           edgecolor=color, facecolor='none')\n",
    "                    ax.add_patch(rect)\n",
    "                    \n",
    "                    # Add text label with confidence score\n",
    "                    label_name = classes[label]\n",
    "                    ax.text(x1, y1-10, f'Prediction: {score:.2f}', \n",
    "                            color='white', \n",
    "                            bbox=dict(facecolor=color[:3], alpha=0.6), \n",
    "                            fontsize=12)\n",
    "            \n",
    "            # plt.title(f\"Image: {file_name} - Ground Truth & Predictions\", fontsize=14)\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save or display the image\n",
    "            if save_dir:\n",
    "                # Ensure directory exists\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_path = os.path.join(save_dir, f\"gt_pred_{file_name}\")\n",
    "                plt.savefig(save_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "                plt.close(fig)\n",
    "                print(f\"Image saved to: {save_path}\")\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "# Example usage\n",
    "def run_visualization(annotation_file, images_dir, model, device, test_transforms, save_dir=None):\n",
    "    visualize_ground_truth_and_predictions(\n",
    "        annotation_file=annotation_file,\n",
    "        images_dir=images_dir,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        test_transforms=test_transforms,\n",
    "        max_images=999999999999999999999999999,  # Adjust as needed\n",
    "        confidence_threshold=0.5,\n",
    "        classes=['background', 'hazmat'],\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "\n",
    "# To use this function, call it with your model and data:\n",
    "\n",
    "# Example usage:\n",
    "run_visualization(\n",
    "    annotation_file=\"data/public_dataset/test/annotations/instances_test.json\",\n",
    "    images_dir=\"data/public_dataset/test/images\",\n",
    "    model=model,  # Your loaded model\n",
    "    device=device,  # torch.device object\n",
    "    test_transforms=test_transforms,  # Your transformation pipeline\n",
    "    save_dir=\"output/faster_rcnn_paper_examples_2\"  # Optional: directory to save results\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_color_with_opacity(score):\n",
    "    \"\"\"\n",
    "    Get a color with opacity based on the confidence score.\n",
    "    Higher confidence = more red and higher opacity.\n",
    "    Lower confidence = random color and lower opacity.\n",
    "    \"\"\"\n",
    "    if score > 0.75:\n",
    "        # High confidence: Red with high opacity\n",
    "        color = (1, 0, 0, min(1.0, 0.3 + score))  # Red with opacity based on score\n",
    "    else:\n",
    "        # Low confidence: Random color with lower opacity\n",
    "        color = (random.random(), random.random(), random.random(), max(0.3, score))\n",
    "    return color\n",
    "\n",
    "def load_image(image_path, transforms=None):\n",
    "    \"\"\"Load an image and apply transforms if provided.\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transforms:\n",
    "        for transform in transforms:\n",
    "            image, _ = transform(image, target=None)  # No target during inference\n",
    "    return image\n",
    "\n",
    "def visualize_ground_truth_and_predictions(annotation_file, images_dir, model, device, test_transforms, \n",
    "                                          confidence_threshold=0.5, \n",
    "                                          classes=['background', 'hazmat'], save_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize both ground truth and model predictions on images, ensuring each image is processed only once.\n",
    "    \n",
    "    Args:\n",
    "        annotation_file (str): Path to the annotations JSON file\n",
    "        images_dir (str): Directory containing the images\n",
    "        model: The detection model\n",
    "        device: The computation device (CPU/GPU)\n",
    "        test_transforms: Transforms to apply to the image\n",
    "        confidence_threshold (float): Threshold for prediction confidence\n",
    "        classes (list): List of class names\n",
    "        save_dir (str, optional): Directory to save output images instead of displaying\n",
    "    \"\"\"\n",
    "    # Load annotations\n",
    "    with open(annotation_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # Create a dictionary mapping image_id to all its annotations\n",
    "        image_annotations = defaultdict(list)\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            image_annotations[annotation[\"image_id\"]].append(annotation)\n",
    "        \n",
    "        # Create a dictionary mapping file_name to image_id to ensure uniqueness\n",
    "        filename_to_id = {}\n",
    "        for image in data[\"images\"]:\n",
    "            filename_to_id[image[\"file_name\"]] = image[\"id\"]\n",
    "        \n",
    "        # Process each unique image\n",
    "        processed_images = set()\n",
    "        for image_data in data[\"images\"]:\n",
    "            image_id = image_data[\"id\"]\n",
    "            file_name = image_data[\"file_name\"]\n",
    "            \n",
    "            # Skip if we've already processed this image or if it has no annotations\n",
    "            if file_name in processed_images or image_id not in image_annotations:\n",
    "                continue\n",
    "                \n",
    "            # Mark as processed\n",
    "            processed_images.add(file_name)\n",
    "            \n",
    "            # Read the image\n",
    "            image_path = os.path.join(images_dir, file_name)\n",
    "            \n",
    "            # Load image for CV2/matplotlib display\n",
    "            cv_image = cv2.imread(image_path)\n",
    "            if cv_image is None:\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Convert BGR (OpenCV format) to RGB (Matplotlib format)\n",
    "            cv_image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load image for model prediction\n",
    "            tensor_image = load_image(image_path, transforms=test_transforms)\n",
    "            tensor_image = tensor_image.to(device)\n",
    "            \n",
    "            # Make predictions\n",
    "            start_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                predictions = model([tensor_image])\n",
    "            prediction_time = time.time() - start_time\n",
    "            print(f\"Prediction time for {file_name}: {prediction_time:.4f} seconds\")\n",
    "            \n",
    "            # Create figure and axis\n",
    "            fig, ax = plt.subplots(1, figsize=(12, 10))\n",
    "            ax.imshow(cv_image_rgb)\n",
    "            \n",
    "            # Draw ground truth annotations\n",
    "            annotations_for_image = image_annotations[image_id]\n",
    "            for annotation in annotations_for_image:\n",
    "                x, y, w, h = annotation[\"bbox\"]\n",
    "                x, y, w, h = round(x), round(y), round(w), round(h)\n",
    "                \n",
    "                # Draw the bounding box with thick white lines\n",
    "                rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='white', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add \"Ground Truth\" label\n",
    "                ax.text(x + w+20, y+h+20, \"Ground Truth\", color='white', fontsize=12, ha='right',\n",
    "                        bbox=dict(facecolor='black', alpha=0.7, edgecolor='none', pad=1))\n",
    "            \n",
    "            # Draw model predictions\n",
    "            boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "            labels = predictions[0]['labels'].cpu().numpy()\n",
    "            scores = predictions[0]['scores'].cpu().numpy()\n",
    "            \n",
    "            # Filter predictions based on confidence threshold\n",
    "            keep = scores >= confidence_threshold\n",
    "            boxes = boxes[keep]\n",
    "            labels = labels[keep]\n",
    "            scores = scores[keep]\n",
    "            \n",
    "            for box, label, score in zip(boxes, labels, scores):\n",
    "                if label == 1:  # Only plot hazmat codes (adjust as needed)\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    color = get_color_with_opacity(score)\n",
    "                    \n",
    "                    # Draw rectangle with opacity\n",
    "                    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, \n",
    "                                           edgecolor=color, facecolor='none')\n",
    "                    ax.add_patch(rect)\n",
    "                    \n",
    "                    # Add text label with confidence score\n",
    "                    label_name = classes[label]\n",
    "                    ax.text(x1, y1-10, f'Prediction: {score:.2f}', \n",
    "                            color='white', \n",
    "                            bbox=dict(facecolor=color[:3], alpha=0.6), \n",
    "                            fontsize=12)\n",
    "            \n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save or display the image\n",
    "            if save_dir:\n",
    "                # Ensure directory exists\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_path = os.path.join(save_dir, f\"gt_pred_{file_name}\")\n",
    "                plt.savefig(save_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "                plt.close(fig)\n",
    "                print(f\"Image saved to: {save_path}\")\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "# Example usage\n",
    "def run_visualization(annotation_file, images_dir, model, device, test_transforms, save_dir=None):\n",
    "    visualize_ground_truth_and_predictions(\n",
    "        annotation_file=annotation_file,\n",
    "        images_dir=images_dir,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        test_transforms=test_transforms,\n",
    "        confidence_threshold=0.5,\n",
    "        classes=['background', 'hazmat'],\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "\n",
    "# To use this function, call it with your model and data:\n",
    "\n",
    "run_visualization(\n",
    "    annotation_file=\"data/public_dataset/test/annotations/instances_test.json\",\n",
    "    images_dir=\"data/public_dataset/test/images\",\n",
    "    model=model,  # Your loaded model\n",
    "    device=device,  # torch.device object\n",
    "    test_transforms=test_transforms,  # Your transformation pipeline\n",
    "    save_dir=\"output/ground_truth_and_predictions\"  # Optional: directory to save results\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id's of images with problems:\n",
    "#### backlights\n",
    "- 85\n",
    "- 275\n",
    "####  Duplicate UN Number Predictions\n",
    "- 277\n",
    "####  Small UN Numbers Are Not Being Predicted (False negative)\n",
    "- 144\n",
    "- 135\n",
    "- 133\n",
    "####  Red Letters on Truck Plates\n",
    "- 215\n",
    "- 154\n",
    "#### Back of the Truck Identified as a UN Number\n",
    "- 257\n",
    "#### obscured UN Numbers are hard to Recognize\n",
    "- 187\n",
    "#### Yellow UN numbers are harder to recognize\n",
    "- 256\n",
    "- 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to get the author and link and researcher from id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(r\"data/yolo_models/yolo11x_10epoch_augmented.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "def draw_rectangles(image_path, results):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    img_height, img_width, _ = image.shape\n",
    "    boxes = results.boxes   \n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = map(int, box.xyxy[0])  # Convert to integers\n",
    "        confidence = box.conf[0]  # Confidence score\n",
    "        class_id = int(box.cls[0])  # Class ID\n",
    "        label = results.names[class_id]  # Class label\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "        # Put the label and confidence score\n",
    "        cv2.putText(image, f\"{label} {confidence:.2f}\", (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    \n",
    "    # Convert BGR image to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(r\"data/yolo_models/yolo11x_10epoch_augmented.pt\")\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "def draw_rectangles(image_path, results):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    img_height, img_width, _ = image.shape\n",
    "    boxes = results.boxes   \n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = map(int, box.xyxy[0])  # Convert to integers\n",
    "        confidence = box.conf[0]  # Confidence score\n",
    "        class_id = int(box.cls[0])  # Class ID\n",
    "        label = results.names[class_id]  # Class label\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "        # Put the label and confidence score\n",
    "        cv2.putText(image, f\"{label} {confidence:.2f}\", (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    \n",
    "    # Convert BGR image to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "# get predictions for this image\n",
    "results = model(\"images/hazard_plate.jpg\")\n",
    "for result in results:\n",
    "    print(result.boxes)\n",
    "    draw_rectangles(result.path, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def get_color_with_opacity(score):\n",
    "    \"\"\"\n",
    "    Get a color with opacity based on the confidence score.\n",
    "    Higher confidence = more red and higher opacity.\n",
    "    Lower confidence = random color and lower opacity.\n",
    "    \"\"\"\n",
    "    if score > 0.75:\n",
    "        # High confidence: Red with high opacity\n",
    "        color = (1, 0, 0, min(1.0, 0.3 + score))  # Red with opacity based on score\n",
    "    else:\n",
    "        # Low confidence: Random color with lower opacity\n",
    "        color = (random.random(), random.random(), random.random(), max(0.3, score))\n",
    "    return color\n",
    "\n",
    "def visualize_ground_truth_and_predictions(annotation_file, images_dir, model, \n",
    "                                          confidence_threshold=0.5, \n",
    "                                          save_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize both ground truth and YOLO model predictions on images.\n",
    "    \n",
    "    Args:\n",
    "        annotation_file (str): Path to the annotations JSON file\n",
    "        images_dir (str): Directory containing the images\n",
    "        model: The YOLO model\n",
    "        confidence_threshold (float): Threshold for prediction confidence\n",
    "        save_dir (str, optional): Directory to save output images instead of displaying\n",
    "    \"\"\"\n",
    "    # Load annotations\n",
    "    with open(annotation_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # Create a dictionary mapping image_id to all its annotations\n",
    "        image_annotations = defaultdict(list)\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            image_annotations[annotation[\"image_id\"]].append(annotation)\n",
    "        \n",
    "        # Create a dictionary mapping file_name to image_id to ensure uniqueness\n",
    "        filename_to_id = {}\n",
    "        for image in data[\"images\"]:\n",
    "            filename_to_id[image[\"file_name\"]] = image[\"id\"]\n",
    "        \n",
    "        # Process each unique image\n",
    "        processed_images = set()\n",
    "        for image_data in data[\"images\"]:\n",
    "            image_id = image_data[\"id\"]\n",
    "            file_name = image_data[\"file_name\"]\n",
    "            \n",
    "            # Skip if we've already processed this image or if it has no annotations\n",
    "            if file_name in processed_images or image_id not in image_annotations:\n",
    "                continue\n",
    "                \n",
    "            # Mark as processed\n",
    "            processed_images.add(file_name)\n",
    "            \n",
    "            # Read the image\n",
    "            image_path = os.path.join(images_dir, file_name)\n",
    "            \n",
    "            # Load image for CV2/matplotlib display\n",
    "            cv_image = cv2.imread(image_path)\n",
    "            if cv_image is None:\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Convert BGR (OpenCV format) to RGB (Matplotlib format)\n",
    "            cv_image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Make predictions with YOLO\n",
    "            start_time = time.time()\n",
    "            results = model(image_path)\n",
    "            prediction_time = time.time() - start_time\n",
    "            print(f\"Prediction time for {file_name}: {prediction_time:.4f} seconds\")\n",
    "            \n",
    "            # Create figure and axis\n",
    "            fig, ax = plt.subplots(1, figsize=(12, 10))\n",
    "            ax.imshow(cv_image_rgb)\n",
    "            \n",
    "            # Draw ground truth annotations (white boxes)\n",
    "            annotations_for_image = image_annotations[image_id]\n",
    "            for annotation in annotations_for_image:\n",
    "                x, y, w, h = annotation[\"bbox\"]\n",
    "                x, y, w, h = round(x), round(y), round(w), round(h)\n",
    "                \n",
    "                # Draw the bounding box with thick white lines\n",
    "                rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='white', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add \"Ground Truth\" label\n",
    "                ax.text(x + w + 5, y + 5, \"Ground Truth\", color='white', fontsize=12, \n",
    "                        bbox=dict(facecolor='black', alpha=0.7, edgecolor='none', pad=1))\n",
    "            \n",
    "            # Draw YOLO model predictions (colored boxes based on confidence)\n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                for box in boxes:\n",
    "                    confidence = float(box.conf[0])\n",
    "                    \n",
    "                    # Filter predictions based on confidence threshold\n",
    "                    if confidence >= confidence_threshold:\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                        class_id = int(box.cls[0])\n",
    "                        label = result.names.get(class_id, f\"Class {class_id}\")\n",
    "                        \n",
    "                        color = get_color_with_opacity(confidence)\n",
    "                        \n",
    "                        # Draw rectangle with opacity\n",
    "                        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, \n",
    "                                               edgecolor=color, facecolor='none')\n",
    "                        ax.add_patch(rect)\n",
    "                        \n",
    "                        # Add text label with confidence score\n",
    "                        ax.text(x1, y1 - 10, f'Prediction: {confidence:.2f}', \n",
    "                                color='white', \n",
    "                                bbox=dict(facecolor=color[:3], alpha=0.6), \n",
    "                                fontsize=12)\n",
    "            \n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save or display the image\n",
    "            if save_dir:\n",
    "                # Ensure directory exists\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_path = os.path.join(save_dir, f\"gt_pred_{file_name}\")\n",
    "                plt.savefig(save_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "                plt.close(fig)\n",
    "                print(f\"Image saved to: {save_path}\")\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "def run_visualization(annotation_file, images_dir, model_path, save_dir=None):\n",
    "    \"\"\"\n",
    "    Run the visualization for a YOLO model.\n",
    "    \n",
    "    Args:\n",
    "        annotation_file (str): Path to the COCO format annotation file\n",
    "        images_dir (str): Directory containing images\n",
    "        model_path (str): Path to YOLO model file\n",
    "        save_dir (str, optional): Directory to save output images\n",
    "    \"\"\"\n",
    "    # Load YOLO model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Run visualization\n",
    "    visualize_ground_truth_and_predictions(\n",
    "        annotation_file=annotation_file,\n",
    "        images_dir=images_dir,\n",
    "        model=model,\n",
    "        confidence_threshold=0.5,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "\n",
    "run_visualization(\n",
    "    annotation_file=\"data/public_dataset/test/annotations/instances_test.json\",\n",
    "    images_dir=\"data/public_dataset/test/images\",\n",
    "    model_path=\"data/yolo_models/yolo11x_10epoch_augmented.pt\",\n",
    "    # save_dir=\"output/ground_truth_and_predictions\"  # Optional: directory to save results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Augmentations eval\n",
    "## YOLO v11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def extract_id(filename):\n",
    "    \"\"\"Extracts the numeric ID from filenames like 'shadow-0-aug_124.jpg'.\"\"\"\n",
    "    match = re.search(r'aug_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def get_color_with_opacity(score):\n",
    "    \"\"\"\n",
    "    Get a color with opacity based on the confidence score.\n",
    "    Higher confidence = more red and higher opacity.\n",
    "    Lower confidence = random color and lower opacity.\n",
    "    \"\"\"\n",
    "    if score > 0.75:\n",
    "        # High confidence: Red with high opacity\n",
    "        color = (1, 0, 0, min(1.0, 0.3 + score))  # Red with opacity based on score\n",
    "    else:\n",
    "        # Low confidence: Random color with lower opacity\n",
    "        color = (random.random(), random.random(), random.random(), max(0.3, score))\n",
    "    return color\n",
    "\n",
    "def visualize_ground_truth_and_predictions(annotation_file, images_dir, model, \n",
    "                                          confidence_threshold=0.5, \n",
    "                                          save_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize both ground truth and YOLO model predictions on images.\n",
    "    \n",
    "    Args:\n",
    "        annotation_file (str): Path to the annotations JSON file\n",
    "        images_dir (str): Directory containing the images\n",
    "        model: The YOLO model\n",
    "        confidence_threshold (float): Threshold for prediction confidence\n",
    "        save_dir (str, optional): Directory to save output images instead of displaying\n",
    "    \"\"\"\n",
    "    # Load annotations\n",
    "    with open(annotation_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # Create a dictionary mapping image_id to all its annotations\n",
    "        image_annotations = defaultdict(list)\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            image_annotations[annotation[\"image_id\"]].append(annotation)\n",
    "        \n",
    "        # Create a dictionary mapping numeric ID to image data\n",
    "        id_to_image = {}\n",
    "        for image in data[\"images\"]:\n",
    "            file_name = image[\"file_name\"]\n",
    "            numeric_id = extract_id(file_name)\n",
    "            if numeric_id is not None:\n",
    "                id_to_image[numeric_id] = image\n",
    "        \n",
    "        # Get all image files in the directory\n",
    "        image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(image_annotations)\n",
    "        # Process each image file\n",
    "        for file_name in image_files:\n",
    "            # Extract numeric ID from filename\n",
    "            numeric_id = extract_id(file_name)\n",
    "            if numeric_id is None:\n",
    "                print(f\"Couldn't extract numeric ID from {file_name}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Find the corresponding image data and annotations\n",
    "            if numeric_id not in id_to_image:\n",
    "                print(f\"No image data found for ID {numeric_id} ({file_name}), skipping\")\n",
    "                continue\n",
    "                \n",
    "            image_data = id_to_image[numeric_id]\n",
    "            image_id = image_data[\"id\"]\n",
    "            \n",
    "            # Skip if it has no annotations\n",
    "            if image_id not in image_annotations:\n",
    "                print(f\"No annotations found for image ID {image_id} ({file_name}), skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Read the image\n",
    "            image_path = os.path.join(images_dir, file_name)\n",
    "            \n",
    "            # Load image for CV2/matplotlib display\n",
    "            cv_image = cv2.imread(image_path)\n",
    "            if cv_image is None:\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Convert BGR (OpenCV format) to RGB (Matplotlib format)\n",
    "            cv_image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Make predictions with YOLO\n",
    "            start_time = time.time()\n",
    "            results = model(image_path)\n",
    "            prediction_time = time.time() - start_time\n",
    "            print(f\"Prediction time for {file_name}: {prediction_time:.4f} seconds\")\n",
    "            \n",
    "            # Create figure and axis\n",
    "            fig, ax = plt.subplots(1, figsize=(12, 10))\n",
    "            ax.imshow(cv_image_rgb)\n",
    "            \n",
    "            # Draw ground truth annotations (white boxes)\n",
    "            annotations_for_image = image_annotations[image_id]\n",
    "            for annotation in annotations_for_image:\n",
    "                x, y, w, h = annotation[\"bbox\"]\n",
    "                x, y, w, h = round(x), round(y), round(w), round(h)\n",
    "                \n",
    "                # Draw the bounding box with thick white lines\n",
    "                rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='white', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add \"Ground Truth\" label\n",
    "                ax.text(x + w + 5, y + 5, \"Ground Truth\", color='white', fontsize=12, \n",
    "                        bbox=dict(facecolor='black', alpha=0.7, edgecolor='none', pad=1))\n",
    "            \n",
    "            # Draw YOLO model predictions (colored boxes based on confidence)\n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                for box in boxes:\n",
    "                    confidence = float(box.conf[0])\n",
    "                    \n",
    "                    # Filter predictions based on confidence threshold\n",
    "                    if confidence >= confidence_threshold:\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                        class_id = int(box.cls[0])\n",
    "                        label = result.names.get(class_id, f\"Class {class_id}\")\n",
    "                        \n",
    "                        color = get_color_with_opacity(confidence)\n",
    "                        \n",
    "                        # Draw rectangle with opacity\n",
    "                        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, \n",
    "                                               edgecolor=color, facecolor='none')\n",
    "                        ax.add_patch(rect)\n",
    "                        \n",
    "                        # Add text label with confidence score\n",
    "                        ax.text(x1, y1 - 10, f'{label}: {confidence:.2f}', \n",
    "                                color='white', \n",
    "                                bbox=dict(facecolor=color[:3], alpha=0.6), \n",
    "                                fontsize=12)\n",
    "            \n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save or display the image\n",
    "            if save_dir:\n",
    "                # Ensure directory exists\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_path = os.path.join(save_dir, f\"gt_pred_{file_name}\")\n",
    "                plt.savefig(save_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "                plt.close(fig)\n",
    "                print(f\"Image saved to: {save_path}\")\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "def run_visualization(annotation_file, images_dir, model_path, save_dir=None):\n",
    "    \"\"\"\n",
    "    Run the visualization for a YOLO model.\n",
    "    \n",
    "    Args:\n",
    "        annotation_file (str): Path to the COCO format annotation file\n",
    "        images_dir (str): Directory containing images\n",
    "        model_path (str): Path to YOLO model file\n",
    "        save_dir (str, optional): Directory to save output images\n",
    "    \"\"\"\n",
    "    # Load YOLO model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Run visualization\n",
    "    visualize_ground_truth_and_predictions(\n",
    "        annotation_file=annotation_file,\n",
    "        images_dir=images_dir,\n",
    "        model=model,\n",
    "        confidence_threshold=0.5,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "\n",
    "run_visualization(\n",
    "    annotation_file=\"data/public_dataset/test/annotations/instances_test.json\",\n",
    "    images_dir=\"output/sun_flare\",\n",
    "    model_path=\"data/yolo_models/yolo11x_10epoch_augmented.pt\",\n",
    "    # save_dir=\"output/ground_truth_and_predictions\"  # Optional: directory to save results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def get_color_with_opacity(score):\n",
    "    \"\"\"\n",
    "    Get a color with opacity based on the confidence score.\n",
    "    Higher confidence = more red and higher opacity.\n",
    "    Lower confidence = random color and lower opacity.\n",
    "    \"\"\"\n",
    "    if score > 0.75:\n",
    "        # High confidence: Red with high opacity\n",
    "        color = (1, 0, 0, min(1.0, 0.3 + score))  # Red with opacity based on score\n",
    "    else:\n",
    "        # Low confidence: Random color with lower opacity\n",
    "        color = (random.random(), random.random(), random.random(), max(0.3, score))\n",
    "    return color\n",
    "\n",
    "def visualize_ground_truth_and_predictions(annotation_file, images_dir, model, \n",
    "                                          confidence_threshold=0.5, \n",
    "                                          save_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize both ground truth and YOLO model predictions on images.\n",
    "    \n",
    "    Args:\n",
    "        annotation_file (str): Path to the annotations JSON file in COCO format\n",
    "        images_dir (str): Directory containing the images\n",
    "        model: The YOLO model\n",
    "        confidence_threshold (float): Threshold for prediction confidence\n",
    "        save_dir (str, optional): Directory to save output images instead of displaying\n",
    "    \"\"\"\n",
    "    # Load annotations\n",
    "    with open(annotation_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # Create a dictionary mapping image_id to all its annotations\n",
    "        image_annotations = defaultdict(list)\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            image_annotations[annotation[\"image_id\"]].append(annotation)\n",
    "        \n",
    "        # Create a dictionary mapping image_id to image data (including filename)\n",
    "        image_data_by_id = {}\n",
    "        for image in data[\"images\"]:\n",
    "            image_data_by_id[image[\"id\"]] = image\n",
    "        \n",
    "        # Process each image defined in the annotations\n",
    "        for image_id, annotations in image_annotations.items():\n",
    "            # Skip if we don't have image data for this id\n",
    "            if image_id not in image_data_by_id:\n",
    "                print(f\"No image data found for image ID {image_id}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Get image data including filename\n",
    "            image_data = image_data_by_id[image_id]\n",
    "            file_name = image_data[\"file_name\"]\n",
    "            \n",
    "            # Read the image\n",
    "            # get filename without extension\n",
    "            file_name_no_ext = os.path.splitext(file_name)[0]\n",
    "\n",
    "            image_path = images_dir + file_name_no_ext + \".jpg\"\n",
    "            \n",
    "            # Check if file exists\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Load image for CV2/matplotlib display\n",
    "            cv_image = cv2.imread(image_path)\n",
    "            if cv_image is None:\n",
    "                print(f\"Failed to read image: {image_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Convert BGR (OpenCV format) to RGB (Matplotlib format)\n",
    "            cv_image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Make predictions with YOLO\n",
    "            start_time = time.time()\n",
    "            results = model(image_path)\n",
    "            prediction_time = time.time() - start_time\n",
    "            print(f\"Prediction time for {file_name} (ID: {image_id}): {prediction_time:.4f} seconds\")\n",
    "            \n",
    "            # Create figure and axis\n",
    "            fig, ax = plt.subplots(1, figsize=(12, 10))\n",
    "            ax.imshow(cv_image_rgb)\n",
    "            \n",
    "            # Draw ground truth annotations (white boxes)\n",
    "            for annotation in annotations:\n",
    "                x, y, w, h = annotation[\"bbox\"]\n",
    "                x, y, w, h = round(x), round(y), round(w), round(h)\n",
    "                \n",
    "                # Draw the bounding box with thick white lines\n",
    "                rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='white', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add \"Ground Truth\" label\n",
    "                ax.text(x + w + 5, y + 5, \"Ground Truth\", color='white', fontsize=12, \n",
    "                        bbox=dict(facecolor='black', alpha=0.7, edgecolor='none', pad=1))\n",
    "            \n",
    "            # Draw YOLO model predictions (colored boxes based on confidence)\n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                for box in boxes:\n",
    "                    confidence = float(box.conf[0])\n",
    "                    \n",
    "                    # Filter predictions based on confidence threshold\n",
    "                    if confidence >= confidence_threshold:\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                        class_id = int(box.cls[0])\n",
    "                        label = result.names.get(class_id, f\"Class {class_id}\")\n",
    "                        \n",
    "                        color = get_color_with_opacity(confidence)\n",
    "                        \n",
    "                        # Draw rectangle with opacity\n",
    "                        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, \n",
    "                                               edgecolor=color, facecolor='none')\n",
    "                        ax.add_patch(rect)\n",
    "                        \n",
    "                        # Add text label with confidence score\n",
    "                        ax.text(x1, y1 - 10, f'{label}: {confidence:.2f}', \n",
    "                                color='white', \n",
    "                                bbox=dict(facecolor=color[:3], alpha=0.6), \n",
    "                                fontsize=12)\n",
    "            \n",
    "            # Add image filename and ID as title\n",
    "            plt.title(f\"File: {file_name} (ID: {image_id})\", fontsize=14)\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save or display the image\n",
    "            if save_dir:\n",
    "                # Ensure directory exists\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_path = os.path.join(save_dir, f\"gt_pred_{file_name}\")\n",
    "                plt.savefig(save_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "                plt.close(fig)\n",
    "                print(f\"Image saved to: {save_path}\")\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "def run_visualization(annotation_file, images_dir, model_path, save_dir=None):\n",
    "    \"\"\"\n",
    "    Run the visualization for a YOLO model.\n",
    "    \n",
    "    Args:\n",
    "        annotation_file (str): Path to the COCO format annotation file\n",
    "        images_dir (str): Directory containing images\n",
    "        model_path (str): Path to YOLO model file\n",
    "        save_dir (str, optional): Directory to save output images\n",
    "    \"\"\"\n",
    "    # Load YOLO model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Run visualization\n",
    "    visualize_ground_truth_and_predictions(\n",
    "        annotation_file=annotation_file,\n",
    "        images_dir=images_dir,\n",
    "        model=model,\n",
    "        confidence_threshold=0.5,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# run_visualization(\n",
    "#     annotation_file=\"data/public_dataset/test/annotations/instances_test.json\",\n",
    "#     images_dir=\"output/sun_flare/sun_flare-0-aug_\",\n",
    "#     model_path=\"data/yolo_models/yolo11x_10epoch_augmented.pt\",\n",
    "#     # save_dir=\"output/ground_truth_and_predictions\"  # Optional: directory to save results\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_color_with_opacity(score):\n",
    "    \"\"\"\n",
    "    Get a color with opacity based on the confidence score.\n",
    "    Higher confidence = more red and higher opacity.\n",
    "    Lower confidence = random color and lower opacity.\n",
    "    \"\"\"\n",
    "    if score > 0.75:\n",
    "        # High confidence: Red with high opacity\n",
    "        color = (1, 0, 0, min(1.0, 0.3 + score))\n",
    "    else:\n",
    "        # Low confidence: Random color with lower opacity\n",
    "        color = (random.random(), random.random(), random.random(), max(0.3, score))\n",
    "    return color\n",
    "\n",
    "def load_image(image_path, transforms=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transforms:\n",
    "        for transform in transforms:\n",
    "            image, _ = transform(image, target=None)  # No target during inference\n",
    "    return image\n",
    "\n",
    "def visualize_ground_truth_and_predictions(annotation_file, images_dir, model, device, test_transforms,\n",
    "                                             confidence_threshold=0.5, classes=['background', 'hazmat'], save_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize both ground truth and Faster R-CNN model predictions on images.\n",
    "    \n",
    "    Args:\n",
    "        annotation_file (str): Path to the annotations JSON file (COCO format)\n",
    "        images_dir (str): Directory containing the images\n",
    "        model: The Faster R-CNN model\n",
    "        device: torch.device (CPU/GPU)\n",
    "        test_transforms: Transformation pipeline to apply to images for model input\n",
    "        confidence_threshold (float): Threshold for prediction confidence\n",
    "        classes (list): List of class names; adjust as needed\n",
    "        save_dir (str, optional): Directory to save output images instead of displaying\n",
    "    \"\"\"\n",
    "    # Load annotations\n",
    "    with open(annotation_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Map image IDs to annotations\n",
    "    image_annotations = defaultdict(list)\n",
    "    for annotation in data[\"annotations\"]:\n",
    "        image_annotations[annotation[\"image_id\"]].append(annotation)\n",
    "    \n",
    "    # Map image IDs to image data\n",
    "    image_data_by_id = {}\n",
    "    for image in data[\"images\"]:\n",
    "        image_data_by_id[image[\"id\"]] = image\n",
    "    \n",
    "    for image_id, annotations in image_annotations.items():\n",
    "        # Skip if image data isn't found\n",
    "        if image_id not in image_data_by_id:\n",
    "            print(f\"No image data found for image ID {image_id}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        image_data = image_data_by_id[image_id]\n",
    "        file_name = image_data[\"file_name\"]\n",
    "        file_name_no_ext = os.path.splitext(file_name)[0]\n",
    "        image_path = images_dir + file_name_no_ext + \".jpg\"\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Load image for display via OpenCV\n",
    "        cv_image = cv2.imread(image_path)\n",
    "        if cv_image is None:\n",
    "            print(f\"Failed to read image: {image_path}\")\n",
    "            continue\n",
    "        cv_image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Prepare image for model prediction using PIL and transforms\n",
    "        tensor_image = load_image(image_path, transforms=test_transforms)\n",
    "        tensor_image = tensor_image.to(device)\n",
    "        \n",
    "        # Get predictions from Faster R-CNN (wrap in no_grad)\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            predictions = model([tensor_image])\n",
    "        prediction_time = time.time() - start_time\n",
    "        print(f\"Prediction time for {file_name} (ID: {image_id}): {prediction_time:.4f} seconds\")\n",
    "        \n",
    "        # Create figure for plotting\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 10))\n",
    "        ax.imshow(cv_image_rgb)\n",
    "        \n",
    "        # Draw ground truth annotations (white boxes)\n",
    "        for annotation in annotations:\n",
    "            x, y, w, h = map(round, annotation[\"bbox\"])\n",
    "            rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='white', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x + w + 5, y + 5, \"Ground Truth\", color='white', fontsize=12,\n",
    "                    bbox=dict(facecolor='black', alpha=0.7, edgecolor='none', pad=1))\n",
    "        \n",
    "        # Process Faster R-CNN predictions\n",
    "        pred = predictions[0]\n",
    "        boxes = pred['boxes'].cpu().numpy()\n",
    "        labels = pred['labels'].cpu().numpy()\n",
    "        scores = pred['scores'].cpu().numpy()\n",
    "        \n",
    "        # Filter predictions based on confidence threshold\n",
    "        keep = scores >= confidence_threshold\n",
    "        boxes = boxes[keep]\n",
    "        labels = labels[keep]\n",
    "        scores = scores[keep]\n",
    "        \n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            # Get class name from the provided classes list (adjust if needed)\n",
    "            label_name = classes[label] if label < len(classes) else f\"Class {label}\"\n",
    "            x1, y1, x2, y2 = box\n",
    "            color = get_color_with_opacity(score)\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "                                     edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1 - 10, f'{label_name}: {score:.2f}', \n",
    "                    color='white', \n",
    "                    bbox=dict(facecolor=color[:3], alpha=0.6), \n",
    "                    fontsize=12)\n",
    "        \n",
    "        plt.title(f\"File: {file_name} (ID: {image_id})\", fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save or display the image\n",
    "        if save_dir:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(save_dir, f\"gt_pred_{file_name}\")\n",
    "            plt.savefig(save_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "            plt.close(fig)\n",
    "            print(f\"Image saved to: {save_path}\")\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "def run_visualization(annotation_file, images_dir, model, device, test_transforms,\n",
    "                      confidence_threshold=0.5, classes=['background', 'hazmat'], save_dir=None):\n",
    "    \"\"\"\n",
    "    Run the visualization for a Faster R-CNN model.\n",
    "    \n",
    "    Args:\n",
    "        annotation_file (str): Path to the annotations JSON file\n",
    "        images_dir (str): Directory containing the images\n",
    "        model: Loaded Faster R-CNN model\n",
    "        device: torch.device object (CPU or GPU)\n",
    "        test_transforms: Transformation pipeline for model input\n",
    "        confidence_threshold (float): Threshold for displaying predictions\n",
    "        classes (list): List of class names\n",
    "        save_dir (str, optional): Directory to save output images\n",
    "    \"\"\"\n",
    "    visualize_ground_truth_and_predictions(\n",
    "        annotation_file=annotation_file,\n",
    "        images_dir=images_dir,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        test_transforms=test_transforms,\n",
    "        confidence_threshold=confidence_threshold,\n",
    "        classes=classes,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you've already loaded your Faster R-CNN model, defined your device (e.g., torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")),\n",
    "# and set up your test_transforms:\n",
    "\n",
    "# run_visualization(\n",
    "#     annotation_file=\"data/public_dataset/test/annotations/instances_test.json\",\n",
    "#     images_dir=\"output/sun_flare/sun_flare-0-aug_\",\n",
    "#     model=model,              # Your loaded Faster R-CNN model\n",
    "#     device=device,            # torch.device instance\n",
    "#     test_transforms=test_transforms,  # Your transformation pipeline\n",
    "#     # save_dir=\"output/ground_truth_and_predictions\"  # Optional: directory to save results\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to make the same code but faster rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
